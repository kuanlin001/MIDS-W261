{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATSCI W261 ASSIGNMENT 2\n",
    "Section 2<br/>\n",
    "Student: Kuan Lin<br/>\n",
    "Date: 1/23/2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <b>HW2.0. What is a race condition in the context of parallel computation? Give an example. What is MapReduce? How does it differ from Hadoop? Which programming paradigm is Hadoop based on? Explain and give a simple example in code and show the code running.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Race condition occurs when multiple threads or processes attempting to access and write to a shared object.  If no proper synchronization, some threads may retrieve incorrect/staled data while the shared object is being modified by a different thread.  An example would be if multiple threads are trying to increment an integer in a shared memory.  Because integer increment is not atomic, if multiple threads trying to increment the integer simultaneously, one thread may be trying to increment on a staled value, while the actual value has already been modified by a different thread.</p>\n",
    "<p>\n",
    "MapReduce is a programming model that first produces a new sequence of objects by applying a function (map) to an existing sequence of objects, and aggregate results in the newly created sequence (reduce).  The map step is often done in parrallel.<br/>\n",
    "Hadoop is an open source framework consisting of a distributed file system and also an executing engine that can execute codes in a distributed fashion via the MapReduce paradigm.\n",
    "</p>\n",
    "<p>\n",
    "Hadoop is based on functional programming paradigm, which treats computation as mathmatical evaluations and avoid producing side effects and changing object states.  The code block below shows how to calculate sum of squares of a list of integers using functional paradigm.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n"
     ]
    }
   ],
   "source": [
    "# MapReduce example\n",
    "# calculating sum of squares with functional programming paradigm\n",
    "\n",
    "input = [1, 2, 3, 4, 5]\n",
    "\n",
    "# map step\n",
    "input_squares = map(lambda x:x**2, input)\n",
    "# reduce step\n",
    "results = reduce(lambda x1,x2:x1+x2, input_squares)\n",
    "\n",
    "print results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <b>HW2.1. Sort in Hadoop MapReduce</b>\n",
    "* Given as input: Records of the form `<integer, “NA”>`, where integer is any integer, and “NA” is just the empty string. Output: sorted key value pairs of the form `<integer, “NA”>` in decreasing order; what happens if you have multiple reducers? Do you need additional steps? Explain.\n",
    "* Write code to generate N  random records of the form `<integer, “NA”>`. Let N = 10,000. Write the python Hadoop streaming map-reduce job to perform this sort. Display the top 10 biggest numbers. Display the 10 smallest numbers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have multiple reducers, we will need one more script to perform merge sort using the output files from all of the reducers.  This is because we do not have control which intermediate key-value pairs is sent to which reducer, and thus each output files may have skipped numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting generateNRandom.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile generateNRandom.py\n",
    "#!/usr/bin/python\n",
    "# Generate N random integers\n",
    "import sys\n",
    "from random import randint\n",
    "\n",
    "writer = open(\"hw2_1.txt\", 'w')\n",
    "N = int(sys.argv[1])\n",
    "# generate N random integers\n",
    "for i in xrange(N):\n",
    "    writer.write(str(randint(0, N)) + '\\n')\n",
    "writer.close()\n",
    "print \"file written\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file written\r\n"
     ]
    }
   ],
   "source": [
    "!chmod +x generateNRandom.py; python generateNRandom.py 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# put the file on hdfs\n",
    "!hdfs dfs -mkdir /w261/hw2;\n",
    "!hdfs dfs -mkdir /w261/hw2/hw2_1;\n",
    "!hdfs dfs -put hw2_1.txt /w261/hw2/hw2_1/hw2_1.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "# no need to do anything in the mapper, just pass along the numbers and let Hadoop do the sorting\n",
    "import sys\n",
    "for line in sys.stdin:    \n",
    "    print \"%s\" % (line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "# just printing the numbers.  Actual sorting is done in hadoop setup\n",
    "import sys\n",
    "for line in sys.stdin:    \n",
    "    print \"%s\" % (line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/24 05:26:47 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /w261/hw2/hw2_1/output\n",
      "packageJobJar: [] [/usr/lib/hadoop/lib/hadoop-streaming-2.6.0.jar] /tmp/streamjob1368099165353184764.jar tmpDir=null\n",
      "16/01/24 05:26:54 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/24 05:26:55 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/24 05:26:56 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/24 05:26:56 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/01/24 05:26:56 INFO Configuration.deprecation: mapred.output.key.comparator.class is deprecated. Instead, use mapreduce.job.output.key.comparator.class\n",
      "16/01/24 05:26:56 INFO Configuration.deprecation: mapred.text.key.comparator.options is deprecated. Instead, use mapreduce.partition.keycomparator.options\n",
      "16/01/24 05:26:57 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1453604265894_0013\n",
      "16/01/24 05:26:58 INFO impl.YarnClientImpl: Submitted application application_1453604265894_0013\n",
      "16/01/24 05:26:58 INFO mapreduce.Job: The url to track the job: http://ip-172-31-55-170.ec2.internal:8088/proxy/application_1453604265894_0013/\n",
      "16/01/24 05:26:58 INFO mapreduce.Job: Running job: job_1453604265894_0013\n",
      "16/01/24 05:27:15 INFO mapreduce.Job: Job job_1453604265894_0013 running in uber mode : false\n",
      "16/01/24 05:27:15 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/01/24 05:27:37 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "16/01/24 05:27:38 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/24 05:27:52 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/24 05:27:53 INFO mapreduce.Job: Job job_1453604265894_0013 completed successfully\n",
      "16/01/24 05:27:53 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=78909\n",
      "\t\tFILE: Number of bytes written=496847\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=49224\n",
      "\t\tHDFS: Number of bytes written=58903\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=41503\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=11987\n",
      "\t\tTotal time spent by all map tasks (ms)=41503\n",
      "\t\tTotal time spent by all reduce tasks (ms)=11987\n",
      "\t\tTotal vcore-seconds taken by all map tasks=41503\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=11987\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=42499072\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=12274688\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=10000\n",
      "\t\tMap output records=10000\n",
      "\t\tMap output bytes=58903\n",
      "\t\tMap output materialized bytes=78915\n",
      "\t\tInput split bytes=196\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=6270\n",
      "\t\tReduce shuffle bytes=78915\n",
      "\t\tReduce input records=10000\n",
      "\t\tReduce output records=10000\n",
      "\t\tSpilled Records=20000\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=812\n",
      "\t\tCPU time spent (ms)=4850\n",
      "\t\tPhysical memory (bytes) snapshot=553582592\n",
      "\t\tVirtual memory (bytes) snapshot=7588679680\n",
      "\t\tTotal committed heap usage (bytes)=392372224\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=49028\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=58903\n",
      "16/01/24 05:27:53 INFO streaming.StreamJob: Output directory: /w261/hw2/hw2_1/output\n"
     ]
    }
   ],
   "source": [
    "!cp mapper.py /data/w261/hw2/mapper.py; cp reducer.py /data/w261/hw2/reducer.py;\n",
    "!chmod +x /data/w261/hw2/mapper.py; chmod +x /data/w261/hw2/reducer.py;\n",
    "#!hdfs dfs -rm -r /w261/hw2/hw2_1/output;\n",
    "# the key is to set mapred.text.key.comparator.options=-nr, meaning reverse numerical sorting\n",
    "!$HADOOP_HOME/bin/hadoop jar $HADOOP_HOME/lib/hadoop-*streaming*.jar -D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator -D mapred.text.key.comparator.options=-nr -mapper /data/w261/hw2/mapper.py -reducer /data/w261/hw2/reducer.py -input /w261/hw2/hw2_1/hw2_1.txt -output /w261/hw2/hw2_1/output;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\t\r\n",
      "10000\t\r\n",
      "9999\t\r\n",
      "9997\t\r\n",
      "9997\t\r\n",
      "9996\t\r\n",
      "9992\t\r\n",
      "9992\t\r\n",
      "9992\t\r\n",
      "9991\t\r\n",
      "cat: Unable to write to output stream.\r\n"
     ]
    }
   ],
   "source": [
    "# display 10 biggest numbers:\n",
    "!hdfs dfs -cat /w261/hw2/hw2_1/output/part-00000 | head -10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\t\r\n",
      "4\t\r\n",
      "3\t\r\n",
      "3\t\r\n",
      "3\t\r\n",
      "1\t\r\n",
      "0\t\r\n",
      "0\t\r\n",
      "0\t\r\n",
      "0\t\r\n"
     ]
    }
   ],
   "source": [
    "# display 10 smallest numbers:\n",
    "!hdfs dfs -cat /w261/hw2/hw2_1/output/part-00000 | tail -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <b>HW2.2.  WORDCOUNT\n",
    "* Using the Enron data from HW1 and Hadoop MapReduce streaming, write the mapper/reducer job that  will determine the word count (number of occurrences) of each white-space delimitted token (assume spaces, fullstops, comma as delimiters). Examine the word “assistance” and report its word count results.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# put the enronn file on hdfs\n",
    "!hdfs dfs -put enronemail_1h.txt /w261/hw2/enronemail_1h.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "import string\n",
    "# split by spaces, fullstops, and comma\n",
    "patt = re.compile('[\\s|\\.|,]')\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    if line == '': continue\n",
    "    lineArr = line.split('\\t')\n",
    "    # skip bad entrieds\n",
    "    if len(lineArr) != 4: continue\n",
    "    subject = lineArr[2]\n",
    "    body = lineArr [3]\n",
    "    subject_body = subject + ' ' + body\n",
    "    # remove punctuations\n",
    "    subject_body = subject_body.translate(string.maketrans(\"\",\"\"), string.punctuation)\n",
    "    words = re.split(patt, subject_body)\n",
    "    for word in words:\n",
    "        word = word.strip()\n",
    "        if word != '':\n",
    "            print '%s\\t%s' % (word, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "\n",
    "current_word = None\n",
    "current_count = 0\n",
    "word = None\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "\n",
    "    # parse the input we got from mapper.py\n",
    "    word, count = line.split('\\t', 1)\n",
    "\n",
    "    # convert count (currently a string) to int\n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        # count was not a number, so silently\n",
    "        # ignore/discard this line\n",
    "        continue\n",
    "\n",
    "    # this IF-switch only works because Hadoop sorts map output\n",
    "    # by key (here: word) before it is passed to the reducer\n",
    "    if current_word == word:\n",
    "        current_count += count\n",
    "    else:\n",
    "        if current_word:\n",
    "            # write result to STDOUT\n",
    "            print '%s\\t%s' % (current_word, current_count)\n",
    "        current_count = count\n",
    "        current_word = word\n",
    "\n",
    "# do not forget to output the last word if needed!\n",
    "if current_word == word:\n",
    "    print '%s\\t%s' % (current_word, current_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/24 06:53:45 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /w261/hw2/hw2_2_output\n",
      "16/01/24 06:53:47 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "packageJobJar: [/data/w261/hw2/mapper.py, /data/w261/hw2/reducer.py] [/usr/lib/hadoop/lib/hadoop-streaming-2.6.0.jar] /tmp/streamjob3052510899272968637.jar tmpDir=null\n",
      "16/01/24 06:53:52 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/24 06:53:53 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/24 06:53:54 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/24 06:53:55 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/01/24 06:53:55 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1453604265894_0019\n",
      "16/01/24 06:53:56 INFO impl.YarnClientImpl: Submitted application application_1453604265894_0019\n",
      "16/01/24 06:53:56 INFO mapreduce.Job: The url to track the job: http://ip-172-31-55-170.ec2.internal:8088/proxy/application_1453604265894_0019/\n",
      "16/01/24 06:53:56 INFO mapreduce.Job: Running job: job_1453604265894_0019\n",
      "16/01/24 06:54:11 INFO mapreduce.Job: Job job_1453604265894_0019 running in uber mode : false\n",
      "16/01/24 06:54:11 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/01/24 06:54:33 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "16/01/24 06:54:34 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/24 06:54:47 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/24 06:54:47 INFO mapreduce.Job: Job job_1453604265894_0019 completed successfully\n",
      "16/01/24 06:54:48 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=300209\n",
      "\t\tFILE: Number of bytes written=942462\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=216879\n",
      "\t\tHDFS: Number of bytes written=56271\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=40081\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=10339\n",
      "\t\tTotal time spent by all map tasks (ms)=40081\n",
      "\t\tTotal time spent by all reduce tasks (ms)=10339\n",
      "\t\tTotal vcore-seconds taken by all map tasks=40081\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=10339\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=41042944\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=10587136\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=101\n",
      "\t\tMap output records=30512\n",
      "\t\tMap output bytes=239179\n",
      "\t\tMap output materialized bytes=300215\n",
      "\t\tInput split bytes=200\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=5664\n",
      "\t\tReduce shuffle bytes=300215\n",
      "\t\tReduce input records=30512\n",
      "\t\tReduce output records=5664\n",
      "\t\tSpilled Records=61024\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=830\n",
      "\t\tCPU time spent (ms)=5140\n",
      "\t\tPhysical memory (bytes) snapshot=560562176\n",
      "\t\tVirtual memory (bytes) snapshot=7593975808\n",
      "\t\tTotal committed heap usage (bytes)=392372224\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=216679\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=56271\n",
      "16/01/24 06:54:48 INFO streaming.StreamJob: Output directory: /w261/hw2/hw2_2_output\n"
     ]
    }
   ],
   "source": [
    "!cp mapper.py /data/w261/hw2/mapper.py; cp reducer.py /data/w261/hw2/reducer.py;\n",
    "!chmod +x /data/w261/hw2/mapper.py; chmod +x /data/w261/hw2/reducer.py;\n",
    "!hdfs dfs -rm -r /w261/hw2/hw2_2_output;\n",
    "!$HADOOP_HOME/bin/hadoop jar $HADOOP_HOME/lib/hadoop-*streaming*.jar -mapper mapper.py -file /data/w261/hw2/mapper.py -reducer reducer.py -file /data/w261/hw2/reducer.py -input /w261/hw2/enronemail_1h.txt -output /w261/hw2/hw2_2_output;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistance\t9\r\n"
     ]
    }
   ],
   "source": [
    "# examine the word \"assistance\"\n",
    "!hdfs dfs -cat /w261/hw2/hw2_2_output/part-00000 | grep assistance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above number is less than 10 because one of the record containing the word \"assistance\" does not have the properly formated field width."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <b>HW2.2.1  Using Hadoop MapReduce and your wordcount job (from HW2.2) determine the top-10 occurring tokens (most frequent tokens)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "# I am using the word count as the key and ask Hadoop to do descending sort.  This should now give me the top-10 token\n",
    "import sys\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    if line == '': continue\n",
    "    print '\\t'.join([line.split('\\t')[1], line.split('\\t')[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "# no need to do much, descending sort is done in hadoop command line (see next cell).\n",
    "import sys\n",
    "# print out only the top 10 tokens\n",
    "top_n = 10\n",
    "printcnt = 0\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    if printcnt < top_n:\n",
    "        print '\\t'.join([line.split('\\t')[1], line.split('\\t')[0]])\n",
    "        printcnt += 1\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/24 07:26:46 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /w261/hw2/hw2_2_1_output\n",
      "16/01/24 07:26:48 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "packageJobJar: [/data/w261/hw2/mapper.py, /data/w261/hw2/reducer.py] [/usr/lib/hadoop/lib/hadoop-streaming-2.6.0.jar] /tmp/streamjob4936071954852985045.jar tmpDir=null\n",
      "16/01/24 07:26:53 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/24 07:26:54 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/24 07:26:56 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/24 07:26:56 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/01/24 07:26:56 INFO Configuration.deprecation: mapred.output.key.comparator.class is deprecated. Instead, use mapreduce.job.output.key.comparator.class\n",
      "16/01/24 07:26:56 INFO Configuration.deprecation: mapred.text.key.comparator.options is deprecated. Instead, use mapreduce.partition.keycomparator.options\n",
      "16/01/24 07:26:57 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1453604265894_0022\n",
      "16/01/24 07:26:57 INFO impl.YarnClientImpl: Submitted application application_1453604265894_0022\n",
      "16/01/24 07:26:57 INFO mapreduce.Job: The url to track the job: http://ip-172-31-55-170.ec2.internal:8088/proxy/application_1453604265894_0022/\n",
      "16/01/24 07:26:57 INFO mapreduce.Job: Running job: job_1453604265894_0022\n",
      "16/01/24 07:27:15 INFO mapreduce.Job: Job job_1453604265894_0022 running in uber mode : false\n",
      "16/01/24 07:27:15 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/01/24 07:27:37 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "16/01/24 07:27:38 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/24 07:27:52 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/24 07:27:52 INFO mapreduce.Job: Job job_1453604265894_0022 completed successfully\n",
      "16/01/24 07:27:52 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=67605\n",
      "\t\tFILE: Number of bytes written=478244\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=57020\n",
      "\t\tHDFS: Number of bytes written=76\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=41531\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=10290\n",
      "\t\tTotal time spent by all map tasks (ms)=41531\n",
      "\t\tTotal time spent by all reduce tasks (ms)=10290\n",
      "\t\tTotal vcore-seconds taken by all map tasks=41531\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=10290\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=42527744\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=10536960\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=5664\n",
      "\t\tMap output records=5664\n",
      "\t\tMap output bytes=56271\n",
      "\t\tMap output materialized bytes=67611\n",
      "\t\tInput split bytes=212\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=104\n",
      "\t\tReduce shuffle bytes=67611\n",
      "\t\tReduce input records=5664\n",
      "\t\tReduce output records=10\n",
      "\t\tSpilled Records=11328\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=729\n",
      "\t\tCPU time spent (ms)=4360\n",
      "\t\tPhysical memory (bytes) snapshot=563634176\n",
      "\t\tVirtual memory (bytes) snapshot=7596371968\n",
      "\t\tTotal committed heap usage (bytes)=392372224\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=56808\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=76\n",
      "16/01/24 07:27:52 INFO streaming.StreamJob: Output directory: /w261/hw2/hw2_2_1_output\n"
     ]
    }
   ],
   "source": [
    "!cp mapper.py /data/w261/hw2/mapper.py; cp reducer.py /data/w261/hw2/reducer.py;\n",
    "!chmod +x /data/w261/hw2/mapper.py; chmod +x /data/w261/hw2/reducer.py;\n",
    "!hdfs dfs -rm -r /w261/hw2/hw2_2_1_output;\n",
    "# -D mapred.text.key.comparator.options=-nr cause hadoop to do numerically descending sort.  Word count is the key\n",
    "!$HADOOP_HOME/bin/hadoop jar $HADOOP_HOME/lib/hadoop-*streaming*.jar -D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator -D mapred.text.key.comparator.options=-nr -mapper mapper.py -file /data/w261/hw2/mapper.py -reducer reducer.py -file /data/w261/hw2/reducer.py -input /w261/hw2/hw2_2_output/part-00000 -output /w261/hw2/hw2_2_1_output;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\t1207\r\n",
      "to\t941\r\n",
      "and\t622\r\n",
      "of\t539\r\n",
      "a\t518\r\n",
      "you\t421\r\n",
      "in\t402\r\n",
      "your\t381\r\n",
      "for\t358\r\n",
      "on\t254\r\n"
     ]
    }
   ],
   "source": [
    "# the top 10 tokens:\n",
    "!hdfs dfs -cat /w261/hw2/hw2_2_1_output/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <b>HW2.3. Multinomial NAIVE BAYES with NO Smoothing\n",
    "* Using the Enron data from HW1 and Hadoop MapReduce, write  a mapper/reducer job(s) that will both learn  Naive Bayes classifier and classify the Enron email messages using the learnt Naive Bayes classifier. Use all white-space delimitted tokens as independent input variables (assume spaces, fullstops, commas as delimiters).\n",
    "* Count up how many times you need to process a zero probabilty for each class and report.\n",
    "* Report the performance of your learnt classifier in terms of misclassifcation error rate of your multinomial Naive Bayes Classifier. Plot a histogram of the  posterior probabilities (i.e., Pr(Class|Doc)) for each class over the training set. Summarize what you see.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Kuan Lin\n",
    "## Description: mapper code for HW1.5\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import string\n",
    "\n",
    "useAllWords = True\n",
    "findwords = None\n",
    "\n",
    "spam_word_counts = {}\n",
    "ham_word_counts = {}\n",
    "spam_total_words = 0\n",
    "ham_total_words = 0\n",
    "\n",
    "def findWordCount(doc, word):\n",
    "\tcount = 0\n",
    "\tfor w in doc.split(' '):\n",
    "\t\tif w == word: count += 1\n",
    "\treturn count\n",
    "\n",
    "for line in sys.stdin:\n",
    "\tline = line.strip().lower()\n",
    "\tif line == \"\": continue\n",
    "\tlineArr = line.split('\\t')\n",
    "\t# check data integrity.  each valid line should have 4 fields, with second field equals either 0 or 1\n",
    "\tif len(lineArr) != 4 or (lineArr[1] != \"0\" and lineArr[1] != \"1\"): continue\n",
    "\t\n",
    "\tspam_flag = lineArr[1]\n",
    "\tsubject_body = lineArr[2] + ' ' + lineArr[3]\n",
    "\t# remove punctuations\n",
    "\tsubject_body = subject_body.translate(string.maketrans(\"\",\"\"), string.punctuation).strip().lower()\n",
    "\tall_words = [word.strip() for word in subject_body.split(' ') if word.strip() != '']\n",
    "\t#print str(all_words)\n",
    "\tif useAllWords:\n",
    "\t\tfindwords = set([w for w in all_words])\n",
    "\t\n",
    "\tif spam_flag == \"1\":\n",
    "\t\tspam_total_words += len(all_words)\n",
    "\telse:\n",
    "\t\tham_total_words += len(all_words)\n",
    "\t\t\n",
    "\tfor word in findwords:\n",
    "\t\t#pattern = re.compile('[\\s+]?(' + word + ')[\\s+]?')\n",
    "\t\tif spam_flag == \"1\":\n",
    "\t\t\tif word in spam_word_counts: spam_word_counts[word] += findWordCount(subject_body, word)\n",
    "\t\t\telse: spam_word_counts[word] = findWordCount(subject_body, word)\n",
    "\t\telse:\n",
    "\t\t\tif word in ham_word_counts: ham_word_counts[word] += findWordCount(subject_body, word)\n",
    "\t\t\telse: ham_word_counts[word] = findWordCount(subject_body, word)\n",
    "\t\t\t\n",
    "print '\\n'.join([word + '\\t' + str(spam_word_counts[word]) + '\\t1' for word in spam_word_counts] + \n",
    "[word + '\\t' + str(ham_word_counts[word]) + '\\t0' for word in ham_word_counts] +\n",
    "['class_total_words\\t' + str(spam_total_words) + '\\t1', 'class_total_words\\t' + str(ham_total_words) + '\\t0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Kuan Lin\n",
    "## Description: reducer code for HW1.5\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import math\n",
    "import string\n",
    "import subprocess\n",
    "\n",
    "files = sys.argv[1:]\n",
    "# data to classify.  File location is on HDFS\n",
    "data=\"/w261/hw2/enronemail_1h.txt\"\n",
    "\n",
    "spam_word_counts = {}\n",
    "ham_word_counts = {}\n",
    "spam_total_words = 0\n",
    "ham_total_words = 0\n",
    "unique_words = set()\n",
    "spam_zero_count_processed = 0\n",
    "ham_zero_count_processed = 0\n",
    "\n",
    "# assume equal prior for spam and ham\n",
    "spam_prior = 0.44\n",
    "ham_prior = 0.56\n",
    "\n",
    "def findWordCount(doc, word):\n",
    "\tcount = 0\n",
    "\tfor w in doc.split(' '):\n",
    "\t\tif w == word: count += 1\n",
    "\treturn count\n",
    "\n",
    "def spamWordProb(word):\n",
    "\tcount = 0\n",
    "\tif word in spam_word_counts: count = spam_word_counts[word]\n",
    "\t# No laplace smoothing\n",
    "\t#return float(count+1)/float(spam_total_words+len(unique_words))\n",
    "\treturn float(count)/float(spam_total_words)\n",
    "\t\n",
    "def hamWordProb(word):\n",
    "\tcount = 0\n",
    "\tif word in ham_word_counts: count = ham_word_counts[word]\n",
    "\t# No laplace smoothing\n",
    "\t#return float(count+1)/float(ham_total_words+len(unique_words))\n",
    "\treturn float(count)/float(ham_total_words)\n",
    "\t\n",
    "def classifyWithWordList(document, word_list):\n",
    "\tglobal spam_zero_count_processed\n",
    "\tglobal ham_zero_count_processed\n",
    "\tspam_logprob = math.log(spam_prior)\n",
    "\tham_logprob = math.log(ham_prior)\n",
    "\tfor word in word_list:\n",
    "\t\t#pattern = re.compile('[\\s+]?(' + word + ')[\\s+]?')\n",
    "\t\tword_count = findWordCount(document, word)\n",
    "\t\tspam_word_prob = spamWordProb(word)\n",
    "\t\tham_word_prob = hamWordProb(word)\n",
    "\t\t# update prob if word count > 0.  Otherwise record the event as per instruction.\n",
    "\t\tif spam_word_prob > 0:\n",
    "\t\t\tspam_logprob += math.log(spam_word_prob)*word_count\n",
    "\t\telse:\n",
    "\t\t\tspam_zero_count_processed += 1\n",
    "\t\t\t# using a small default probability in order to avoid taking log(0) which will throw an error\n",
    "\t\t\tspam_logprob += math.log(0.0001)*word_count\n",
    "\t\tif ham_word_prob > 0:\n",
    "\t\t\tham_logprob += math.log(ham_word_prob)*word_count\n",
    "\t\telse:\n",
    "\t\t\tham_zero_count_processed += 1\n",
    "\t\t\t# using a small default probability in order to avoid taking log(0) which will throw an error\n",
    "\t\t\tham_logprob += math.log(0.0001)*word_count\n",
    "\t# take the exponential to recover probability\n",
    "\tspam_prob = math.e**spam_logprob\n",
    "\tham_prob = math.e**ham_logprob\n",
    "\tif spam_logprob > ham_logprob:\n",
    "\t\treturn (1, spam_prob, ham_prob)\n",
    "\telse:\n",
    "\t\treturn (0, spam_prob, ham_prob)\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    if line == \"\": continue\n",
    "\n",
    "    lineArr = line.split('\\t')\n",
    "    word = lineArr[0]\n",
    "    count = int(lineArr[1])\n",
    "    spam_flag = int(lineArr[2])\n",
    "\n",
    "    if spam_flag == 1:\n",
    "        if word == \"class_total_words\":\n",
    "            spam_total_words += count\n",
    "            continue\n",
    "        if word in spam_word_counts:\n",
    "            spam_word_counts[word] += count\n",
    "        else:\n",
    "            spam_word_counts[word] = count\n",
    "    else:\n",
    "        if word == \"class_total_words\":\n",
    "            ham_total_words += count\n",
    "            continue\n",
    "        if word in ham_word_counts:\n",
    "            ham_word_counts[word] += count\n",
    "        else:\n",
    "            ham_word_counts[word] = count\n",
    "\n",
    "# perform classification with all words\n",
    "unique_words = set([k for k in spam_word_counts.keys()] + [k for k in ham_word_counts.keys()])\n",
    "total_msg_classified = 0\n",
    "total_msg_correct = 0\n",
    "tpc = 0\n",
    "fpc = 0\n",
    "tnc = 0\n",
    "fnc = 0\n",
    "\n",
    "cat = subprocess.Popen([\"hadoop\", \"fs\", \"-cat\", data], stdout=subprocess.PIPE)\n",
    "for line in cat.stdout:\n",
    "\tline = line.strip()\n",
    "\tlineArr = line.split('\\t')\n",
    "\t# check data integrity.  each valid line should have 4 fields, with second field equals either 0 or 1\n",
    "\tif len(lineArr) != 4 or (lineArr[1] != \"0\" and lineArr[1] != \"1\"): continue\n",
    "\tmsg_id = lineArr[0]\n",
    "\tspam_flag = lineArr[1]\n",
    "\tsubject_body = lineArr[2] + ' ' + lineArr[3]\n",
    "\t# remove punctuations\n",
    "\tsubject_body = subject_body.translate(string.maketrans(\"\",\"\"), string.punctuation).strip().lower()\n",
    "\tpredicted_results = classifyWithWordList(subject_body, unique_words)\n",
    "\tspam_prob = predicted_results[1]\n",
    "\tham_prob = predicted_results[2]\n",
    "\tnorm = spam_prob + ham_prob\n",
    "\t# calculate true probability by dividing by the normailzation number, so that spam_prob and ham_prob add up to one.\n",
    "\tif norm > 0:\n",
    "\t\tspam_prob = spam_prob / norm\n",
    "\t\tham_prob = ham_prob / norm\n",
    "\telse:\n",
    "\t\tspam_prob = 0\n",
    "\t\tham_prob = 0\n",
    "\tspam_flag_predicted = str(predicted_results[0])\n",
    "\ttotal_msg_classified += 1\n",
    "\tif spam_flag == \"1\":\n",
    "\t\tif spam_flag_predicted == \"1\": tpc += 1\n",
    "\t\telse: fnc += 1\n",
    "\telse:\n",
    "\t\tif spam_flag_predicted == \"1\": fpc += 1\n",
    "\t\telse: tnc += 1\n",
    "\tif spam_flag_predicted == spam_flag: total_msg_correct += 1\n",
    "\tprint '\\t'.join([msg_id, spam_flag, spam_flag_predicted, str(spam_prob), str(ham_prob)])\n",
    "print \"-----------------------------------------------------\"\n",
    "print \"feature size: \" + str(len(unique_words))\n",
    "print \"total document classified: \" + str(total_msg_classified)\n",
    "print \"total document correctly classified: \" + str(total_msg_correct)\n",
    "print \"Spam zero prob processed: %s\" % spam_zero_count_processed\n",
    "print \"Non-Spam zero prob processed: %s\" % ham_zero_count_processed\n",
    "print \"TPR: %.4f\"%(float(tpc)/float(tpc+fnc))\n",
    "print \"FPR: %.4f\"%(float(fpc)/float(fpc+tnc))\n",
    "print \"accuracy: %.4f\"%(float(total_msg_correct)/float(total_msg_classified))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/24 08:55:17 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /w261/hw2/hw2_3_output\n",
      "16/01/24 08:55:21 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "packageJobJar: [/data/w261/hw2/mapper.py, /data/w261/hw2/reducer.py] [/usr/lib/hadoop/lib/hadoop-streaming-2.6.0.jar] /tmp/streamjob292718487624004076.jar tmpDir=null\n",
      "16/01/24 08:55:28 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/24 08:55:29 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/24 08:55:32 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/24 08:55:32 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/01/24 08:55:34 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1453604265894_0024\n",
      "16/01/24 08:55:35 INFO impl.YarnClientImpl: Submitted application application_1453604265894_0024\n",
      "16/01/24 08:55:35 INFO mapreduce.Job: The url to track the job: http://ip-172-31-55-170.ec2.internal:8088/proxy/application_1453604265894_0024/\n",
      "16/01/24 08:55:35 INFO mapreduce.Job: Running job: job_1453604265894_0024\n",
      "16/01/24 08:55:58 INFO mapreduce.Job: Job job_1453604265894_0024 running in uber mode : false\n",
      "16/01/24 08:55:58 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/01/24 08:56:29 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "16/01/24 08:56:31 INFO mapreduce.Job:  map 83% reduce 0%\n",
      "16/01/24 08:56:33 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/24 08:56:50 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/24 08:57:42 INFO mapreduce.Job: Job job_1453604265894_0024 completed successfully\n",
      "16/01/24 08:57:43 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=111973\n",
      "\t\tFILE: Number of bytes written=565990\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=216879\n",
      "\t\tHDFS: Number of bytes written=4015\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=63155\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=67326\n",
      "\t\tTotal time spent by all map tasks (ms)=63155\n",
      "\t\tTotal time spent by all reduce tasks (ms)=67326\n",
      "\t\tTotal vcore-seconds taken by all map tasks=63155\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=67326\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=64670720\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=68941824\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=101\n",
      "\t\tMap output records=8280\n",
      "\t\tMap output bytes=95407\n",
      "\t\tMap output materialized bytes=111979\n",
      "\t\tInput split bytes=200\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=5664\n",
      "\t\tReduce shuffle bytes=111979\n",
      "\t\tReduce input records=8280\n",
      "\t\tReduce output records=107\n",
      "\t\tSpilled Records=16560\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=1318\n",
      "\t\tCPU time spent (ms)=10680\n",
      "\t\tPhysical memory (bytes) snapshot=552763392\n",
      "\t\tVirtual memory (bytes) snapshot=7595245568\n",
      "\t\tTotal committed heap usage (bytes)=392372224\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=216679\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=4015\n",
      "16/01/24 08:57:43 INFO streaming.StreamJob: Output directory: /w261/hw2/hw2_3_output\n"
     ]
    }
   ],
   "source": [
    "!cp mapper.py /data/w261/hw2/mapper.py; cp reducer.py /data/w261/hw2/reducer.py;\n",
    "!chmod +x /data/w261/hw2/mapper.py; chmod +x /data/w261/hw2/reducer.py;\n",
    "!hdfs dfs -rm -r /w261/hw2/hw2_3_output;\n",
    "!$HADOOP_HOME/bin/hadoop jar $HADOOP_HOME/lib/hadoop-*streaming*.jar -mapper mapper.py -file /data/w261/hw2/mapper.py -reducer reducer.py -file /data/w261/hw2/reducer.py -input /w261/hw2/enronemail_1h.txt -output /w261/hw2/hw2_3_output;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0017.2003-12-18.GP\t1\t1\t0.999614803486\t0.000385196513989\r\n",
      "0017.2004-08-01.BG\t1\t0\t0\t0\r\n",
      "0017.2004-08-02.BG\t1\t1\t0\t0\r\n",
      "0018.1999-12-14.kaminski\t0\t0\t0\t0\r\n",
      "0018.2001-07-13.SA_and_HP\t1\t1\t0\t0\r\n",
      "0018.2003-12-18.GP\t1\t1\t0\t0\r\n",
      "-----------------------------------------------------\t\r\n",
      "feature size: 5663\t\r\n",
      "total document classified: 98\t\r\n",
      "total document correctly classified: 92\t\r\n",
      "Spam zero prob processed: 181300\t\r\n",
      "Non-Spam zero prob processed: 276850\t\r\n",
      "TPR: 0.8837\t\r\n",
      "FPR: 0.0182\t\r\n",
      "accuracy: 0.9388\t\r\n"
     ]
    }
   ],
   "source": [
    "# prediction result summary\n",
    "!hdfs dfs -cat /w261/hw2/hw2_3_output/part-00000 | tail -15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Accuracy is 94%.  Training Error is 6.0%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# extract posterior probs\n",
    "import subprocess\n",
    "spam_posterior = []\n",
    "ham_posterior = []\n",
    "\n",
    "# grab the output from HDFS\n",
    "cat = subprocess.Popen([\"hadoop\", \"fs\", \"-cat\", '/w261/hw2/hw2_3_output/part-00000'], stdout=subprocess.PIPE)\n",
    "for line in cat.stdout:\n",
    "    line = line.strip()\n",
    "    if line == '': continue\n",
    "    lineArr = line.split('\\t')\n",
    "    if len(lineArr) != 5: continue\n",
    "    spam_posterior.append(float(lineArr[3]))\n",
    "    ham_posterior.append(float(lineArr[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfAAAAEKCAYAAADtpQeZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nWQX9f5aN2wJKA3qKAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x3987d90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(spam_posterior)\n",
    "plt.title(\"Posterior distribution for Spam Msg\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(ham_posterior)\n",
    "plt.title(\"Posterior distribution for Non-Spam Msg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, because we are using training set to make predictions, it's very easy for the model to be over-confident.  Therefore we have lots of extreme probabilities, and few in the middle-ground."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <b>HW2.4 Repeat HW2.3 with the following modification: use Laplace plus-one smoothing. Compare the misclassifcation error rates for 2.3 versus 2.4 and explain the differences.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Kuan Lin\n",
    "## Description: reducer code for HW1.5\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import math\n",
    "import string\n",
    "import subprocess\n",
    "\n",
    "files = sys.argv[1:]\n",
    "data=\"/w261/hw2/enronemail_1h.txt\"\n",
    "\n",
    "spam_word_counts = {}\n",
    "ham_word_counts = {}\n",
    "spam_total_words = 0\n",
    "ham_total_words = 0\n",
    "unique_words = set()\n",
    "spam_zero_count_processed = 0\n",
    "ham_zero_count_processed = 0\n",
    "\n",
    "# assume equal prior for spam and ham\n",
    "spam_prior = 0.44\n",
    "ham_prior = 0.56\n",
    "\n",
    "def findWordCount(doc, word):\n",
    "\tcount = 0\n",
    "\tfor w in doc.split(' '):\n",
    "\t\tif w == word: count += 1\n",
    "\treturn count\n",
    "\n",
    "def spamWordProb(word):\n",
    "\tcount = 0\n",
    "\tif word in spam_word_counts: count = spam_word_counts[word]\n",
    "\t# Applying laplace smoothing\n",
    "\treturn float(count+1)/float(spam_total_words+len(unique_words))\n",
    "\t#return float(count)/float(spam_total_words)\n",
    "\t\n",
    "def hamWordProb(word):\n",
    "\tcount = 0\n",
    "\tif word in ham_word_counts: count = ham_word_counts[word]\n",
    "\t# Applying laplace smoothing\n",
    "\treturn float(count+1)/float(ham_total_words+len(unique_words))\n",
    "\t#return float(count)/float(ham_total_words)\n",
    "\t\n",
    "def classifyWithWordList(document, word_list):\n",
    "\tglobal spam_zero_count_processed\n",
    "\tglobal ham_zero_count_processed\n",
    "\tspam_logprob = math.log(spam_prior)\n",
    "\tham_logprob = math.log(ham_prior)\n",
    "\tfor word in word_list:\n",
    "\t\t#pattern = re.compile('[\\s+]?(' + word + ')[\\s+]?')\n",
    "\t\tword_count = findWordCount(document, word)\n",
    "\t\tspam_word_prob = spamWordProb(word)\n",
    "\t\tham_word_prob = hamWordProb(word)\n",
    "\t\tif spam_word_prob > 0:\n",
    "\t\t\tspam_logprob += math.log(spam_word_prob)*word_count\n",
    "\t\telse:\n",
    "\t\t\tspam_zero_count_processed += 1\n",
    "\t\t\tspam_logprob += math.log(0.0001)*word_count\n",
    "\t\tif ham_word_prob > 0:\n",
    "\t\t\tham_logprob += math.log(ham_word_prob)*word_count\n",
    "\t\telse:\n",
    "\t\t\tham_zero_count_processed += 1\n",
    "\t\t\tham_logprob += math.log(0.0001)*word_count\n",
    "\t# recover probability by taking exponential\n",
    "\tspam_prob = math.e**spam_logprob\n",
    "\tham_prob = math.e**ham_logprob\n",
    "\tif spam_logprob > ham_logprob:\n",
    "\t\treturn (1, spam_prob, ham_prob)\n",
    "\telse:\n",
    "\t\treturn (0, spam_prob, ham_prob)\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    if line == \"\": continue\n",
    "\n",
    "    lineArr = line.split('\\t')\n",
    "    word = lineArr[0]\n",
    "    count = int(lineArr[1])\n",
    "    spam_flag = int(lineArr[2])\n",
    "\n",
    "    if spam_flag == 1:\n",
    "        if word == \"class_total_words\":\n",
    "            spam_total_words += count\n",
    "            continue\n",
    "        if word in spam_word_counts:\n",
    "            spam_word_counts[word] += count\n",
    "        else:\n",
    "            spam_word_counts[word] = count\n",
    "    else:\n",
    "        if word == \"class_total_words\":\n",
    "            ham_total_words += count\n",
    "            continue\n",
    "        if word in ham_word_counts:\n",
    "            ham_word_counts[word] += count\n",
    "        else:\n",
    "            ham_word_counts[word] = count\n",
    "\n",
    "# perform classification with all words\n",
    "unique_words = set([k for k in spam_word_counts.keys()] + [k for k in ham_word_counts.keys()])\n",
    "total_msg_classified = 0\n",
    "total_msg_correct = 0\n",
    "tpc = 0\n",
    "fpc = 0\n",
    "tnc = 0\n",
    "fnc = 0\n",
    "\n",
    "cat = subprocess.Popen([\"hadoop\", \"fs\", \"-cat\", data], stdout=subprocess.PIPE)\n",
    "for line in cat.stdout:\n",
    "\tline = line.strip()\n",
    "\tlineArr = line.split('\\t')\n",
    "\t# check data integrity.  each valid line should have 4 fields, with second field equals either 0 or 1\n",
    "\tif len(lineArr) != 4 or (lineArr[1] != \"0\" and lineArr[1] != \"1\"): continue\n",
    "\tmsg_id = lineArr[0]\n",
    "\tspam_flag = lineArr[1]\n",
    "\tsubject_body = lineArr[2] + ' ' + lineArr[3]\n",
    "\t# remove punctuations\n",
    "\tsubject_body = subject_body.translate(string.maketrans(\"\",\"\"), string.punctuation).strip().lower()\n",
    "\tpredicted_results = classifyWithWordList(subject_body, unique_words)\n",
    "\tspam_prob = predicted_results[1]\n",
    "\tham_prob = predicted_results[2]\n",
    "\tnorm = spam_prob + ham_prob\n",
    "\tif norm > 0:\n",
    "\t\tspam_prob = spam_prob / norm\n",
    "\t\tham_prob = ham_prob / norm\n",
    "\telse:\n",
    "\t\tspam_prob = 0\n",
    "\t\tham_prob = 0\n",
    "\tspam_flag_predicted = str(predicted_results[0])\n",
    "\ttotal_msg_classified += 1\n",
    "\tif spam_flag == \"1\":\n",
    "\t\tif spam_flag_predicted == \"1\": tpc += 1\n",
    "\t\telse: fnc += 1\n",
    "\telse:\n",
    "\t\tif spam_flag_predicted == \"1\": fpc += 1\n",
    "\t\telse: tnc += 1\n",
    "\tif spam_flag_predicted == spam_flag: total_msg_correct += 1\n",
    "\tprint '\\t'.join([msg_id, spam_flag, spam_flag_predicted, str(spam_prob), str(ham_prob)])\n",
    "print \"-----------------------------------------------------\"\n",
    "print \"feature size: \" + str(len(unique_words))\n",
    "print \"total document classified: \" + str(total_msg_classified)\n",
    "print \"total document correctly classified: \" + str(total_msg_correct)\n",
    "print \"Spam zero prob processed: %s\" % spam_zero_count_processed\n",
    "print \"Non-Spam zero prob processed: %s\" % ham_zero_count_processed\n",
    "print \"TPR: %.4f\"%(float(tpc)/float(tpc+fnc))\n",
    "print \"FPR: %.4f\"%(float(fpc)/float(fpc+tnc))\n",
    "print \"accuracy: %.4f\"%(float(total_msg_correct)/float(total_msg_classified))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/w261/hw2/hw2_4_output': No such file or directory\n",
      "16/01/24 09:32:17 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "packageJobJar: [/data/w261/hw2/mapper.py, /data/w261/hw2/reducer.py] [/usr/lib/hadoop/lib/hadoop-streaming-2.6.0.jar] /tmp/streamjob3103003523537383682.jar tmpDir=null\n",
      "16/01/24 09:32:24 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/24 09:32:26 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/24 09:32:29 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/24 09:32:29 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/01/24 09:32:30 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1453604265894_0025\n",
      "16/01/24 09:32:32 INFO impl.YarnClientImpl: Submitted application application_1453604265894_0025\n",
      "16/01/24 09:32:32 INFO mapreduce.Job: The url to track the job: http://ip-172-31-55-170.ec2.internal:8088/proxy/application_1453604265894_0025/\n",
      "16/01/24 09:32:32 INFO mapreduce.Job: Running job: job_1453604265894_0025\n",
      "16/01/24 09:32:56 INFO mapreduce.Job: Job job_1453604265894_0025 running in uber mode : false\n",
      "16/01/24 09:32:56 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/01/24 09:33:26 INFO mapreduce.Job:  map 33% reduce 0%\n",
      "16/01/24 09:33:27 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "16/01/24 09:33:29 INFO mapreduce.Job:  map 83% reduce 0%\n",
      "16/01/24 09:33:32 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/24 09:33:50 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/24 09:35:41 INFO mapreduce.Job: Job job_1453604265894_0025 completed successfully\n",
      "16/01/24 09:35:41 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=111973\n",
      "\t\tFILE: Number of bytes written=565993\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=216879\n",
      "\t\tHDFS: Number of bytes written=3961\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=62558\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=128555\n",
      "\t\tTotal time spent by all map tasks (ms)=62558\n",
      "\t\tTotal time spent by all reduce tasks (ms)=128555\n",
      "\t\tTotal vcore-seconds taken by all map tasks=62558\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=128555\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=64059392\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=131640320\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=101\n",
      "\t\tMap output records=8280\n",
      "\t\tMap output bytes=95407\n",
      "\t\tMap output materialized bytes=111979\n",
      "\t\tInput split bytes=200\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=5664\n",
      "\t\tReduce shuffle bytes=111979\n",
      "\t\tReduce input records=8280\n",
      "\t\tReduce output records=107\n",
      "\t\tSpilled Records=16560\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=998\n",
      "\t\tCPU time spent (ms)=8950\n",
      "\t\tPhysical memory (bytes) snapshot=550674432\n",
      "\t\tVirtual memory (bytes) snapshot=7589036032\n",
      "\t\tTotal committed heap usage (bytes)=392372224\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=216679\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=3961\n",
      "16/01/24 09:35:41 INFO streaming.StreamJob: Output directory: /w261/hw2/hw2_4_output\n"
     ]
    }
   ],
   "source": [
    "!cp mapper.py /data/w261/hw2/mapper.py; cp reducer.py /data/w261/hw2/reducer.py;\n",
    "!chmod +x /data/w261/hw2/mapper.py; chmod +x /data/w261/hw2/reducer.py;\n",
    "!hdfs dfs -rm -r /w261/hw2/hw2_4_output;\n",
    "!$HADOOP_HOME/bin/hadoop jar $HADOOP_HOME/lib/hadoop-*streaming*.jar -mapper mapper.py -file /data/w261/hw2/mapper.py -reducer reducer.py -file /data/w261/hw2/reducer.py -input /w261/hw2/enronemail_1h.txt -output /w261/hw2/hw2_4_output;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0017.2003-12-18.GP\t1\t1\t0.999999529496\t4.70503735935e-07\r\n",
      "0017.2004-08-01.BG\t1\t1\t0\t0\r\n",
      "0017.2004-08-02.BG\t1\t1\t0\t0\r\n",
      "0018.1999-12-14.kaminski\t0\t0\t0\t0\r\n",
      "0018.2001-07-13.SA_and_HP\t1\t1\t0\t0\r\n",
      "0018.2003-12-18.GP\t1\t1\t0\t0\r\n",
      "-----------------------------------------------------\t\r\n",
      "feature size: 5663\t\r\n",
      "total document classified: 98\t\r\n",
      "total document correctly classified: 98\t\r\n",
      "Spam zero prob processed: 0\t\r\n",
      "Non-Spam zero prob processed: 0\t\r\n",
      "TPR: 1.0000\t\r\n",
      "FPR: 0.0000\t\r\n",
      "accuracy: 1.0000\t\r\n"
     ]
    }
   ],
   "source": [
    "# prediction result summary\n",
    "!hdfs dfs -cat /w261/hw2/hw2_4_output/part-00000 | tail -15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Accuracy is 100% with Laplace Smoothing.  Training Error is 0%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extract posterior probs\n",
    "import subprocess\n",
    "spam_posterior = []\n",
    "ham_posterior = []\n",
    "\n",
    "# grab output from hdfs\n",
    "cat = subprocess.Popen([\"hadoop\", \"fs\", \"-cat\", '/w261/hw2/hw2_4_output/part-00000'], stdout=subprocess.PIPE)\n",
    "for line in cat.stdout:\n",
    "    line = line.strip()\n",
    "    if line == '': continue\n",
    "    lineArr = line.split('\\t')\n",
    "    if len(lineArr) != 5: continue\n",
    "    spam_posterior.append(float(lineArr[3]))\n",
    "    ham_posterior.append(float(lineArr[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfAAAAEKCAYAAADtpQeZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xu8HWV97/HPN0FJws2QBFKIkIJ4RW6eUsALS5FTNFHp\nqUIFIopWehSl9YYiwt5YX622SkRp7fH2EmKQu1hNIm1xo5GTIxAkgagoBZIobCDBBAGRkt/543l2\nMlnZl9l7r9vs9X2/Xvu1Z2bNmvmtWfPMbz0zzzyjiMDMzMyqZVK7AzAzM7PRcwI3MzOrICdwMzOz\nCnICNzMzqyAncDMzswpyAjczM6ugrkngkh6TNLdJyz5W0rrC+J2SXtWgZZ8iaVlhfIukAxqx7Ly8\npmwXSS+RtCYv/6xGL9+qz2VyyOW7TFopDU/gku6T9ISkzZI2SlosafdxLG//XEDGFWtE7BYR941n\nGSOtorCugyPih8PNXPZzRcTiiDhhsPWMlqQfSDqjbvnN2i4fAZbm5X9xvAuTtKekb0p6NO9bd0v6\naAPiHG9cx+bv8Zq66Yfk6Te2K7ZCLC6TLpPQ+DLZk7fXmwvTJudp+413+YOsb4qkSyQ9lH+E3Cfp\n841ezxjiGthvbqubPkPSHyT9V7PW3YwaeADzImJ34CXAS4HecSxPeZka05ulyeNY97jfP9yiGeFz\nDbHuMW2HNpgD3DWWNw7xuS8hba/98751ArBm7OE11MPA0ZKmF6adDvyiTfHUc5ksuWhcJgc1xOcO\nYAPQK0l105uhB3gB8NKI2A04Gri5Sesai2mSXlwYPwW4p6lrjIiG/gH3Aq8pjH8GWJaH9wduAB4D\n1gFnF+Y7BrgDeBzoBy7K0+8Hnsnv2Qz8aZ7+PuA+YBPQBxxYWNYW4D3Az4FfFaYdkIenA1fn9z4I\nfKrw3tOB5cDngIeA3kE+4zTgqhzTauBDwNrBtgHw8rKfq27d/aSD7OnAj+o+2/uAX+b4vwgov3YB\ncFlh3v3z/JOAvwP+G3gir+/iMWyXHwH/SCq064E3DbEP/Gde15N5Xc9rwDb/BTB/mP2ufrt8obBd\nDgB+CGwEfgtcAzyn7vv6EPDTHO9XgL2AJfn7+REwfYj1Hkval/8ZeE+eNilvn/OAGwvT/jnHsAm4\nE3hJfm2vvM02A/8P+GTxO3eZdJkcZru0s0xeACwCbgfelqdNzvHv18j48/zfB84aYT//aP7+NwOX\nA1Pya88BlgGP5Nf+nVQZGHjvD0jlbnn+/q8HZuTPtwlYBfzxEOsd+E7PBT5TmH4L8DHgvwrTevJ+\n9BhwN9v2yeL+eyfwYWDdiGW7UQeJIQrKc3Mwn87jtwL/lHfeFwAPkGoGA6+dmod3Bo4obJxnyAUi\nTzuFdECfm8fPAW6vK1DfAXYFnp2nPcO2QnE18K28ntmkX6XvLexUfwDOyOPPHuQzfj7vALsAs/IO\nPNTBYjSfa4d152k/rPtsS/O698qxn1UoUJfW7VjPAJMKO+kZdZ9lNNvlKbYV1L8GHhpmP9huXQ3Y\n5t8gFaK3AQcN8vpw2+UA4JV5eI/83f1L3fe1nFTI/4i0X94GvBB4FinB/d0Qn/NYYC1wFLAiT3td\njuWdbEvg80kFerc8fiCwVx6+Hvg6sFOO9d7id+4y6TI5zHZpZ5m8ALiUtG/fQ0re9Qm8kfH3kn4g\nngkcXPyeCt/xSmAmaT+7Efin/Np0YB5pP5+S415Wt21+AewL7EY61txN+rEnUvlcNERcA9/pfqRj\ngYAXk84QHkdO4DnmtcDeeXwf8o8I0v77fVIin0naR9cOtS22rrtRB4m6jbiZVNP4DfB/clDPA34P\nTK37Qhbn4ZuA84E9h9g4kwrTbgTeURifRPo1fVChQB1Tt5wtpIPjlLxjHlB47R3AzYWd6pcjfMZf\nA7W6Qj7UwaJvFJ9rh3Uz+MGiuO53kmsDjO1gMZrtcnfhtan5vfuOdLBo0DafCnyCVECfytv4DWW2\nyyDLmgfcVfd9vbUwfiVwSWH8PcC/DbGsYwe+e9IB4PmkX/5vZfsE/mpS7fNItk8QU4Cn2b42cD6N\nT+Auky6TjS6TWz8bsIKUWLcm8CbEPwk4m3Ta/ElSjf5ddd/x2wvjxzFELZaUTB+v2zYfK4x/Bvhe\nYfz1wOohlrX1OyX92P+fwN+Tat/FBH5gjvk1wE6D7L/HFsbfRokE3qxW6G+KiD0jYp+IeHdEPAHs\nDWyIiCcL863N0wH+inR97ueSbpN04jDLnwN8PjfI2Ug6/RKkX94DHhzivTNIO9m6wrRiHJBqIcPZ\ni7TBB6wfZt53U/5zlVk3g6x776FmHIUy22XrNi18jzs3aNnDfu6IeDIiPhkRR5B+TS8CrpQ0szDb\noNtF0r6SrpHUL+lRUo1g17pV9BeGnxpkvMznvAw4C6gB19XF/wPgS/nvIUlfzQ3JZpAK/m/qYm80\nl8ltXCYbs82LzgM+Tkrao1nHoPFLekVuqLZZ0ur8+paI+HxEHAPsTjod/a+SXlpY3lDHgN0lfV3S\nr/Mx4MfAlLpr9406Brwd+Ms8vFVE3AN8kHSqvl/SVZL2zS/vxRiOAc1K4IM16ugHZkgqfsHPzdOJ\niLsj4uSI2It0begKSbsyeIOIB0i/JPfMf9MjYteIKNOgYQPbTncM2I/tv6yR9JNOtRQ/x6BG+bnK\nql/3QOx/INWsBhSTGyOssxHbpSXLzsnnU6QCdWDhpaG2yz+Qri0dGBHTSYWrGQ2PFpFq69+LiN8P\nEvfC/APkhcAfk67XbSDVOvYpzDqnCbG5TGYuk41fdkT8B/Ar0v4/8JnGvI6IWB6ptfzuEfHSQV5/\nOiK+RGpAWmw4NtT38JH82iH5GHBMnt7o48A1pDN890TEDkk4Ir4ZES8nbYc/kK7/Q2pnUDwGDLn/\nFrXsPvCI+BXp2tsn860GLyD9El4MIOkkSc/Jsz9GOqgFqdFRkA54A74MnCvpefm9u0p6U8k4fk+6\nFvfJfFvCHwEfGIijpGuAj+X17kVqwDKoUX6usj6c17036ZTSFXn6HcCrJD1X0i6k65BFG4G5gy2w\nQdtlUI1YtqRzJB2ah3cmfe7fkU5LDxhqu+xC+gX9RH7tQ+P9TIOJdOvPq0i1kfr4j5B0eP7F//sc\nz5a8bZYA50t6ltL9xG9vRnyDxOsy6TLZyGWfR0qUTVmHpPdIOibvq5MlnQbsSbqsNuAsSbMk7Ub6\ngTzwPUwjXar6XT7zdf5YYhguPNhauXg16exVffwH5TMLk3Msv2fbj52rgY/mfWgW8N4yK23WbWRD\neTNwOKmg/CepIc1382snAr+U9DtSK84FEfF4RGwmtYS8NZ+eOzIiFgH/CiyVtIl0EP/zEWIoTjuT\nVHvrJ7U8/rcY3X2R55IK/gP5c9TvkMV1lf5cJdcdpAYzPyU1sriJdIsVEbGE1CDq56RGWN+ve+8X\ngAWSfitp4SCxjna7DPdd17823m0+Bbg8b8dHgDeQGlttKswz6HYhnWo7mnQdeAnpoDJcrGOuiUXE\nzREx2Kni6aRTaptJp8c2s+3X95mkg/gG0r50Oanm0iguky6Tg7023m2+/cLT2Zaf0Lz4t5C2629J\nLcM/AJwcEb8szHMVqT3Gr0nHiU/k6ReRGqk+Srpe/x+jWG8ZW98fESsj4t5B5pkCLMwxPEw6IzDw\ng+7jpDYjD+T4r6HEMWDgVofhZ0odZpxBqjV8OSIuVrrn9QrSNYYHSBty0zCLMWsaSVuA50VE0zpN\naBVJF5Iaf721CcvuJTWwe4ZU+34bqQwvJrULuIuU0P670es2ayZJ9wLvjIi2d540Xkqd+/xVRBw9\n3Hwj1sAlHQGcSur84TBgfm400AssiYhDSffXXTjuqM26kKTn59PXSDqEdBr7+ias50BgAXBwRLyI\nVKM5BbiYVPM+hFRTcjebZi0kabakP8nDc0mX+UY8BpQ5hf5C0v2tT0XEM6QOMd5AalY/0MpuEenC\nvVm7jPcUWDvtQTr1/BjpFOuXIuJbTVjPRlLDmV0k7US6bed+4KiIGDhYLCLd12tWNVU+BjwbuDQf\nA24l3db22ZHetFOJBa8GevIp86dIiXsVMCsiNgBExCP5wrtZW0REs7rXbLqIuIV032+z1/OopM+S\nbuV5gnTP6l2ka4UD1rN9S16zSoiIppehZomItcCLRvu+EWvgEbGa1LDjJtLF9VVU+5eOWVfKLdz/\nltTxxD6k1vmvbWtQZjZmZWrg5PvtvgQg6QJSK8CHJc2IiA1KnWk8NNh7JTnZm5UUEc18MMaRwI8j\nYiOApOtIt70V702ewxCdSLgsm5XX5LIMlLyNTNKM/H82cBKpJ6slpAYx5P9Lh3p/jKH7x1b9XXDB\nBW2PwTE6voiW5MZ7gKMkTc33ox9Hur1phbb1RnYaFS3LVfieOz0+x9iYv1YpVQMHvp1vjH+a1BF9\nv6QeUg9GZ5C6wzupSTGaWQNExC2SriZdBnuGdF/uJcC1wOJ8+9oa0pOQzKzDlT2F/spBpm0Ejm94\nRGbWNBHRy47PAr+X1NGNmVVI2Rr4uEydukfDlvXxj5/Heec1roJQq9UatqxmcYzj1+nxWWN0+vfc\n6fGBY6ySUj2xjWsFUqQ2b41wGW95y0quvPJrDVqeWeeQRLSg4ctYSYpWXt8zq6pWleWW1MBTPxWN\nMG3kWczMzLpAy55GZmZmZo3jBG5mZlZBTuBmZmYV1KJr4GY2EbzsZcc1bFl77z2Da6+9lClTpjRs\nmWbdxAnczEpbufLchi1r551PYsOGDey7r5+dYjYWTuBmNgqNq4FPnuyat9l4+Bq4mZlZBTmBm5mZ\nVZATuJmZWQU5gZuZmVWQE7iZmVkFOYGbmZlVkBO4mZlZBZVK4JJ6Jd0t6WeSrpI0VdJcSTdLWiXp\nckm+p9zMzKxFRkzgkg4EFgAHR8SLgC3AKcDFwKcj4hCgHzirmYGamZnZNmVq4BuBPwC75Fr2VOB+\n4KiIuD7PswiY35wQzczMrN6ICTwiHgU+C6wFfg1sAu4CHinMth5wh8ZmHU7S8yXdLmll/r9J0vsl\nTZd0g6Q7JC2TtEe7YzWz4Y143VrSAcDfAvuTkvdVwGtHt5qewnAt/5l1t76+Pvr6+lq6zoi4Gzgc\nQNIk0o/v64BeYElELJT0N8CFwNktDc7MRqVMw7MjgR9HxEYASdcBrwJmFuaZQzoQDKFnzAGaTVS1\nWo1arbZ1vLe3t9UhvBa4JyLWSZpHKuuQLomtwAncrKOVuQZ+D3BUbnku0uOIfg6skHRinuc0YGmT\nYjSz5jgZWJyHZ0XEBoCIeASY1baozKyUMtfAbwGuBlYBPwOmAJeQfp2fI2kVMBv4QhPjNLMGkvQs\n4I2kS2IA0cZwzGwMSt27HRG9pGtkRfcCRzc8IjNrhdcBt+XaNsDDkmZExAZJM4GHBn9bT2G4htuz\nmLWnPQuUTOBmNuG8Fbi8ML6E1N/Dwvx/iEtiPU0Oy6x62tWexQncrMtImkZqwPbuwuQe4ApJZwAP\nAie1ITQzGwUncLMuExFPUNdILd9lcnx7IjKzsfDDTMzMzCrICdzMzKyCnMDNzMwqyAnczMysgpzA\nzczMKsgJ3MzMrIKcwM3MzCrICdzMzKyCnMDNzMwqyAnczMysgpzAzczMKsgJ3MzMrIKcwM3MzCpo\nxAQu6fmSbpe0Mv/fJOn9kqZLukHSHZKWSdqjFQGbmZlZiQQeEXdHxOERcQTwMuBx4DqgF1gSEYcC\ny4ALmxqpmZmZbTXaU+ivBe6JiHXAPOCyPH1RHjczM7MWGG0CPxlYnIdnRcQGgIh4BJjVyMDMzMxs\naDuVnVHSs4A3AufkSVF+NT2F4Vr+M+tufX199PX1tTsMM6uo0gkceB1wW65tAzwsaUZEbJA0E3ho\n6Lf2jDlAs4mqVqtRq9W2jvf29rYvGDOrnNGcQn8rcHlhfAmwIA8vAJY2Kigzaw5Je0i6Mt89skbS\nUb6jxKyaSiVwSdNIDdiuLUzuAeZJWkWqnZ/f8OjMrNG+DFyb7x45GFiD7ygxq6RSp9Aj4gnqGqlF\nxEbg+GYEZWaNJ2lP4LCIOAkgIrYAmyXNA47Msy0CVgBntydKMyvLPbGZdY+DgEfyKfQ7JX1D0q74\njhKzShpNIzYzq7ZJwJ8A74+IWyVdBHwC31FiNi7tuqPECdyse6wD1kfErXn8GlIC9x0lZuPQrjtK\nfArdrEtExHrSKfSD8qTjgJ/hO0rMKsk1cLPu8i5gsaSpwFrgVEDAFZLOAB4ETmpjfGZWkhO4WReJ\niDtI18Hr+Y4Ss4rxKXQzM7MKcgI3MzOrICdwMzOzCnICNzMzqyAncDMzswpyAjczM6sgJ3AzM7MK\ncgI3MzOrICdwMzOzCnICNzMzq6BSCVzSHvkZwndIWiPpKEnTJd2Qpy2TtEezgzUzM7OkbA38y8C1\nEXEocDCwBugFluRpy4ALmxOimZmZ1RsxgUvaEzgsIr4FEBFbImIzMA+4LM+2KI+bmZlZC5SpgR9E\neobwlZLulPQNSbsCsyJiA0BEPALMamagZmZmtk2Zx4lOIj1+8P0Rcauki4BPAFF+NT2F4Vr+M+tu\nfX199PX1tTsMM6uoMgl8HbA+Im7N49eQEvjDkmZExAZJM4GHhl5EzzjDNJt4arUatVpt63hvb2/7\ngjGzyhnxFHpErCedQj8oTzoO+BmwBFiQpy0AljYlQjMzM9tBmRo4wLuAxZKmAmuBUwEBV0g6A3gQ\nOKk5IZqZmVm9Ugk8Iu4gXQevd3xjwzEzM7MyytbAzWyCkHQfsAnYAjwdEUdKmg5cAewNPACcHBGb\n2helmY3EXamadZ8tQC0iDo+II/M0d8xkVjFO4GbdR+xY9t0xk1nFOIGbdZ8twMBzDN6bp7ljJrOK\n8TVws+5zdEQ8JGkWsFTSLyjdMVNPYbiGO2Uya1+nTE7gZl0mIh7K/x+WdA3pDpOSHTP1tCpMs8po\nV6dMPoVu1kUkTcv9OSBpF+AE4C7cMZNZ5bgGbtZd9ga+LWkLMA34VkR8R9Jy3DGTWaU4gZt1kYi4\nFzh0kOkbccdMZpXiU+hmZmYV5ARuZmZWQU7gZmZmFeQEbmZmVkFO4GZmZhXkBG5mZlZBpW4j8+MH\nzczMOkvZGrgfP2hmZtZByiZwP37QzMysg4ymBu7HD5qZmXWIsl2pjuPxg2ZmZtZopRL4+B4/CH6G\nsNmO2vUMYTObGEZM4JKmARERTxYeP/hZtj1+cCEjPn6wpwGhmk0s7XqGsJlNDGVq4H78oJmZWYcZ\nMYH78YNmZmadxz2xmZmZVZATuJmZWQU5gZuZmVWQE7iZmVkFOYGbdRlJkyStlPSdPD5X0s2SVkm6\nXFLZDp7MrI2cwM26z9nAmsL4xcCnI+IQoB84qy1RmdmoOIGbdRFJc4DXA1/J45NJXSVfn2dZBMxv\nU3hmNgpO4Gbd5SLgw2x7lsFewMOF19cD+7Y6KDMbPV/rMusSkuYB/RHxU0m14kvll9JTGK7h5xqY\nte+5Bk7gZt3j5cAbJb0emArsBnwGmFGYZw6pFj6EnuZFZ1ZR7XqugU+hm3WJiDg3IvaLiAOAvwRu\njIgFwApJb8qzncawDyYys07hBG5mZwMflbQKmA18oc3xmFkJPoVu1oUi4ibgpjx8L3B0eyMys9Fy\nDdzMzKyCnMDNzMwqyAnczMysgkoncPefbGZm1jlGUwN3/8lmZmYdolQCd//JZmZmnaVsDdz9J5uZ\nmXWQERN4sf9ktu8zeRT9J5uZmVkjlWl41uD+k2v4AQhm7XsAgplNDIqIkecamFk6FvhgRLwxt0b/\nakRcL2khsDYiPjfIe2Lbmffx+hpvectyrrzyaw1anlnnkEREdOyZrcaWZZg2bV/uvvsn7Luvr77Z\nxNKqsjye+8Ddf7KZmVmbjOrebfefbGZm1hncE5uZmVkFOYGbmZlVkBO4mZlZBTmBm5mZVZATuJmZ\nWQU5gZuZmVWQE7iZmVkFOYGbdRFJO0u6RdJKSb+Q9Lk8fa6kmyWtknS5pFH1EWFmrecEbtZFIuIp\n4FURcQTwYuAYSa8GLgY+HRGHAP3AWW0M08xKcAI36zIR8WQe3Jl0DOgHjoqI6/P0RcD8dsRmZuU5\ngZt1GUmTJN0OPAj0AY8CjxRmWQ/4CSNmHc7Xucy6TERsAQ6XtDvwfeCn5d/dUxiu4UcDm7Xv0cBO\n4GZdKiI2S1oCHADMLLw0h1QLH0RP0+Myq5parUatVts63tvb25L1+hS6WReRNEPSrnl4KnA8cDuw\nQtKJebbTgKVtCtHMSnIN3Ky77ANcKglgCrA4Ir4naQ2wWNKFwBrgw22M0cxKcAI36yIRsRo4fJDp\n9wJHtz4iMxurEU+hu+MHMzOzzjNiAnfHD2ZmZp2nVCM2d/xgZmbWWUolcHf8YGZm1llKXbceX8cP\n4M4fzHbUrs4fzGxiGFXDs7F1/ADu/MFsR+3q/MHMJoYyrdDd8YOZmVmHKVMDd8cPZmZmHWbEBO6O\nH8zMzDqP+0I3MzOrICdwMzOzCnICNzMzqyAncDMzswpyAjczM6sgJ3AzM7MKcgI3MzOrICdwMzOz\nCnICNzMzqyAncDMzswpyAjfrEpLmSLpJ0mpJP5f0kTx9uqQbJN0haZmkPdodq5mNzAncrHs8Dbw3\nIl4K/A/gnZIOAXqBJRFxKLAMuLCNMZq13OzZc5HUsL9WcQI36xIR0R8Rd+bh3wGrgTnAPOCyPNui\nPG7WNfr77weigX+t4QRu1oUkzSXVwn8EzIqIDQAR8Qgwq32RmVlZZZ4HbmYTiKRdgauAsyPiMUmj\nqDL0FIZr+c+s2/Xlv9ZyAjfrIpJ2Aq4GvhkR1+fJD0uaEREbJM0EHhp6CT1Nj9Gsemps/2O2tyVr\nHfEUuluumk0oXwPWRMTCwrQlwII8vABY2vKozGzUylwDd8tVswlA0suBU4HXSLpd0kpJJ5Cq1fMk\nrQJeB5zfxjDNrKQRT6FHRD/Qn4d/J6nYcvXIPNsiYAVwdpPiNLNxiogfA5OHePn4VsZiZuM3qlbo\nbrlqZmbWGUo3YnPLVbPG6uvro6+vr91hmFlFlUrgbrlq1ni1Wo1arbZ1vLe3NS1XzWxiKHsK3S1X\nzczMOsiINfBCy9XVkm4n9RN3LqlafYWkM4AHgZOaGKeZmZkVlGmF7parZmZmHcZ9oZuZmVWQE7iZ\nmVkFOYGbmZlVkBO4mZlZBTmBm5mZVZATuJmZWQU5gZuZmVWQE7iZmVkFOYGbmZlVkBO4mZlZBTmB\nm5mZVZATuJmZWQU5gZt1EUlfldQvaVVh2nRJN0i6Q9IySXu0M0YzK8cJ3Ky7fB34s7ppvcCSiDgU\nWAZc2PKozGzUnMDNukhELAcerZs8D7gsDy/K42bW4ZzAzWxmRGwAiIhHgFltjsfMShgxgfuamZmZ\nWefZqcQ8Xwe+AFxamDZwzWyhpL8hXTM7uwnxmVnzPSxpRkRskDQTeGjoWXsKw7X8Z9bt+vJfa42Y\nwCNiuaT96ybPA47Mw4uAFTiBm1WF8t+AJcACYGH+v3Tot/Y0MSyzqqqx/Y/Z3pastUwNfDDbXTOT\n5GtmZhUgaTHpSDND0lrggvx3paQzgAeBk9oXoZmVNdYEPko9heEaPu1mBn19ffT19bV0nRFxyhAv\nHd/SQMxs3MaawEdxzQx82s1sR7VajVqttnW8t7c1p93MbGIoexvZUNfMYMRrZmYTz+zZc5HU0D8z\ns9EYsQbua2ZmO+rvvx+IBi/VSdzMyivTCt3XzMzMzDqMe2IzMzOrICdwMzOzCnICNzMzqyAncDMz\nswpyAjczM6sgJ3AzM7MKcgI3MzOrICdwMzOzCnICNzMzqyAncDMzswpyAjczM6sgJ3AzM7MKcgI3\nMzOrICdwMzOzCnICNzMzq6BxJXBJJ0haLekuSec0Kigzaz2XZ7NqGXMCl/Rs4F+APwMOBd4s6bBG\nBdYqfX197Q5hRI5x/Do9vnZzeW6NTo8PHGOVjKcG/qfAnRHxm4j4b+AKYF5jwmqdKuwI3Rjj7Nlz\nkdSwv/nzT2xofBPQhCjP8+ef2ND9ZvbsuQ2NrxvLcjNUIcZW2Gkc750DrCuMrweOHV84Zkl///1A\nNGx5jz+uhi1rgpoQ5fnxxzfRyP2mv9/7jXWu8STw0nbf/Q0NWc7TT69l552PasiyzGz0GlWWAZ54\nYiOTJ09u2PLMuo0ixvZrVdIrgXMiYn4e/xCwc0R8qm6+xv0cNpvgIqItVb4y5dll2ay8VpTl8dTA\nfwK8RNI+wMPAycCZ9TO164BkZqMyYnl2WTbrLGNO4BHxlKT/DdwACLgsIlY2LDIzaxmXZ7PqGfMp\ndDMzM2ufhvXENlInEJKeLelbeZ7lkvZr1LobFN+H8murJd0kaW4r4ysTY2G+v5C0RdIRnRafpJMk\n3S7pDknfbGV8ZWKU9AJJKyTdmed5U4vj+6qkfkmrhpnn8zm22yQd3sr48vo7uiyXjLGt5bnTy3Je\nd0eXZ5flEiJi3H/As4F7gX1Ip+VvAQ6rm+cDwMI8fCJwfSPW3cD4XkFqtAPw18B1rYqvbIx5vl2B\nm4CbgSM6KT7gEGAFMC2P79lp2xC4DDgzD78IWNfiGF8BHAasGuL1/zWw7wGHAz/twG3YtrI8ihjb\nVp47vSyPYhu2rTy7LJf7a1QNvEwnEPPyBge4HjhaUqsaxYwYX0Qsj4in8uhy0o7TSmU70vgk8A/A\nU4O81kxl4nsHcElEPAEQERs7MMZ1wO55+DnA/S2Mj4hYDjw6zCzzgEV53tuByZL2bUVsWaeX5VIx\ntrk8d3pZhs4vzy7LJTQqgQ/WCcScoeaJ9JNkA7BXg9Y/kjLxFZ0JfKepEe1oxBjzKZg5EbG0lYFl\nZbbhC4HDJN2a/97YsuiSMjH+PXC6pHXAd4H3tSi2suo/w68Zfl9t9vo7rSxvt/6s08pzp5dl6Pzy\n7LJcQks6chlCR96SIulU4GV0WC9UuYbzOeD04uQ2hTOUScBc4EhgP+BmST+KiOF+pbba54CvRMRC\nSUeRfiG/pM0xVV2n7YdbdWJ5rkhZhs4vz11flhtVA19P+oIHzMnTitYBz4WtO/CepPtNW6FMfEh6\nLXAu8IbCogOqAAABh0lEQVSIeLpFsQ0YKcbdSDtnn6R7gaOA61vY+KXsd/ydiNgSEfcBa4AXtCY8\noFyMrwSuAoiIFcAUSa2sPY5kPbmcZIPuq01efyeXZej88tzpZRk6vzy7LJfRoIv5O7OtwcGzSA0O\njqib54PARXn4z0k7RqsaG5SJ73DgV8CBrYprtDHWzf8D4PBOii9/r1/PwzPzzjqrw2L8LnB6Hn4R\n8AAwucXf9Vxg9RCv/QVwbR4+Arij0/bDdpblUcTYtvLc6WV5FNuwbeXZZbnk+hv4QU4A7gTuAj6a\np/UC8wtfyJXAalKry7kt3tAjxffveQdYCdwOfLuV8ZWJsW7eG4c7KLQrPuCz+fWfAad12jYk1SD+\nb379LmBei+NbDPyG1HBpLamh0JnAuwvzfDHHtrLVB/aS27CtZblkjG0tz51elsvG2M7y7LI88p87\ncjEzM6ughnXkYmZmZq3jBG5mZlZBTuBmZmYV5ARuZmZWQU7gZmZmFeQEbmZmVkFO4GZmZhXkBG5m\nZlZB/x9XQ6iCWy8ZAwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x3120910>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(spam_posterior)\n",
    "plt.title(\"Posterior distribution for Spam Msg\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(ham_posterior)\n",
    "plt.title(\"Posterior distribution for Non-Spam Msg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With laplace smoothing, the posterior probability is even more extreme.  But this is because we are using training set to calculate prediction error.  In hold-out sets the posterior probability may not be so extreme.<br/>\n",
    "With laplace smoothing, the training error is zero, meaning that the model has fitted all of the variations in the training data.  With non-smoothing, I was using 0.001 as the default probability for any words not existed in spam or non-spam class training sets.  As such, the non-smoothed model can fit to less variations in the training set and therefore resulted in a slightly higher training error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <b>HW2.5. Repeat HW2.4. This time when modeling and classification ignore tokens with a frequency of less than three (3) in the training set. How does it affect the misclassifcation error of learnt naive multinomial Bayesian Classifier on the training dataset.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Kuan Lin\n",
    "## Description: reducer code for HW1.5\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import math\n",
    "import string\n",
    "import subprocess\n",
    "\n",
    "files = sys.argv[1:]\n",
    "data=\"/w261/hw2/enronemail_1h.txt\"\n",
    "\n",
    "spam_word_counts = {}\n",
    "ham_word_counts = {}\n",
    "spam_total_words = 0\n",
    "ham_total_words = 0\n",
    "unique_words = set()\n",
    "spam_zero_count_processed = 0\n",
    "ham_zero_count_processed = 0\n",
    "\n",
    "# assume equal prior for spam and ham\n",
    "spam_prior = 0.44\n",
    "ham_prior = 0.56\n",
    "\n",
    "def findWordCount(doc, word):\n",
    "\tcount = 0\n",
    "\tfor w in doc.split(' '):\n",
    "\t\tif w == word: count += 1\n",
    "\treturn count\n",
    "\n",
    "def spamWordProb(word):\n",
    "\tcount = 0\n",
    "\tif word in spam_word_counts: count = spam_word_counts[word]\n",
    "\t# ignore frequecy of less than 3\n",
    "\tif count < 3: count = 0\n",
    "\t# Applying laplace smoothing\n",
    "\treturn float(count+1)/float(spam_total_words+len(unique_words))\n",
    "\t#return float(count)/float(spam_total_words)\n",
    "\t\n",
    "def hamWordProb(word):\n",
    "\tcount = 0\n",
    "\tif word in ham_word_counts: count = ham_word_counts[word]\n",
    "\t# ignore frequecy of less than 3\n",
    "\tif count < 3: count = 0\n",
    "\t# Applying laplace smoothing\n",
    "\treturn float(count+1)/float(ham_total_words+len(unique_words))\n",
    "\t#return float(count)/float(ham_total_words)\n",
    "\t\n",
    "def classifyWithWordList(document, word_list):\n",
    "\tglobal spam_zero_count_processed\n",
    "\tglobal ham_zero_count_processed\n",
    "\tspam_logprob = math.log(spam_prior)\n",
    "\tham_logprob = math.log(ham_prior)\n",
    "\tfor word in word_list:\n",
    "\t\t#pattern = re.compile('[\\s+]?(' + word + ')[\\s+]?')\n",
    "\t\tword_count = findWordCount(document, word)\n",
    "\t\tspam_word_prob = spamWordProb(word)\n",
    "\t\tham_word_prob = hamWordProb(word)\n",
    "\t\tif spam_word_prob > 0:\n",
    "\t\t\tspam_logprob += math.log(spam_word_prob)*word_count\n",
    "\t\telse:\n",
    "\t\t\tspam_zero_count_processed += 1\n",
    "\t\t\tspam_logprob += math.log(0.0001)*word_count\n",
    "\t\tif ham_word_prob > 0:\n",
    "\t\t\tham_logprob += math.log(ham_word_prob)*word_count\n",
    "\t\telse:\n",
    "\t\t\tham_zero_count_processed += 1\n",
    "\t\t\tham_logprob += math.log(0.0001)*word_count\n",
    "\t\n",
    "\tspam_prob = math.e**spam_logprob\n",
    "\tham_prob = math.e**ham_logprob\n",
    "\tif spam_logprob > ham_logprob:\n",
    "\t\treturn (1, spam_prob, ham_prob)\n",
    "\telse:\n",
    "\t\treturn (0, spam_prob, ham_prob)\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    if line == \"\": continue\n",
    "\n",
    "    lineArr = line.split('\\t')\n",
    "    word = lineArr[0]\n",
    "    count = int(lineArr[1])\n",
    "    spam_flag = int(lineArr[2])\n",
    "\n",
    "    if spam_flag == 1:\n",
    "        if word == \"class_total_words\":\n",
    "            spam_total_words += count\n",
    "            continue\n",
    "        if word in spam_word_counts:\n",
    "            spam_word_counts[word] += count\n",
    "        else:\n",
    "            spam_word_counts[word] = count\n",
    "    else:\n",
    "        if word == \"class_total_words\":\n",
    "            ham_total_words += count\n",
    "            continue\n",
    "        if word in ham_word_counts:\n",
    "            ham_word_counts[word] += count\n",
    "        else:\n",
    "            ham_word_counts[word] = count\n",
    "\n",
    "# perform classification with all words\n",
    "unique_words = set([k for k in spam_word_counts.keys()] + [k for k in ham_word_counts.keys()])\n",
    "total_msg_classified = 0\n",
    "total_msg_correct = 0\n",
    "tpc = 0\n",
    "fpc = 0\n",
    "tnc = 0\n",
    "fnc = 0\n",
    "\n",
    "cat = subprocess.Popen([\"hadoop\", \"fs\", \"-cat\", data], stdout=subprocess.PIPE)\n",
    "for line in cat.stdout:\n",
    "\tline = line.strip()\n",
    "\tlineArr = line.split('\\t')\n",
    "\t# check data integrity.  each valid line should have 4 fields, with second field equals either 0 or 1\n",
    "\tif len(lineArr) != 4 or (lineArr[1] != \"0\" and lineArr[1] != \"1\"): continue\n",
    "\tmsg_id = lineArr[0]\n",
    "\tspam_flag = lineArr[1]\n",
    "\tsubject_body = lineArr[2] + ' ' + lineArr[3]\n",
    "\t# remove punctuations\n",
    "\tsubject_body = subject_body.translate(string.maketrans(\"\",\"\"), string.punctuation).strip().lower()\n",
    "\tpredicted_results = classifyWithWordList(subject_body, unique_words)\n",
    "\tspam_prob = predicted_results[1]\n",
    "\tham_prob = predicted_results[2]\n",
    "\tnorm = spam_prob + ham_prob\n",
    "\tif norm > 0:\n",
    "\t\tspam_prob = spam_prob / norm\n",
    "\t\tham_prob = ham_prob / norm\n",
    "\telse:\n",
    "\t\tspam_prob = 0\n",
    "\t\tham_prob = 0\n",
    "\tspam_flag_predicted = str(predicted_results[0])\n",
    "\ttotal_msg_classified += 1\n",
    "\tif spam_flag == \"1\":\n",
    "\t\tif spam_flag_predicted == \"1\": tpc += 1\n",
    "\t\telse: fnc += 1\n",
    "\telse:\n",
    "\t\tif spam_flag_predicted == \"1\": fpc += 1\n",
    "\t\telse: tnc += 1\n",
    "\tif spam_flag_predicted == spam_flag: total_msg_correct += 1\n",
    "\tprint '\\t'.join([msg_id, spam_flag, spam_flag_predicted, str(spam_prob), str(ham_prob)])\n",
    "print \"-----------------------------------------------------\"\n",
    "print \"feature size: \" + str(len(unique_words))\n",
    "print \"total document classified: \" + str(total_msg_classified)\n",
    "print \"total document correctly classified: \" + str(total_msg_correct)\n",
    "print \"Spam zero prob processed: %s\" % spam_zero_count_processed\n",
    "print \"Non-Spam zero prob processed: %s\" % ham_zero_count_processed\n",
    "print \"TPR: %.4f\"%(float(tpc)/float(tpc+fnc))\n",
    "print \"FPR: %.4f\"%(float(fpc)/float(fpc+tnc))\n",
    "print \"accuracy: %.4f\"%(float(total_msg_correct)/float(total_msg_classified))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/24 09:50:57 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "packageJobJar: [/data/w261/hw2/mapper.py, /data/w261/hw2/reducer.py] [/usr/lib/hadoop/lib/hadoop-streaming-2.6.0.jar] /tmp/streamjob3398854005381174200.jar tmpDir=null\n",
      "16/01/24 09:51:01 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/24 09:51:02 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/24 09:51:04 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/24 09:51:04 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/01/24 09:51:05 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1453604265894_0026\n",
      "16/01/24 09:51:06 INFO impl.YarnClientImpl: Submitted application application_1453604265894_0026\n",
      "16/01/24 09:51:06 INFO mapreduce.Job: The url to track the job: http://ip-172-31-55-170.ec2.internal:8088/proxy/application_1453604265894_0026/\n",
      "16/01/24 09:51:06 INFO mapreduce.Job: Running job: job_1453604265894_0026\n",
      "16/01/24 09:51:23 INFO mapreduce.Job: Job job_1453604265894_0026 running in uber mode : false\n",
      "16/01/24 09:51:23 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/01/24 09:51:47 INFO mapreduce.Job:  map 33% reduce 0%\n",
      "16/01/24 09:51:49 INFO mapreduce.Job:  map 83% reduce 0%\n",
      "16/01/24 09:51:50 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/24 09:52:06 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/24 09:52:59 INFO mapreduce.Job: Job job_1453604265894_0026 completed successfully\n",
      "16/01/24 09:52:59 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=111973\n",
      "\t\tFILE: Number of bytes written=565993\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=216879\n",
      "\t\tHDFS: Number of bytes written=3965\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=46565\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=66880\n",
      "\t\tTotal time spent by all map tasks (ms)=46565\n",
      "\t\tTotal time spent by all reduce tasks (ms)=66880\n",
      "\t\tTotal vcore-seconds taken by all map tasks=46565\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=66880\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=47682560\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=68485120\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=101\n",
      "\t\tMap output records=8280\n",
      "\t\tMap output bytes=95407\n",
      "\t\tMap output materialized bytes=111979\n",
      "\t\tInput split bytes=200\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=5664\n",
      "\t\tReduce shuffle bytes=111979\n",
      "\t\tReduce input records=8280\n",
      "\t\tReduce output records=107\n",
      "\t\tSpilled Records=16560\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=739\n",
      "\t\tCPU time spent (ms)=10120\n",
      "\t\tPhysical memory (bytes) snapshot=553492480\n",
      "\t\tVirtual memory (bytes) snapshot=7593340928\n",
      "\t\tTotal committed heap usage (bytes)=392372224\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=216679\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=3965\n",
      "16/01/24 09:52:59 INFO streaming.StreamJob: Output directory: /w261/hw2/hw2_5_output\n"
     ]
    }
   ],
   "source": [
    "!cp mapper.py /data/w261/hw2/mapper.py; cp reducer.py /data/w261/hw2/reducer.py;\n",
    "!chmod +x /data/w261/hw2/mapper.py; chmod +x /data/w261/hw2/reducer.py;\n",
    "#!hdfs dfs -rm -r /w261/hw2/hw2_5_output;\n",
    "!$HADOOP_HOME/bin/hadoop jar $HADOOP_HOME/lib/hadoop-*streaming*.jar -mapper mapper.py -file /data/w261/hw2/mapper.py -reducer reducer.py -file /data/w261/hw2/reducer.py -input /w261/hw2/enronemail_1h.txt -output /w261/hw2/hw2_5_output;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0017.2003-12-18.GP\t1\t1\t0.999983062144\t1.69378555673e-05\r\n",
      "0017.2004-08-01.BG\t1\t0\t0\t0\r\n",
      "0017.2004-08-02.BG\t1\t1\t0\t0\r\n",
      "0018.1999-12-14.kaminski\t0\t0\t0\t0\r\n",
      "0018.2001-07-13.SA_and_HP\t1\t1\t0\t0\r\n",
      "0018.2003-12-18.GP\t1\t1\t0\t0\r\n",
      "-----------------------------------------------------\t\r\n",
      "feature size: 5663\t\r\n",
      "total document classified: 98\t\r\n",
      "total document correctly classified: 96\t\r\n",
      "Spam zero prob processed: 0\t\r\n",
      "Non-Spam zero prob processed: 0\t\r\n",
      "TPR: 0.9535\t\r\n",
      "FPR: 0.0000\t\r\n",
      "accuracy: 0.9796\t\r\n"
     ]
    }
   ],
   "source": [
    "# prediction result summary\n",
    "!hdfs dfs -cat /w261/hw2/hw2_5_output/part-00000 | tail -15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Accuracy is 98% by ignoring token with frequency < 3, and applying Laplace Smoothing.  Training Error is 2%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfAAAAEKCAYAAADtpQeZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x3120d90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# extract posterior probs\n",
    "import subprocess\n",
    "spam_posterior = []\n",
    "ham_posterior = []\n",
    "\n",
    "cat = subprocess.Popen([\"hadoop\", \"fs\", \"-cat\", '/w261/hw2/hw2_5_output/part-00000'], stdout=subprocess.PIPE)\n",
    "for line in cat.stdout:\n",
    "    line = line.strip()\n",
    "    if line == '': continue\n",
    "    lineArr = line.split('\\t')\n",
    "    if len(lineArr) != 5: continue\n",
    "    spam_posterior.append(float(lineArr[3]))\n",
    "    ham_posterior.append(float(lineArr[4]))\n",
    "    \n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(spam_posterior)\n",
    "plt.title(\"Posterior distribution for Spam Msg\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(ham_posterior)\n",
    "plt.title(\"Posterior distribution for Non-Spam Msg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By ignoring tokens whose freqencies are less than 3, the posterior probability distributions are less extreme than ones using all tokens.  Training error increases slightly as the model now does not completeley overfit the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* <b>HW2.6 Benchmark your code with the Python SciKit-Learn implementation of the multinomial Naive Bayes algorithm\n",
    "* Run the Multinomial Naive Bayes algorithm (using default settings) from SciKit-Learn over the same training data used in HW2.5 and report the misclassification error (please note some data preparation might be needed to get the Multinomial Naive Bayes algorithm from SkiKit-Learn to run over this dataset)\n",
    "* Prepare a table to present your results, where rows correspond to approach used (SkiKit-Learn versus your Hadoop implementation) and the column presents the training misclassification error\n",
    "* Explain/justify any differences in terms of training error rates over the dataset in HW2.5 between your Multinomial Naive Bayes implementation (in Map Reduce) versus the Multinomial Naive Bayes implementation in SciKit-Learn</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB Accuracy: 1.0000\n",
      "BernoulliNB Accuracy: 0.7857\n"
     ]
    }
   ],
   "source": [
    "data = \"enronemail_1h.txt\"\n",
    "\n",
    "import string\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import *\n",
    "\n",
    "train_data_raw = []\n",
    "train_labels = []\n",
    "\n",
    "# load data from file\n",
    "for line in open(data, 'r'):\n",
    "    line = line.strip().lower()\n",
    "    if line == \"\": continue\n",
    "    lineArr = line.split('\\t')\n",
    "    if len(lineArr) != 4 or (lineArr[1] != \"0\" and lineArr[1] != \"1\"): continue\n",
    "    spam_flag = lineArr[1]\n",
    "    subject_body = lineArr[2] + ' ' + lineArr[3]\n",
    "    subject_body = subject_body.translate(string.maketrans(\"\",\"\"), string.punctuation).strip().lower()\n",
    "    train_data_raw.append(subject_body)\n",
    "    train_labels.append(spam_flag)\n",
    "\n",
    "# fit with count vectorizer\n",
    "train_data = CountVectorizer().fit_transform(train_data_raw)\n",
    "#print train_data.shape\n",
    "\n",
    "# Multinomial NB with default settings:\n",
    "multiNB_train_accuracy = MultinomialNB().fit(train_data, train_labels).score(train_data, train_labels)\n",
    "print \"MultinomialNB Accuracy: %.4f\" % multiNB_train_accuracy\n",
    "\n",
    "# Bernouli NB with default settings:\n",
    "bernoulliNB_train_accuracy = BernoulliNB().fit(train_data, train_labels).score(train_data, train_labels)\n",
    "print \"BernoulliNB Accuracy: %.4f\" % bernoulliNB_train_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code obtains classification accuracy scores for MultinomialNB and BernoulliNB classifier.  The accuracy score for our custom-built MultinomialNB is obtained previously.  Training error is just 1-accuracy.  Thus:\n",
    "\n",
    "<table>\n",
    "<tr><th></th><th>Train Accuracy</th><th>Train Error</th></tr>\n",
    "<tr><th>Sklearn MultinomialNB</th><td>1.00</td><td>0.00</td></tr>\n",
    "<tr><th>Sklearn BernoulliNB</th><td>0.7857</td><td>0.2143</td></tr>\n",
    "<tr><th>Custom MultinomialNB with Laplace Smoothing</th><td>1.00</td><td>0.00</td></tr>\n",
    "<tr><th>Custom MultinomialNB with Laplace Smoothing and ignoring token freq less than 3</th><td>0.98</td><td>0.02</td></tr>\n",
    "<tr><th>Custom MultinomialNB No Smoothing</th><td>0.94</td><td>0.06</td></tr>\n",
    "</table>\n",
    "\n",
    "* Since we are calculating error rate using training data, it is not surprising that the MultinomialNB from Sklearn and our own custom-built model achieved zero training error.  This means that both models can explain all of the variations in the training data.  However, this does not gaurantee that the model will perform as well on future data.\n",
    "* BernoulliNB model performs worse because it uses less information from the training data.  The BernoulliNB model only looks at if a term appears in a document, and does not take into account of how many times the term appear in the same document.  This will cause the BernoulliNB model to be unable to distinquish documents with the same words but with different frequencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <b>HHW 2.6.1 OPTIONAL (note this exercise is a stretch HW and optional)\n",
    "* Run the Bernoulli Naive Bayes algorithm from SciKit-Learn (using default settings) over the same training data used in HW2.6 and report the misclassification error \n",
    "* Discuss the performance differences in terms of misclassification error rates over the dataset in HW2.5 between the  Multinomial Naive Bayes implementation in SciKit-Learn with the  Bernoulli Naive Bayes implementation in SciKit-Learn. Why such big differences. Explain.\n",
    "* Which approach to Naive Bayes would you recommend for SPAM detection? Justify your selection.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result for Bernoulli Naive Bayes model is presented together in the above cell in HW2.6.  The main reason why the Bernoulli model performsly relatively poorly is because 1.) it does not account for relative frequency and only accounts the presence/omission of words 2.) it explicitly penalize omission of words, which is not optimal for Spam classification (omission of non-spam words should not increase the probability of a message being spam).  For these reasons we should select the Multinomial approach over the Bernoulli approach in spam classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
