{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 5.0\n",
    "What is a data warehouse? What is a Star schema? When is it used?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data warehouse is an integrated database often used to store both current and historical data for the purpose of reporting and analytics.\n",
    "\n",
    "A Star Schema is a special case of snow-flake schema where there has a fact table at the center surrounded by dimension tables that are referenced to the fact table with foriegn keys.  The star schema has a relaxed normalization rule compared to other types of schemas and therefore can be easier to implement and query for small-scale information systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 5.1\n",
    "In the database world What is 3NF? Does machine learning use data in 3NF? If so why? \n",
    "In what form does ML consume data?\n",
    "Why would one use log files that are denormalized?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3NF (3rd normal form) must first satisfy 1NF and 2NF, which includes:\n",
    "* Eliminate repeating groups in individual tables.\n",
    "* Create a separate table for each set of related data.\n",
    "* Identify each set of related data with a primary key\n",
    "* no non-prime attribute is dependent on any proper subset of any candidate key of the table\n",
    "\n",
    "In addition, the schema must also satisfy:\n",
    "* Every non-prime attribute of a table is non-transitively dependent on every key of a table\n",
    "\n",
    "In instance-based (such as K-mean) and supervised ML algorithms, the algorithm must be able to access all attributes (that made through feature selections) on each data point, and therefore many ML algorithms need a dense matrix of data rather than a normaled form.  Use a denormalized log file allows ML algorithm to parse out all required attributes of the data which satisfies the aforementioned requiredments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 5.2\n",
    "Using MRJob, implement a hashside join (memory-backed map-side) for left, \n",
    "right and inner joins. Run your code on the  data used in HW 4.4: (Recall HW 4.4: Find the most frequent visitor of each page using mrjob and the output of 4.2  (i.e., transfromed log file). In this output please include the webpage URL, webpageID and Visitor ID.)\n",
    "\n",
    "Justify which table you chose as the Left table in this hashside join.\n",
    "\n",
    "Please report the number of rows resulting from:\n",
    "\n",
    "(1) Left joining Table Left with Table Right<br/>\n",
    "(2) Right joining Table Left with Table Right<br/>\n",
    "(3) Inner joining Table Left with Table Right<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: Since the url table is much smaller, it is better to store the url table in memory as a hashtable and perform the join as the right table.  Therefore the better option is to use the visit-info table on the left, and use the url-table as the memory-backed hash table on the right.  Also, it is possible that the url is not documented, and therefore option 1 (left join) is a prefered solution so that we don't have to drop visit data in the case of unmatched url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# first, reproduced the needed tables from hw4\n",
    "\n",
    "# make the reformated table for visit info:\n",
    "source_file = 'anonymous-msweb.data'\n",
    "output_file = 'reformated_webdata.csv'\n",
    "\n",
    "writer = open(output_file, 'w')\n",
    "current_visitor = None\n",
    "for line in open(source_file, 'r'):\n",
    "    line = line.strip()\n",
    "    if line == '' or (not line.startswith(\"C,\") and not line.startswith(\"V\")): continue\n",
    "    lineArr = line.split(',')\n",
    "    if lineArr[0] == 'C':\n",
    "        current_visitor = lineArr[2]\n",
    "    elif current_visitor != None:\n",
    "        writer.write(line + ',C,' + current_visitor + '\\n')\n",
    "writer.close()\n",
    "\n",
    "# make the url table:\n",
    "# need to first extract out the urls, will be used to lookup url in reducer\n",
    "import csv\n",
    "source_file = 'anonymous-msweb.data'\n",
    "output_file = 'url_data.csv'\n",
    "writer = open(output_file, 'w')\n",
    "\n",
    "for line in open(source_file, 'r'):\n",
    "    line = line.strip()\n",
    "    if line == '':\n",
    "        continue\n",
    "    if line.startswith('A,'):\n",
    "        lineArr = list(csv.reader(line.splitlines(), delimiter=',', quotechar='\"'))[0]\n",
    "        writer.write(','.join([lineArr[1], lineArr[4].replace('/', '')]) + '\\n')\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing top_visitor.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile top_visitor.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.protocol import RawValueProtocol\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "# option1: Left Join\n",
    "\n",
    "class TopVisitor(MRJob):\n",
    "    \n",
    "    OUTPUT_PROTOCOL = RawValueProtocol\n",
    "    \n",
    "    def steps(self):\n",
    "        # two step processes: firs step to calculate visit counts per user and page combination\n",
    "        # second step to rank user by their visit counts within each page\n",
    "        return [\n",
    "            MRStep(mapper=self.process_visits, reducer=self.aggregate_visits),\n",
    "            MRStep(reducer_init=self.load_url_info, reducer=self.find_most_freq_visitor)\n",
    "        ]\n",
    "    \n",
    "    def process_visits(self, _, line):\n",
    "        # emiting visit counts per user and page combination\n",
    "        lineArr = line.strip().split(',')\n",
    "        page_id = lineArr[1]\n",
    "        visit_counts = int(lineArr[2])\n",
    "        visitor_id = lineArr[4]\n",
    "        yield (page_id, visitor_id), visit_counts \n",
    "        \n",
    "    def aggregate_visits(self, page_visitor_id, visit_counts):\n",
    "        # calculate how many visits from each visitor on the particular page\n",
    "        yield page_visitor_id[0], (page_visitor_id[1], sum(visit_counts))\n",
    "        \n",
    "    def load_url_info(self):\n",
    "        # loading up URL dictionary to used for lookup in reducer\n",
    "        self.URLs = {}\n",
    "        for line in open('url_data.csv', 'r'):\n",
    "            lineArr = line.strip().split(',')\n",
    "            self.URLs[lineArr[0]] = lineArr[1]\n",
    "            \n",
    "    def find_most_freq_visitor(self, page_id, visitor_data):\n",
    "        # find the most frequent visotor for each page.\n",
    "        max_visitor = None\n",
    "        max_visits = 0\n",
    "        for visitor_id, visit_counts in visitor_data:\n",
    "            if visit_counts > max_visits:\n",
    "                max_visitor = visitor_id\n",
    "                max_visits = visit_counts\n",
    "        page_url = 'UNKN'\n",
    "        # lookup URL\n",
    "        if page_id in self.URLs:\n",
    "            page_url = self.URLs[page_id]\n",
    "        \n",
    "        yield None, '\\t'.join([page_url, page_id, max_visitor])\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    TopVisitor.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "no configs found; falling back on auto-configuration\n",
      "no configs found; falling back on auto-configuration\n",
      "creating tmp directory c:\\temp\\top_visitor.kuanlin.20160214.233953.231000\n",
      "\n",
      "PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "\n",
      "writing to c:\\temp\\top_visitor.kuanlin.20160214.233953.231000\\step-0-mapper_part-00000\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "writing to c:\\temp\\top_visitor.kuanlin.20160214.233953.231000\\step-0-mapper-sorted\n",
      "> sort 'c:\\temp\\top_visitor.kuanlin.20160214.233953.231000\\step-0-mapper_part-00000'\n",
      "writing to c:\\temp\\top_visitor.kuanlin.20160214.233953.231000\\step-0-reducer_part-00000\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "writing to c:\\temp\\top_visitor.kuanlin.20160214.233953.231000\\step-1-mapper_part-00000\n",
      "Counters from step 2:\n",
      "  (no counters found)\n",
      "writing to c:\\temp\\top_visitor.kuanlin.20160214.233953.231000\\step-1-mapper-sorted\n",
      "> sort 'c:\\temp\\top_visitor.kuanlin.20160214.233953.231000\\step-1-mapper_part-00000'\n",
      "writing to c:\\temp\\top_visitor.kuanlin.20160214.233953.231000\\step-1-reducer_part-00000\n",
      "Counters from step 2:\n",
      "  (no counters found)\n",
      "Moving c:\\temp\\top_visitor.kuanlin.20160214.233953.231000\\step-1-reducer_part-00000 -> c:\\temp\\top_visitor.kuanlin.20160214.233953.231000\\output\\part-00000\n",
      "Streaming final output from c:\\temp\\top_visitor.kuanlin.20160214.233953.231000\\output\n",
      "removing tmp directory c:\\temp\\top_visitor.kuanlin.20160214.233953.231000\n"
     ]
    }
   ],
   "source": [
    "!python top_visitor.py reformated_webdata.csv --file url_data.csv > 5_2_left_join.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting top_visitor.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile top_visitor.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.protocol import RawValueProtocol\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "# option2: Right Join\n",
    "\n",
    "class TopVisitor(MRJob):\n",
    "    \n",
    "    OUTPUT_PROTOCOL = RawValueProtocol\n",
    "    \n",
    "    def steps(self):\n",
    "        # two step processes: firs step to calculate visit counts per user and page combination\n",
    "        # second step to rank user by their visit counts within each page\n",
    "        return [\n",
    "            MRStep(mapper=self.process_visits, reducer=self.aggregate_visits),\n",
    "            MRStep(reducer_init=self.load_url_info, reducer=self.find_most_freq_visitor, reducer_final=self.output_info)\n",
    "        ]\n",
    "    \n",
    "    def process_visits(self, _, line):\n",
    "        # emiting visit counts per user and page combination\n",
    "        lineArr = line.strip().split(',')\n",
    "        page_id = lineArr[1]\n",
    "        visit_counts = int(lineArr[2])\n",
    "        visitor_id = lineArr[4]\n",
    "        yield (page_id, visitor_id), visit_counts \n",
    "        \n",
    "    def aggregate_visits(self, page_visitor_id, visit_counts):\n",
    "        # calculate how many visits from each visitor on the particular page\n",
    "        yield page_visitor_id[0], (page_visitor_id[1], sum(visit_counts))\n",
    "        \n",
    "    def load_url_info(self):\n",
    "        # loading up URL dictionary to used for lookup in reducer\n",
    "        self.URLs = {}\n",
    "        self.visit_data = {}\n",
    "        for line in open('url_data.csv', 'r'):\n",
    "            lineArr = line.strip().split(',')\n",
    "            self.URLs[lineArr[0]] = lineArr[1]\n",
    "            \n",
    "    def find_most_freq_visitor(self, page_id, visitor_data):\n",
    "        # find the most frequent visotor for each page.\n",
    "        max_visitor = None\n",
    "        max_visits = 0\n",
    "        for visitor_id, visit_counts in visitor_data:\n",
    "            if visit_counts > max_visits:\n",
    "                max_visitor = visitor_id\n",
    "                max_visits = visit_counts\n",
    "        self.visit_data[page_id] = max_visitor\n",
    "\n",
    "    def output_info(self):\n",
    "        writer = open('5_2_right_join.txt', 'w')\n",
    "        for page_id in self.URLs:\n",
    "            if page_id in self.visit_data:\n",
    "                writer.write('\\t'.join([self.URLs[page_id], page_id, self.visit_data[page_id]]) + '\\n')\n",
    "            else:\n",
    "                writer.write('\\t'.join([self.URLs[page_id], page_id, 'no_visitor_data']) + '\\n')\n",
    "        writer.close()\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    TopVisitor.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "no configs found; falling back on auto-configuration\n",
      "no configs found; falling back on auto-configuration\n",
      "creating tmp directory c:\\temp\\top_visitor.kuanlin.20160214.235335.585000\n",
      "\n",
      "PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "\n",
      "writing to c:\\temp\\top_visitor.kuanlin.20160214.235335.585000\\step-0-mapper_part-00000\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "writing to c:\\temp\\top_visitor.kuanlin.20160214.235335.585000\\step-0-mapper-sorted\n",
      "> sort 'c:\\temp\\top_visitor.kuanlin.20160214.235335.585000\\step-0-mapper_part-00000'\n",
      "writing to c:\\temp\\top_visitor.kuanlin.20160214.235335.585000\\step-0-reducer_part-00000\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "writing to c:\\temp\\top_visitor.kuanlin.20160214.235335.585000\\step-1-mapper_part-00000\n",
      "Counters from step 2:\n",
      "  (no counters found)\n",
      "writing to c:\\temp\\top_visitor.kuanlin.20160214.235335.585000\\step-1-mapper-sorted\n",
      "> sort 'c:\\temp\\top_visitor.kuanlin.20160214.235335.585000\\step-1-mapper_part-00000'\n",
      "writing to c:\\temp\\top_visitor.kuanlin.20160214.235335.585000\\step-1-reducer_part-00000\n",
      "Counters from step 2:\n",
      "  (no counters found)\n",
      "Moving c:\\temp\\top_visitor.kuanlin.20160214.235335.585000\\step-1-reducer_part-00000 -> c:\\temp\\top_visitor.kuanlin.20160214.235335.585000\\output\\part-00000\n",
      "Streaming final output from c:\\temp\\top_visitor.kuanlin.20160214.235335.585000\\output\n",
      "removing tmp directory c:\\temp\\top_visitor.kuanlin.20160214.235335.585000\n"
     ]
    }
   ],
   "source": [
    "# right-side join\n",
    "!python top_visitor.py reformated_webdata.csv --file url_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting top_visitor.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile top_visitor.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.protocol import RawValueProtocol\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "# option3: Inner Join\n",
    "\n",
    "class TopVisitor(MRJob):\n",
    "    \n",
    "    OUTPUT_PROTOCOL = RawValueProtocol\n",
    "    \n",
    "    def steps(self):\n",
    "        # two step processes: firs step to calculate visit counts per user and page combination\n",
    "        # second step to rank user by their visit counts within each page\n",
    "        return [\n",
    "            MRStep(mapper=self.process_visits, reducer=self.aggregate_visits),\n",
    "            MRStep(reducer_init=self.load_url_info, reducer=self.find_most_freq_visitor)\n",
    "        ]\n",
    "    \n",
    "    def process_visits(self, _, line):\n",
    "        # emiting visit counts per user and page combination\n",
    "        lineArr = line.strip().split(',')\n",
    "        page_id = lineArr[1]\n",
    "        visit_counts = int(lineArr[2])\n",
    "        visitor_id = lineArr[4]\n",
    "        yield (page_id, visitor_id), visit_counts \n",
    "        \n",
    "    def aggregate_visits(self, page_visitor_id, visit_counts):\n",
    "        # calculate how many visits from each visitor on the particular page\n",
    "        yield page_visitor_id[0], (page_visitor_id[1], sum(visit_counts))\n",
    "        \n",
    "    def load_url_info(self):\n",
    "        # loading up URL dictionary to used for lookup in reducer\n",
    "        self.URLs = {}\n",
    "        for line in open('url_data.csv', 'r'):\n",
    "            lineArr = line.strip().split(',')\n",
    "            self.URLs[lineArr[0]] = lineArr[1]\n",
    "            \n",
    "    def find_most_freq_visitor(self, page_id, visitor_data):\n",
    "        # find the most frequent visotor for each page.\n",
    "        max_visitor = None\n",
    "        max_visits = 0\n",
    "        for visitor_id, visit_counts in visitor_data:\n",
    "            if visit_counts > max_visits:\n",
    "                max_visitor = visitor_id\n",
    "                max_visits = visit_counts\n",
    "        page_url = 'UNKN'\n",
    "        # lookup URL\n",
    "        if page_id in self.URLs:\n",
    "            page_url = self.URLs[page_id]\n",
    "        \n",
    "        if page_url != 'UNKN':\n",
    "            yield None, '\\t'.join([page_url, page_id, max_visitor])\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    TopVisitor.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "no configs found; falling back on auto-configuration\n",
      "no configs found; falling back on auto-configuration\n",
      "creating tmp directory c:\\temp\\top_visitor.kuanlin.20160214.235537.554000\n",
      "\n",
      "PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "\n",
      "writing to c:\\temp\\top_visitor.kuanlin.20160214.235537.554000\\step-0-mapper_part-00000\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "writing to c:\\temp\\top_visitor.kuanlin.20160214.235537.554000\\step-0-mapper-sorted\n",
      "> sort 'c:\\temp\\top_visitor.kuanlin.20160214.235537.554000\\step-0-mapper_part-00000'\n",
      "writing to c:\\temp\\top_visitor.kuanlin.20160214.235537.554000\\step-0-reducer_part-00000\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "writing to c:\\temp\\top_visitor.kuanlin.20160214.235537.554000\\step-1-mapper_part-00000\n",
      "Counters from step 2:\n",
      "  (no counters found)\n",
      "writing to c:\\temp\\top_visitor.kuanlin.20160214.235537.554000\\step-1-mapper-sorted\n",
      "> sort 'c:\\temp\\top_visitor.kuanlin.20160214.235537.554000\\step-1-mapper_part-00000'\n",
      "writing to c:\\temp\\top_visitor.kuanlin.20160214.235537.554000\\step-1-reducer_part-00000\n",
      "Counters from step 2:\n",
      "  (no counters found)\n",
      "Moving c:\\temp\\top_visitor.kuanlin.20160214.235537.554000\\step-1-reducer_part-00000 -> c:\\temp\\top_visitor.kuanlin.20160214.235537.554000\\output\\part-00000\n",
      "Streaming final output from c:\\temp\\top_visitor.kuanlin.20160214.235537.554000\\output\n",
      "removing tmp directory c:\\temp\\top_visitor.kuanlin.20160214.235537.554000\n"
     ]
    }
   ],
   "source": [
    "!python top_visitor.py reformated_webdata.csv --file url_data.csv > 5_2_inner_join.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Number of rows in left join results:\n",
      "285\n",
      "--------------------------------------\n",
      "Number of rows in right join results:\n",
      "294\n",
      "--------------------------------------\n",
      "Number of rows in inner join results:\n",
      "285\n"
     ]
    }
   ],
   "source": [
    "# report number of rows in the results:\n",
    "print \"--------------------------------------\"\n",
    "print \"Number of rows in left join results:\"\n",
    "print sum(1 for line in open('5_2_left_join.txt', 'r'))\n",
    "\n",
    "print \"--------------------------------------\"\n",
    "print \"Number of rows in right join results:\"\n",
    "print sum(1 for line in open('5_2_right_join.txt', 'r'))\n",
    "\n",
    "print \"--------------------------------------\"\n",
    "print \"Number of rows in inner join results:\"\n",
    "print sum(1 for line in open('5_2_inner_join.txt', 'r'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 5.3 For the remainder of this assignment you will work with two datasets:\n",
    "### 1: unit/systems test data set: SYSTEMS TEST DATASET\n",
    "Three terms, A,B,C and their corresponding strip-docs of co-occurring terms\n",
    "\n",
    "<pre>\n",
    "DocA {X:20, Y:30, Z:5}\n",
    "DocB {X:100, Y:20}\n",
    "DocC {M:5, N:20, Z:5}\n",
    "</pre>\n",
    "\n",
    "### 2: A large subset of the Google n-grams dataset\n",
    "\n",
    "https://aws.amazon.com/datasets/google-books-ngrams/\n",
    "\n",
    "which we have placed in a bucket/folder on Dropbox on s3:\n",
    "\n",
    "   https://www.dropbox.com/sh/tmqpc4o0xswhkvz/AACUifrl6wrMrlK6a3X3lZ9Ea?dl=0 \n",
    "\n",
    "   s3://filtered-5grams/\n",
    "\n",
    "For each HW 5.3 -5.5 Please unit test and system test your code with with SYSTEMS TEST DATASET and show the results. \n",
    "Please compute the expected answer by hand and show your hand calculations. Then show the results you get with you system.\n",
    "Final show your results on the Google n-grams dataset\n",
    "\n",
    "\n",
    "In particular, this bucket contains (~200) files (10Meg each) in the format:\n",
    "\n",
    "\t(ngram) \\t (count) \\t (pages_count) \\t (books_count)\n",
    "\n",
    "Do some EDA on this dataset using mrjob, e.g., \n",
    "\n",
    "- Longest 5-gram (number of characters)\n",
    "- Top 10 most frequent words (please use the count information), i.e., unigrams\n",
    "- Most/Least densely appearing words (count/pages_count) sorted in decreasing order of relative frequency (Hint: save to PART-000* and take the head -n 1000)\n",
    "- Distribution of 5-gram sizes (using counts info.) sorted in decreasing order of relative frequency. \n",
    "OPTIONAL Question:\n",
    "- Plot the log-log plot of the frequency distributuion of unigrams. Does it follow power law distribution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 5.4  (over 2Gig of Data)\n",
    "In this part of the assignment we will focus on developing methods\n",
    "for detecting synonyms, using the Google 5-grams dataset. To accomplish\n",
    "this you must script two main tasks using MRJob:\n",
    "\n",
    "(1) Build stripes of word co-ocurrence for the top 10,000 using the words ranked from 9001,-10,000 as a basis\n",
    "most frequently appearing words across the entire set of 5-grams,\n",
    "and output to a file in your bucket on s3 (bigram analysis, though the words are non-contiguous).\n",
    "\n",
    "(2) Using two (symmetric) comparison methods of your choice \n",
    "(e.g., correlations, distances, similarities), pairwise compare \n",
    "all stripes (vectors), and output to a file in your bucket on s3.\n",
    "\n",
    "==Design notes for (1)==\n",
    "For this task you will be able to modify the pattern we used in HW 3.2\n",
    "(feel free to use the solution as reference). To total the word counts \n",
    "across the 5-grams, output the support from the mappers using the total \n",
    "order inversion pattern:\n",
    "\n",
    "<*word,count>\n",
    "\n",
    "to ensure that the support arrives before the cooccurrences.\n",
    "\n",
    "In addition to ensuring the determination of the total word counts,\n",
    "the mapper must also output co-occurrence counts for the pairs of\n",
    "words inside of each 5-gram. Treat these words as a basket,\n",
    "as we have in HW 3, but count all stripes or pairs in both orders,\n",
    "i.e., count both orderings: (word1,word2), and (word2,word1), to preserve\n",
    "symmetry in our output for (2).\n",
    "\n",
    "==Design notes for (2)==\n",
    "For this task you will have to determine a method of comparison.\n",
    "Here are a few that you might consider:\n",
    "\n",
    "- Jaccard\n",
    "- Cosine similarity\n",
    "- Spearman correlation\n",
    "- Euclidean distance\n",
    "- Taxicab (Manhattan) distance\n",
    "- Shortest path graph distance (a graph, because our data is symmetric!)\n",
    "- Pearson correlation\n",
    "- Kendall correlation\n",
    "...\n",
    "\n",
    "However, be cautioned that some comparison methods are more difficult to\n",
    "parallelize than others, and do not perform more associations than is necessary, \n",
    "since your choice of association will be symmetric.\n",
    "\n",
    "Please use the inverted index (discussed in live session #5) based pattern to compute the pairwise (term-by-term) similarity matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# configuration for running MRJob on EMR\n",
    "import os\n",
    "\n",
    "aws_id = os.environ['AWSAccessKeyId']\n",
    "aws_key = os.environ['AWSSecretKey']\n",
    "\n",
    "writer = open('mrjob_emr.conf', 'w')\n",
    "writer.write(\n",
    "\"\"\"runners:\n",
    "  emr:\n",
    "    aws_access_key_id: %s\n",
    "    aws_region: us-east-1\n",
    "    ec2_instance_type: m1.medium\n",
    "    ec2_master_instance_type: m1.medium\n",
    "    num_ec2_instances: 4\n",
    "    aws_secret_access_key: %s\"\"\"%(aws_id, aws_key)\n",
    ")\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing top_vocabs.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile top_vocabs.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.protocol import RawValueProtocol\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "# this script is to find the top 1000 vocabs words\n",
    "\n",
    "class top_vocabs(MRJob):\n",
    "    # use raw value protocol to get inversion done easier\n",
    "    #INPUT_PROTOCOL = RawValueProtocol\n",
    "    #INTERNAL_PROTOCOL = RawValueProtocol\n",
    "    #OUTPUT_PROTOCOL = RawValueProtocol\n",
    "    \n",
    "    # step1 do word counts\n",
    "    # step2 get the top 1000 vocabs by counts\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.wordCountsMapper, reducer=self.wordCountsReducer),\n",
    "            MRStep(reducer_init=self.rankWords_init, reducer=self.getTopVocabs,\n",
    "                  jobconf={\n",
    "                    'stream.num.map.output.key.fields': 2,\n",
    "                    'mapreduce.partition.keypartitioner.options': '-k1,1',\n",
    "                    'mapred.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "                    'mapred.text.key.comparator.options': \"-k2,2nr\",\n",
    "                    'mapred.reduce.tasks': 1\n",
    "                })\n",
    "        ]      \n",
    "\n",
    "    def wordCountsMapper(self, _, line):\n",
    "        lineArr = line.strip().split('\\t')\n",
    "        ngram_count = int(lineArr[1])\n",
    "        for word in lineArr[0].split(' '):\n",
    "            # each word appear n times as the ngram count\n",
    "            yield word.lower() , ngram_count\n",
    "            \n",
    "    def wordCountsReducer(self, word, counts):\n",
    "        # summing up counts for each word\n",
    "        yield word, sum(counts)\n",
    "        \n",
    "    def rankWords_init(self):\n",
    "        self.vocab = set()\n",
    "        # grab the top 1000 words as vocab\n",
    "        self.vocab_size = 1000\n",
    "        \n",
    "    def getTopVocabs(self, word, count):\n",
    "        if len(self.vocab) < self.vocab_size:\n",
    "            self.vocab.add(word)\n",
    "            yield word, max(count)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    top_vocabs.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!python top_vocabs.py -r emr -c mrjob_emr.conf s3://filtered-5grams/ --output-dir=s3://lin.berkeley.mids/w261/hw5/vocab/ --no-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting synonym_matrix.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile synonym_matrix.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.protocol import RawValueProtocol\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "# this script creates co-occurence matrix and output into an inverted index\n",
    "\n",
    "class synonym_matrix(MRJob):\n",
    "    \n",
    "    SORT_VALUES = True\n",
    "    \n",
    "    def steps(self):\n",
    "        # first step is to count the frequencies of co-occurence terms\n",
    "        # second step is to produce inverted indexes\n",
    "        return [\n",
    "            MRStep(mapper_init=self.bigram_mapper_init, mapper=self.bigram_mapper, reducer=self.bigram_reducer),\n",
    "            MRStep(reducer=self.createInvertedIndex)\n",
    "        ]\n",
    "    \n",
    "    # load up vocabulary\n",
    "    def bigram_mapper_init(self):\n",
    "        self.vocab = set()\n",
    "        for line in open('vocabs.txt', 'r'):\n",
    "            lineArr = line.strip().split('\\t')\n",
    "            self.vocab.add(lineArr[0].replace('\"', '').lower())\n",
    "    \n",
    "    # count up co-occuring bigrams\n",
    "    def bigram_mapper(self, _, line):\n",
    "        emitted_words = set()\n",
    "        lineArr = line.strip().split('\\t')\n",
    "        words = [w.lower() for w in lineArr[0].split(' ')]\n",
    "        # total counts of this particular ngram\n",
    "        ngram_counts = int(lineArr[1])\n",
    "        if len(words) > 1:\n",
    "            for word in words:\n",
    "                if word not in self.vocab or word in emitted_words:\n",
    "                    continue\n",
    "                # emit total ngram count to calculate frequency in the reducer\n",
    "                yield word, ('*', ngram_counts)\n",
    "                emitted_words.add(word)\n",
    "                co_occured_words = set()\n",
    "                for another_word in words:\n",
    "                    if word != another_word and another_word in self.vocab and another_word not in co_occured_words:\n",
    "                        co_occured_words.add(another_word)\n",
    "                        yield word, (another_word, ngram_counts)\n",
    "    \n",
    "    # calculating co-occuring frequencies\n",
    "    def bigram_reducer(self, word, cocurs):       \n",
    "        ngram_total = 0\n",
    "        current_cocurrence = 0\n",
    "        current_coword = None\n",
    "        for coword, count in cocurs:\n",
    "            if coword == '*':\n",
    "                ngram_total += count\n",
    "                continue\n",
    "            if coword == current_coword:\n",
    "                current_cocurrence += count\n",
    "            else:\n",
    "                if current_coword != None:\n",
    "                    yield current_coword, (word, float(current_cocurrence)/float(ngram_total))\n",
    "                current_coword = coword\n",
    "                current_cocurrence = count\n",
    "        if current_coword != None:\n",
    "            yield current_coword, (word, float(current_cocurrence)/float(ngram_total))\n",
    "    \n",
    "    # make the inverted index\n",
    "    def createInvertedIndex(self, cowords, word_freq):\n",
    "        yield cowords, '|'.join([\"%s:%s\"%(word, freq) for (word, freq) in word_freq])\n",
    "     \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    synonym_matrix.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "no configs found; falling back on auto-configuration\n",
      "no configs found; falling back on auto-configuration\n",
      "ignoring partitioner keyword arg (requires real Hadoop): 'org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner'\n",
      "\n",
      "PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "\n",
      "creating tmp directory c:\\temp\\synonym_matrix.kuanlin.20160214.053618.632000\n",
      "writing to c:\\temp\\synonym_matrix.kuanlin.20160214.053618.632000\\step-0-mapper_part-00000\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "writing to c:\\temp\\synonym_matrix.kuanlin.20160214.053618.632000\\step-0-mapper-sorted\n",
      "> sort 'c:\\temp\\synonym_matrix.kuanlin.20160214.053618.632000\\step-0-mapper_part-00000'\n",
      "writing to c:\\temp\\synonym_matrix.kuanlin.20160214.053618.632000\\step-0-reducer_part-00000\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "writing to c:\\temp\\synonym_matrix.kuanlin.20160214.053618.632000\\step-1-mapper_part-00000\n",
      "Counters from step 2:\n",
      "  (no counters found)\n",
      "writing to c:\\temp\\synonym_matrix.kuanlin.20160214.053618.632000\\step-1-mapper-sorted\n",
      "> sort 'c:\\temp\\synonym_matrix.kuanlin.20160214.053618.632000\\step-1-mapper_part-00000'\n",
      "writing to c:\\temp\\synonym_matrix.kuanlin.20160214.053618.632000\\step-1-reducer_part-00000\n",
      "Counters from step 2:\n",
      "  (no counters found)\n",
      "Moving c:\\temp\\synonym_matrix.kuanlin.20160214.053618.632000\\step-1-reducer_part-00000 -> bigram_results\\part-00000\n",
      "removing tmp directory c:\\temp\\synonym_matrix.kuanlin.20160214.053618.632000\n"
     ]
    }
   ],
   "source": [
    " !python synonym_matrix.py googlebooks-eng-all-5gram-20090715-0-filtered.txt --file vocabs.txt --output-dir bigram_results --no-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting synonym_detection_cosine.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile synonym_detection_cosine.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.protocol import RawValueProtocol\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "# synonym detection with cosine similarity\n",
    "\n",
    "class synonym_detection_cosine(MRJob):\n",
    "    \n",
    "    # load up vocabulary\n",
    "    def mapper_init(self):\n",
    "        self.vocab = []\n",
    "        for line in open('vocabs.txt', 'r'):\n",
    "            lineArr = line.strip().split('\\t')\n",
    "            self.vocab.append(lineArr[0].replace('\"', '').lower())\n",
    "            \n",
    "    def mapper(self, _, line):\n",
    "        lineArr = line.strip().replace('\"', '').split('\\t')\n",
    "        co_word = lineArr[0]\n",
    "        doc_terms = {}\n",
    "        # parse the stripes of co-occuring terms\n",
    "        for term_freq in lineArr[1].split('|'):\n",
    "            term = term_freq.split(':')[0]\n",
    "            freq = float(term_freq.split(':')[1])\n",
    "            doc_terms[term] = freq\n",
    "        \n",
    "        # emits components of cosine similarity calculation\n",
    "        for i, term1 in enumerate(self.vocab):\n",
    "            if i != len(self.vocab)-1:\n",
    "                for term2 in self.vocab[i:]:\n",
    "                    if term1 == term2:\n",
    "                        continue\n",
    "                    term1_freq = 0.0\n",
    "                    term2_freq = 0.0\n",
    "                    if term1 in doc_terms:\n",
    "                        term1_freq = doc_terms[term1]\n",
    "                    if term2 in doc_terms:\n",
    "                        term2_freq = doc_terms[term2]\n",
    "                    # collect data into co-occuring terms\n",
    "                    yield term1 + \":\" + term2, (term1_freq**2, term2_freq**2, term1_freq*term2_freq)\n",
    "    \n",
    "    # calculates the overall cosine similarity\n",
    "    def reducer(self, coterms, metric_vals):\n",
    "        a_b = 0.0\n",
    "        a_sqr = 0.0\n",
    "        b_sqr = 0.0\n",
    "        for data in metric_vals:\n",
    "            a_b += data[2]\n",
    "            a_sqr += data[0]\n",
    "            b_sqr += data[1]\n",
    "        consine_sim = a_b/((a_sqr**0.5)*(b_sqr**0.5))\n",
    "        # only print out similarity scoare above 0.5\n",
    "        if consine_sim > 0.5:\n",
    "            yield coterms, consine_sim\n",
    "    \n",
    "            \n",
    "if __name__ == '__main__':\n",
    "    synonym_detection_cosine.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "no configs found; falling back on auto-configuration\n",
      "no configs found; falling back on auto-configuration\n",
      "\n",
      "PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "\n",
      "creating tmp directory c:\\temp\\synonym_detection_cosine.kuanlin.20160214.070507.920000\n",
      "writing to c:\\temp\\synonym_detection_cosine.kuanlin.20160214.070507.920000\\step-0-mapper_part-00000\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "writing to c:\\temp\\synonym_detection_cosine.kuanlin.20160214.070507.920000\\step-0-mapper-sorted\n",
      "> sort 'c:\\temp\\synonym_detection_cosine.kuanlin.20160214.070507.920000\\step-0-mapper_part-00000'\n",
      "writing to c:\\temp\\synonym_detection_cosine.kuanlin.20160214.070507.920000\\step-0-reducer_part-00000\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "Moving c:\\temp\\synonym_detection_cosine.kuanlin.20160214.070507.920000\\step-0-reducer_part-00000 -> synonym_result1\\part-00000\n",
      "removing tmp directory c:\\temp\\synonym_detection_cosine.kuanlin.20160214.070507.920000\n"
     ]
    }
   ],
   "source": [
    " !python synonym_detection_cosine.py bigram_reduced.txt --file vocabs.txt --output-dir synonym_result1 --no-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting synonym_detection_jaccard.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile synonym_detection_jaccard.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.protocol import RawValueProtocol\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "# synonym detection with Jaccard index\n",
    "\n",
    "class synonym_detection_jaccard(MRJob):\n",
    "    \n",
    "    # load up vocabulary\n",
    "    def mapper_init(self):\n",
    "        self.vocab = []\n",
    "        for line in open('vocabs.txt', 'r'):\n",
    "            lineArr = line.strip().split('\\t')\n",
    "            self.vocab.append(lineArr[0].replace('\"', '').lower())\n",
    "            \n",
    "    def mapper(self, _, line):\n",
    "        lineArr = line.strip().replace('\"', '').split('\\t')\n",
    "        co_word = lineArr[0]\n",
    "        doc_terms = {}\n",
    "        # parse the stripes of co-occuring terms\n",
    "        for term_freq in lineArr[1].split('|'):\n",
    "            term = term_freq.split(':')[0]\n",
    "            freq = float(term_freq.split(':')[1])\n",
    "            doc_terms[term] = freq\n",
    "        \n",
    "        # emits components of jaccard similarity calculation\n",
    "        for i, term1 in enumerate(self.vocab):\n",
    "            if i != len(self.vocab)-1:\n",
    "                for term2 in self.vocab[i:]:\n",
    "                    if term1 == term2:\n",
    "                        continue\n",
    "                    term1_score = 0\n",
    "                    term2_score = 0\n",
    "                    if term1 in doc_terms:\n",
    "                        if doc_terms[term1] > 0:\n",
    "                            term1_score = 1\n",
    "                    if term2 in doc_terms:\n",
    "                        if doc_terms[term2] > 0:\n",
    "                            term2_score = 1\n",
    "                    # collect data into co-occuring terms\n",
    "                    yield term1 + \":\" + term2, (term1_score, term2_score)\n",
    "    \n",
    "    # calculates the overall cosine similarity\n",
    "    def reducer(self, coterms, metric_vals):\n",
    "        a_sum = 0\n",
    "        b_sum = 0\n",
    "        a_inter_b = 0\n",
    "        for data in metric_vals:\n",
    "            a_sum += data[0]\n",
    "            b_sum += data[1]\n",
    "            if data[0] > 0 and data[1] > 0:\n",
    "                a_inter_b += 1\n",
    "        jaccard_sim = float(a_inter_b)/float(a_sum + b_sum - a_inter_b)\n",
    "        # only print out similarity scoare above 0.2\n",
    "        if jaccard_sim > 0.2:\n",
    "            yield coterms, jaccard_sim\n",
    "    \n",
    "            \n",
    "if __name__ == '__main__':\n",
    "    synonym_detection_jaccard.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "no configs found; falling back on auto-configuration\n",
      "no configs found; falling back on auto-configuration\n",
      "\n",
      "PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "\n",
      "creating tmp directory c:\\temp\\synonym_detection_jaccard.kuanlin.20160214.071926.856000\n",
      "writing to c:\\temp\\synonym_detection_jaccard.kuanlin.20160214.071926.856000\\step-0-mapper_part-00000\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "writing to c:\\temp\\synonym_detection_jaccard.kuanlin.20160214.071926.856000\\step-0-mapper-sorted\n",
      "> sort 'c:\\temp\\synonym_detection_jaccard.kuanlin.20160214.071926.856000\\step-0-mapper_part-00000'\n",
      "writing to c:\\temp\\synonym_detection_jaccard.kuanlin.20160214.071926.856000\\step-0-reducer_part-00000\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "Moving c:\\temp\\synonym_detection_jaccard.kuanlin.20160214.071926.856000\\step-0-reducer_part-00000 -> synonym_result2\\part-00000\n",
      "removing tmp directory c:\\temp\\synonym_detection_jaccard.kuanlin.20160214.071926.856000\n"
     ]
    }
   ],
   "source": [
    " !python synonym_detection_jaccard.py bigram_reduced.txt --file vocabs.txt --output-dir synonym_result2 --no-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
