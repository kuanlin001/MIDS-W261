{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATSCI W261 ASSIGNMENT 3\n",
    "Section 2<br/>\n",
    "Student: Kuan Lin<br/>\n",
    "Date: 1/29/2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <b>HW3.0. What is a merge sort? Where is it used in Hadoop? How is  a combiner function in the context of Hadoop? Give an example where it can be used and justify why it should be used in the context of this problem. What is the Hadoop shuffle?</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to do..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <b>HW3.1 Use Counters to do EDA (exploratory data analysis and to monitor progress). Counters are lightweight objects in Hadoop that allow you to keep track of system progress in both the map and reduce stages of processing. By default, Hadoop defines a number of standard counters in \"groups\"; these show up in the jobtracker webapp, giving you information such as \"Map input records\", \"Map output records\", etc.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "# mapper to emit counts for the three defined categories\n",
    "import sys\n",
    "for line in sys.stdin:    \n",
    "    line = line.strip().lower()\n",
    "    if line == '' or line.startswith('complaint id'): continue\n",
    "        \n",
    "    # get the complaint category\n",
    "    category = line.split(',')[1].strip()\n",
    "    counter_name = 'other_complaints'\n",
    "    if category == 'debt collection':\n",
    "        counter_name = 'debt_collection_complaints'\n",
    "    elif category == 'mortgage':\n",
    "        counter_name = 'mortgage_complaints'\n",
    "    \n",
    "    # update hadoop counters:\n",
    "    sys.stderr.write(\"reporter:counter:customer_complaints,%s,1\\n\"%counter_name)\n",
    "    # emit regular mapper result\n",
    "    print '%s\\t%s' % (counter_name, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "\n",
    "current_category = None\n",
    "current_count = 0\n",
    "category = None\n",
    "for line in sys.stdin:\n",
    "    category, count = line.strip().split('\\t', 1)\n",
    "    if category == current_category:\n",
    "        current_count += int(count)\n",
    "    else:\n",
    "        if current_category != None:\n",
    "            # write result to STDOUT\n",
    "            print '%s\\t%s' % (current_category, current_count)\n",
    "        current_category = category\n",
    "        current_count = int(count)\n",
    "if category == current_category:\n",
    "    print '%s\\t%s' % (current_category, current_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/30 01:24:08 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /w261/hw3/hw3_1_output\n",
      "16/01/30 01:24:10 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "packageJobJar: [/data/w261/hw3/mapper.py, /data/w261/hw3/reducer.py] [/usr/lib/hadoop/lib/hadoop-streaming-2.6.0.jar] /tmp/streamjob5504775449930135467.jar tmpDir=null\n",
      "16/01/30 01:24:15 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/30 01:24:16 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/30 01:24:18 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/30 01:24:18 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/01/30 01:24:19 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1454112721658_0005\n",
      "16/01/30 01:24:19 INFO impl.YarnClientImpl: Submitted application application_1454112721658_0005\n",
      "16/01/30 01:24:20 INFO mapreduce.Job: The url to track the job: http://ip-172-31-55-170.ec2.internal:8088/proxy/application_1454112721658_0005/\n",
      "16/01/30 01:24:20 INFO mapreduce.Job: Running job: job_1454112721658_0005\n",
      "16/01/30 01:24:36 INFO mapreduce.Job: Job job_1454112721658_0005 running in uber mode : false\n",
      "16/01/30 01:24:36 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/01/30 01:25:02 INFO mapreduce.Job:  map 5% reduce 0%\n",
      "16/01/30 01:25:04 INFO mapreduce.Job:  map 8% reduce 0%\n",
      "16/01/30 01:25:06 INFO mapreduce.Job:  map 13% reduce 0%\n",
      "16/01/30 01:25:07 INFO mapreduce.Job:  map 19% reduce 0%\n",
      "16/01/30 01:25:09 INFO mapreduce.Job:  map 27% reduce 0%\n",
      "16/01/30 01:25:10 INFO mapreduce.Job:  map 33% reduce 0%\n",
      "16/01/30 01:25:12 INFO mapreduce.Job:  map 41% reduce 0%\n",
      "16/01/30 01:25:13 INFO mapreduce.Job:  map 49% reduce 0%\n",
      "16/01/30 01:25:15 INFO mapreduce.Job:  map 55% reduce 0%\n",
      "16/01/30 01:25:16 INFO mapreduce.Job:  map 61% reduce 0%\n",
      "16/01/30 01:25:17 INFO mapreduce.Job:  map 79% reduce 0%\n",
      "16/01/30 01:25:19 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/30 01:25:34 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/30 01:25:35 INFO mapreduce.Job: Job job_1454112721658_0005 completed successfully\n",
      "16/01/30 01:25:35 INFO mapreduce.Job: Counters: 52\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=7392134\n",
      "\t\tFILE: Number of bytes written=15126327\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=50910095\n",
      "\t\tHDFS: Number of bytes written=84\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=78498\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=13955\n",
      "\t\tTotal time spent by all map tasks (ms)=78498\n",
      "\t\tTotal time spent by all reduce tasks (ms)=13955\n",
      "\t\tTotal vcore-seconds taken by all map tasks=78498\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=13955\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=80381952\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=14289920\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=312913\n",
      "\t\tMap output records=312912\n",
      "\t\tMap output bytes=6766304\n",
      "\t\tMap output materialized bytes=7392140\n",
      "\t\tInput split bytes=212\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=3\n",
      "\t\tReduce shuffle bytes=7392140\n",
      "\t\tReduce input records=312912\n",
      "\t\tReduce output records=3\n",
      "\t\tSpilled Records=625824\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=1140\n",
      "\t\tCPU time spent (ms)=21030\n",
      "\t\tPhysical memory (bytes) snapshot=565727232\n",
      "\t\tVirtual memory (bytes) snapshot=7589130240\n",
      "\t\tTotal committed heap usage (bytes)=392372224\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tcustomer_complaints\n",
      "\t\tdebt_collection_complaints=44372\n",
      "\t\tmortgage_complaints=125752\n",
      "\t\tother_complaints=142788\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=50909883\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=84\n",
      "16/01/30 01:25:35 INFO streaming.StreamJob: Output directory: /w261/hw3/hw3_1_output\n"
     ]
    }
   ],
   "source": [
    "!chmod +x mapper.py; chmod +x reducer.py;\n",
    "!hdfs dfs -rm -r /w261/hw3/hw3_1_output;\n",
    "!$HADOOP_HOME/bin/hadoop jar $HADOOP_HOME/lib/hadoop-*streaming*.jar -mapper mapper.py -file /data/w261/hw3/mapper.py -reducer reducer.py -file /data/w261/hw3/reducer.py -input /w261/hw3/Consumer_Complaints.csv -output /w261/hw3/hw3_1_output;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "debt_collection_complaints\t44372\r\n",
      "mortgage_complaints\t125752\r\n",
      "other_complaints\t142788\r\n"
     ]
    }
   ],
   "source": [
    "# examing the results\n",
    "!hdfs dfs -cat /w261/hw3/hw3_1_output/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* <b>HW 3.2 Analyze the performance of your Mappers, Combiners and Reducers using Counters</b>\n",
    "Perform a word count analysis of this single record dataset using a Mapper and Reducer \n",
    "based WordCount (i.e., no combiners are used here) using user defined Counters to count up \n",
    "how many time the mapper and reducer are called. What is the value of your user defined Mapper Counter,\n",
    " and Reducer Counter after completing this word count job. The answer  should be 1 and 4 respectively.\n",
    " Please explain.\n",
    " Please use multiple mappers and reducers for these jobs (at least 2 mappers and 2 reducers).\n",
    "Perform a word count analysis of the Issue column of the Consumer Complaints  Dataset using \n",
    "a Mapper and Reducer based WordCount (i.e., no combiners used anywhere)  using user defined \n",
    "Counters to count up how many time the mapper and reducer are called. What is the value of \n",
    "your user defined Mapper Counter, and Reducer Counter after completing your word count job. \n",
    "\n",
    "Perform a word count analysis of the Issue column of the Consumer Complaints  Dataset using a Mapper, \n",
    "Reducer, and standalone combiner (i.e., not an in-memory combiner) based WordCount using user defined \n",
    "Counters to count up how many time the mapper, combiner, reducer are called. What is the value of your \n",
    "user defined Mapper Counter, and Reducer Counter after completing your word count job. \n",
    "Using a single reducer: What are the top 50 most frequent terms in your word count analysis? \n",
    "Present the top 50 terms and their frequency and their relative frequency. Present the top 50 terms and \n",
    "their frequency and their relative frequency. If there are ties please sort the tokens in \n",
    "alphanumeric/string order. Present bottom 10 tokens (least frequent items). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing hw3_2_input.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile hw3_2_input.txt\n",
    "foo foo quux labs foo bar quux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# put the input file into hdfs\n",
    "!hdfs dfs -put hw3_2_input.txt /w261/hw3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "# record mapper invokation\n",
    "sys.stderr.write(\"reporter:counter:custom_counters,mapper_count,1\\n\")\n",
    "for line in sys.stdin:    \n",
    "    line = line.strip().lower()\n",
    "    if line == '': continue\n",
    "    for word in line.split(' '):\n",
    "        print '%s\\t%s' % (word, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "# record reducer invokation\n",
    "sys.stderr.write(\"reporter:counter:custom_counters,reducer_count,1\\n\")\n",
    "from operator import itemgetter\n",
    "\n",
    "current_word = None\n",
    "current_count = 0\n",
    "word = None\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "    if len(line.split('\\t')) != 2: continue\n",
    "\n",
    "    # parse the input we got from mapper.py\n",
    "    word, count = line.split('\\t', 1)\n",
    "\n",
    "    # convert count (currently a string) to int\n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        # count was not a number, so silently\n",
    "        # ignore/discard this line\n",
    "        continue\n",
    "\n",
    "    # this IF-switch only works because Hadoop sorts map output\n",
    "    # by key (here: word) before it is passed to the reducer\n",
    "    if current_word == word:\n",
    "        current_count += count\n",
    "    else:\n",
    "        if current_word:\n",
    "            # write result to STDOUT\n",
    "            print '%s\\t%s' % (current_word, current_count)\n",
    "        current_count = count\n",
    "        current_word = word\n",
    "\n",
    "# do not forget to output the last word if needed!\n",
    "if current_word == word:\n",
    "    print '%s\\t%s' % (current_word, current_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/w261/hw3/hw3_2_output': No such file or directory\n",
      "16/01/30 02:07:58 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "packageJobJar: [/data/w261/hw3/mapper.py, /data/w261/hw3/reducer.py] [/usr/lib/hadoop/lib/hadoop-streaming-2.6.0.jar] /tmp/streamjob1960028508392768622.jar tmpDir=null\n",
      "16/01/30 02:08:03 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/30 02:08:04 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/30 02:08:06 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/30 02:08:06 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/01/30 02:08:06 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "16/01/30 02:08:06 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/01/30 02:08:07 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1454112721658_0008\n",
      "16/01/30 02:08:07 INFO impl.YarnClientImpl: Submitted application application_1454112721658_0008\n",
      "16/01/30 02:08:07 INFO mapreduce.Job: The url to track the job: http://ip-172-31-55-170.ec2.internal:8088/proxy/application_1454112721658_0008/\n",
      "16/01/30 02:08:07 INFO mapreduce.Job: Running job: job_1454112721658_0008\n",
      "16/01/30 02:08:25 INFO mapreduce.Job: Job job_1454112721658_0008 running in uber mode : false\n",
      "16/01/30 02:08:25 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/01/30 02:08:37 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/30 02:09:19 INFO mapreduce.Job:  map 100% reduce 25%\n",
      "16/01/30 02:09:24 INFO mapreduce.Job:  map 100% reduce 50%\n",
      "16/01/30 02:09:27 INFO mapreduce.Job:  map 100% reduce 75%\n",
      "16/01/30 02:09:28 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/30 02:09:30 INFO mapreduce.Job: Job job_1454112721658_0008 completed successfully\n",
      "16/01/30 02:09:31 INFO mapreduce.Job: Counters: 52\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=83\n",
      "\t\tFILE: Number of bytes written=570158\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=128\n",
      "\t\tHDFS: Number of bytes written=26\n",
      "\t\tHDFS: Number of read operations=15\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=8\n",
      "\tJob Counters \n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=1\n",
      "\t\tLaunched reduce tasks=5\n",
      "\t\tData-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=9910\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=181621\n",
      "\t\tTotal time spent by all map tasks (ms)=9910\n",
      "\t\tTotal time spent by all reduce tasks (ms)=181621\n",
      "\t\tTotal vcore-seconds taken by all map tasks=9910\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=181621\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=10147840\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=185979904\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=1\n",
      "\t\tMap output records=7\n",
      "\t\tMap output bytes=45\n",
      "\t\tMap output materialized bytes=83\n",
      "\t\tInput split bytes=98\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=4\n",
      "\t\tReduce shuffle bytes=83\n",
      "\t\tReduce input records=7\n",
      "\t\tReduce output records=4\n",
      "\t\tSpilled Records=14\n",
      "\t\tShuffled Maps =4\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tGC time elapsed (ms)=2948\n",
      "\t\tCPU time spent (ms)=7510\n",
      "\t\tPhysical memory (bytes) snapshot=683544576\n",
      "\t\tVirtual memory (bytes) snapshot=12668096512\n",
      "\t\tTotal committed heap usage (bytes)=409276416\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tcustom_counters\n",
      "\t\tmapper_count=1\n",
      "\t\treducer_count=4\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=30\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=26\n",
      "16/01/30 02:09:31 INFO streaming.StreamJob: Output directory: /w261/hw3/hw3_2_output\n"
     ]
    }
   ],
   "source": [
    "!chmod +x mapper.py; chmod +x reducer.py;\n",
    "!hdfs dfs -rm -r /w261/hw3/hw3_2_output;\n",
    "!$HADOOP_HOME/bin/hadoop jar $HADOOP_HOME/lib/hadoop-*streaming*.jar -D mapred.map.tasks=1 -D mapred.reduce.tasks=4 -mapper mapper.py -file /data/w261/hw3/mapper.py -reducer reducer.py -file /data/w261/hw3/reducer.py -input /w261/hw3/hw3_2_input.txt -output /w261/hw3/hw3_2_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quux\t2\n",
      "foo\t3\n",
      "bar\t1\n",
      "labs\t1\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /w261/hw3/hw3_2_output/part-0000*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there is only one line and four distinct words, the optimal setting should be using 1 mapper and 4 reducers.  However, it seems that the default setting for Hadoop is to do 2 maper and 1 reducer.  If I specify mapper and reducer task counts in Hadoop Streaming parameter and 1 and 4, I now get 1 and 4 as specified in the homework problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <b>HW3.2 continues</b>: Please use multiple mappers and reducers for these jobs (at least 2 mappers and 2 reducers).\n",
    "Perform a word count analysis of the Issue column of the Consumer Complaints  Dataset using \n",
    "a Mapper and Reducer based WordCount (i.e., no combiners used anywhere)  using user defined \n",
    "Counters to count up how many time the mapper and reducer are called. What is the value of \n",
    "your user defined Mapper Counter, and Reducer Counter after completing your word count job. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "# record mapper invokation\n",
    "sys.stderr.write(\"reporter:counter:custom_counters,mapper_count,1\\n\")\n",
    "for line in sys.stdin:    \n",
    "    line = line.strip().lower()\n",
    "    if line == '' or line.startswith(\"complaint id\"): continue\n",
    "    issue_text = line.split(',')[3]\n",
    "    print '%s\\t%s' % (issue_text, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/30 02:28:36 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /w261/hw3/hw3_2_1_output\n",
      "16/01/30 02:28:38 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "packageJobJar: [/data/w261/hw3/mapper.py, /data/w261/hw3/reducer.py] [/usr/lib/hadoop/lib/hadoop-streaming-2.6.0.jar] /tmp/streamjob1057362170451352704.jar tmpDir=null\n",
      "16/01/30 02:28:43 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/30 02:28:44 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/30 02:28:46 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/30 02:28:46 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/01/30 02:28:46 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "16/01/30 02:28:46 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/01/30 02:28:46 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1454112721658_0011\n",
      "16/01/30 02:28:47 INFO impl.YarnClientImpl: Submitted application application_1454112721658_0011\n",
      "16/01/30 02:28:47 INFO mapreduce.Job: The url to track the job: http://ip-172-31-55-170.ec2.internal:8088/proxy/application_1454112721658_0011/\n",
      "16/01/30 02:28:47 INFO mapreduce.Job: Running job: job_1454112721658_0011\n",
      "16/01/30 02:29:05 INFO mapreduce.Job: Job job_1454112721658_0011 running in uber mode : false\n",
      "16/01/30 02:29:05 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/01/30 02:29:30 INFO mapreduce.Job:  map 17% reduce 0%\n",
      "16/01/30 02:29:31 INFO mapreduce.Job:  map 37% reduce 0%\n",
      "16/01/30 02:29:33 INFO mapreduce.Job:  map 53% reduce 0%\n",
      "16/01/30 02:29:34 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "16/01/30 02:29:35 INFO mapreduce.Job:  map 83% reduce 0%\n",
      "16/01/30 02:29:36 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/30 02:29:59 INFO mapreduce.Job:  map 100% reduce 50%\n",
      "16/01/30 02:30:01 INFO mapreduce.Job:  map 100% reduce 90%\n",
      "16/01/30 02:30:03 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/30 02:30:04 INFO mapreduce.Job: Job job_1454112721658_0011 completed successfully\n",
      "16/01/30 02:30:05 INFO mapreduce.Job: Counters: 51\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=8735828\n",
      "\t\tFILE: Number of bytes written=17927758\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=50910095\n",
      "\t\tHDFS: Number of bytes written=2375\n",
      "\t\tHDFS: Number of read operations=12\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=2\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=56358\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=48833\n",
      "\t\tTotal time spent by all map tasks (ms)=56358\n",
      "\t\tTotal time spent by all reduce tasks (ms)=48833\n",
      "\t\tTotal vcore-seconds taken by all map tasks=56358\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=48833\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=57710592\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=50004992\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=312913\n",
      "\t\tMap output records=312912\n",
      "\t\tMap output bytes=8109992\n",
      "\t\tMap output materialized bytes=8735840\n",
      "\t\tInput split bytes=212\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=80\n",
      "\t\tReduce shuffle bytes=8735840\n",
      "\t\tReduce input records=312912\n",
      "\t\tReduce output records=79\n",
      "\t\tSpilled Records=625824\n",
      "\t\tShuffled Maps =4\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tGC time elapsed (ms)=1171\n",
      "\t\tCPU time spent (ms)=14030\n",
      "\t\tPhysical memory (bytes) snapshot=678158336\n",
      "\t\tVirtual memory (bytes) snapshot=10123059200\n",
      "\t\tTotal committed heap usage (bytes)=453255168\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tcustom_counters\n",
      "\t\tmapper_count=2\n",
      "\t\treducer_count=2\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=50909883\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2375\n",
      "16/01/30 02:30:05 INFO streaming.StreamJob: Output directory: /w261/hw3/hw3_2_1_output\n"
     ]
    }
   ],
   "source": [
    "!chmod +x mapper.py; chmod +x reducer.py;\n",
    "!hdfs dfs -rm -r /w261/hw3/hw3_2_1_output;\n",
    "!$HADOOP_HOME/bin/hadoop jar $HADOOP_HOME/lib/hadoop-*streaming*.jar -D mapred.map.tasks=2 -D mapred.reduce.tasks=2 -mapper mapper.py -file /data/w261/hw3/mapper.py -reducer reducer.py -file /data/w261/hw3/reducer.py -input /w261/hw3/Consumer_Complaints.csv -output /w261/hw3/hw3_2_1_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mapper and reducer counter are both 2, which is exactly what I specified in the parameters.  This is not always going to be the case though - we don't have direct control over how Hadoop is goint to split jobs.  The parameters is just a hint for the system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <b>HW3.2 continues</b>Perform a word count analysis of the Issue column of the Consumer Complaints  Dataset using a Mapper, \n",
    "Reducer, and standalone combiner (i.e., not an in-memory combiner) based WordCount using user defined \n",
    "Counters to count up how many time the mapper, combiner, reducer are called. What is the value of your \n",
    "user defined Mapper Counter, and Reducer Counter after completing your word count job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting combiner.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile combiner.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "# record mapper invokation\n",
    "sys.stderr.write(\"reporter:counter:custom_counters,combiner_count,1\\n\")\n",
    "from operator import itemgetter\n",
    "\n",
    "current_word = None\n",
    "current_count = 0\n",
    "word = None\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "    if len(line.split('\\t')) != 2: continue\n",
    "\n",
    "    # parse the input we got from mapper.py\n",
    "    word, count = line.split('\\t', 1)\n",
    "\n",
    "    # convert count (currently a string) to int\n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        # count was not a number, so silently\n",
    "        # ignore/discard this line\n",
    "        continue\n",
    "\n",
    "    # this IF-switch only works because Hadoop sorts map output\n",
    "    # by key (here: word) before it is passed to the reducer\n",
    "    if current_word == word:\n",
    "        current_count += count\n",
    "    else:\n",
    "        if current_word:\n",
    "            # write result to STDOUT\n",
    "            print '%s\\t%s' % (current_word, current_count)\n",
    "        current_count = count\n",
    "        current_word = word\n",
    "\n",
    "# do not forget to output the last word if needed!\n",
    "if current_word == word:\n",
    "    print '%s\\t%s' % (current_word, current_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/30 04:11:29 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /w261/hw3/hw3_2_2_output\n",
      "16/01/30 04:11:31 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "packageJobJar: [/data/w261/hw3/mapper.py, /data/w261/hw3/combiner.py, /data/w261/hw3/reducer.py] [/usr/lib/hadoop/lib/hadoop-streaming-2.6.0.jar] /tmp/streamjob7026719206050507258.jar tmpDir=null\n",
      "16/01/30 04:11:36 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/30 04:11:37 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/30 04:11:38 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/30 04:11:38 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/01/30 04:11:38 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "16/01/30 04:11:38 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/01/30 04:11:39 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1454112721658_0018\n",
      "16/01/30 04:11:40 INFO impl.YarnClientImpl: Submitted application application_1454112721658_0018\n",
      "16/01/30 04:11:40 INFO mapreduce.Job: The url to track the job: http://ip-172-31-55-170.ec2.internal:8088/proxy/application_1454112721658_0018/\n",
      "16/01/30 04:11:40 INFO mapreduce.Job: Running job: job_1454112721658_0018\n",
      "16/01/30 04:11:58 INFO mapreduce.Job: Job job_1454112721658_0018 running in uber mode : false\n",
      "16/01/30 04:11:58 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/01/30 04:12:25 INFO mapreduce.Job:  map 17% reduce 0%\n",
      "16/01/30 04:12:26 INFO mapreduce.Job:  map 37% reduce 0%\n",
      "16/01/30 04:12:28 INFO mapreduce.Job:  map 53% reduce 0%\n",
      "16/01/30 04:12:29 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "16/01/30 04:12:33 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/30 04:12:55 INFO mapreduce.Job:  map 100% reduce 50%\n",
      "16/01/30 04:12:57 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/30 04:12:58 INFO mapreduce.Job: Job job_1454112721658_0018 completed successfully\n",
      "16/01/30 04:12:58 INFO mapreduce.Job: Counters: 52\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=4588\n",
      "\t\tFILE: Number of bytes written=467754\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=50910095\n",
      "\t\tHDFS: Number of bytes written=2908\n",
      "\t\tHDFS: Number of read operations=12\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=2\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=65066\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=38749\n",
      "\t\tTotal time spent by all map tasks (ms)=65066\n",
      "\t\tTotal time spent by all reduce tasks (ms)=38749\n",
      "\t\tTotal vcore-seconds taken by all map tasks=65066\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=38749\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=66627584\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=39678976\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=312913\n",
      "\t\tMap output records=312912\n",
      "\t\tMap output bytes=8109992\n",
      "\t\tMap output materialized bytes=4600\n",
      "\t\tInput split bytes=212\n",
      "\t\tCombine input records=312912\n",
      "\t\tCombine output records=146\n",
      "\t\tReduce input groups=98\n",
      "\t\tReduce shuffle bytes=4600\n",
      "\t\tReduce input records=146\n",
      "\t\tReduce output records=98\n",
      "\t\tSpilled Records=292\n",
      "\t\tShuffled Maps =4\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tGC time elapsed (ms)=1158\n",
      "\t\tCPU time spent (ms)=12410\n",
      "\t\tPhysical memory (bytes) snapshot=677875712\n",
      "\t\tVirtual memory (bytes) snapshot=10129555456\n",
      "\t\tTotal committed heap usage (bytes)=453255168\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tcustom_counters\n",
      "\t\tcombiner_count=4\n",
      "\t\tmapper_count=2\n",
      "\t\treducer_count=2\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=50909883\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2908\n",
      "16/01/30 04:12:58 INFO streaming.StreamJob: Output directory: /w261/hw3/hw3_2_2_output\n"
     ]
    }
   ],
   "source": [
    "!chmod +x mapper.py; chmod +x reducer.py; chmod +x combiner.py\n",
    "!hdfs dfs -rm -r /w261/hw3/hw3_2_2_output;\n",
    "!$HADOOP_HOME/bin/hadoop jar $HADOOP_HOME/lib/hadoop-*streaming*.jar -D mapred.map.tasks=2 -D mapred.reduce.tasks=2 -mapper mapper.py -file /data/w261/hw3/mapper.py -combiner combiner.py -file /data/w261/hw3/combiner.py -reducer reducer.py -file /data/w261/hw3/reducer.py -input /w261/hw3/Consumer_Complaints.csv -output /w261/hw3/hw3_2_2_output\n",
    "#!$HADOOP_HOME/bin/hadoop jar $HADOOP_HOME/lib/hadoop-*streaming*.jar -D mapred.map.tasks=2 -D mapred.reduce.tasks=2 -mapper mapper.py -file /data/w261/hw3/mapper.py -reducer reducer.py -file /data/w261/hw3/reducer.py -input /w261/hw3/Consumer_Complaints.csv -output /w261/hw3/hw3_2_2_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combiner Counter: 4<br/>\n",
    "Mapper Counter: 2<br/>\n",
    "Reducer Counter: 2<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <b>HW3.2 Continue</b>: Using a single reducer: What are the top 50 most frequent terms in your word count analysis? \n",
    "Present the top 50 terms and their frequency and their relative frequency. If there are ties please sort the tokens in \n",
    "alphanumeric/string order. Present bottom 10 tokens (least frequent items). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"account opening\t16205\r\n",
      "\"making/receiving payments\t3226\r\n",
      "advertising and marketing\t1193\r\n",
      "arbitration\t168\r\n",
      "balance transfer\t502\r\n",
      "balance transfer fee\t95\r\n",
      "bankruptcy\t222\r\n",
      "can't contact lender\t221\r\n",
      "cash advance\t136\r\n",
      "cash advance fee\t104\r\n",
      "cat: Unable to write to output stream.\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /w261/hw3/hw3_2_2_output/* | head -10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import csv\n",
    "# record mapper invokation\n",
    "sys.stderr.write(\"reporter:counter:custom_counters,mapper_count,1\\n\")\n",
    "\n",
    "total_count = 0\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    line = line.strip().lower()\n",
    "    if line == '' or line.startswith(\"complaint id\"): continue\n",
    "    lineArr = list(csv.reader(line.splitlines(), delimiter=',', quotechar='\"'))[0]\n",
    "    issue_text = lineArr[3]\n",
    "    total_count += 1\n",
    "    if issue_text.strip() == '': issue_text = 'blank'\n",
    "    print '%s\\t%s\\t%s' % (\"issue_words\", issue_text, 1)\n",
    "\n",
    "# print total word count from this mapper to calculate relative frequency\n",
    "print '%s\\t%s\\t%s' % (\"issue_words\", \"*\", total_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "# record reducer invokation\n",
    "sys.stderr.write(\"reporter:counter:custom_counters,reducer_count,1\\n\")\n",
    "total_count = 0\n",
    "current_issue = None\n",
    "current_count = 0\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    if line == '': continue\n",
    "    lineArr = line.split('\\t')\n",
    "    issue_text = lineArr[1]\n",
    "    issue_count = int(lineArr[2])\n",
    "    # aggregate total counts\n",
    "    if issue_text == \"*\":\n",
    "        total_count += issue_count\n",
    "    else:\n",
    "        if issue_text == current_issue:\n",
    "            current_count += issue_count\n",
    "        else:\n",
    "            if current_issue != None and total_count != 0:\n",
    "                print \"%s\\t%s\\t%.6f\" % (current_issue, current_count, float(current_count)/float(total_count))\n",
    "            current_issue = issue_text\n",
    "            current_count = issue_count\n",
    "if current_issue != None and total_count != 0:\n",
    "    print \"%s\\t%s\\t%.6f\" % (current_issue, current_count, float(current_count)/float(total_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/30 22:35:59 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /w261/hw3/hw3_2_3_output\n",
      "16/01/30 22:36:02 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "packageJobJar: [/data/w261/hw3/mapper.py, /data/w261/hw3/reducer.py] [/usr/lib/hadoop/lib/hadoop-streaming-2.6.0.jar] /tmp/streamjob3516562719083472012.jar tmpDir=null\n",
      "16/01/30 22:36:06 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/30 22:36:07 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/30 22:36:09 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/30 22:36:09 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/01/30 22:36:09 INFO Configuration.deprecation: mapred.output.key.comparator.class is deprecated. Instead, use mapreduce.job.output.key.comparator.class\n",
      "16/01/30 22:36:09 INFO Configuration.deprecation: mapred.text.key.comparator.options is deprecated. Instead, use mapreduce.partition.keycomparator.options\n",
      "16/01/30 22:36:10 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1454188161308_0010\n",
      "16/01/30 22:36:10 INFO impl.YarnClientImpl: Submitted application application_1454188161308_0010\n",
      "16/01/30 22:36:11 INFO mapreduce.Job: The url to track the job: http://ip-172-31-55-170.ec2.internal:8088/proxy/application_1454188161308_0010/\n",
      "16/01/30 22:36:11 INFO mapreduce.Job: Running job: job_1454188161308_0010\n",
      "16/01/30 22:36:27 INFO mapreduce.Job: Job job_1454188161308_0010 running in uber mode : false\n",
      "16/01/30 22:36:27 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/01/30 22:36:52 INFO mapreduce.Job:  map 18% reduce 0%\n",
      "16/01/30 22:36:55 INFO mapreduce.Job:  map 42% reduce 0%\n",
      "16/01/30 22:36:58 INFO mapreduce.Job:  map 52% reduce 0%\n",
      "16/01/30 22:36:59 INFO mapreduce.Job:  map 62% reduce 0%\n",
      "16/01/30 22:37:02 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "16/01/30 22:37:03 INFO mapreduce.Job:  map 83% reduce 0%\n",
      "16/01/30 22:37:04 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/30 22:37:20 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/30 22:37:20 INFO mapreduce.Job: Job job_1454188161308_0010 completed successfully\n",
      "16/01/30 22:37:21 INFO mapreduce.Job: Counters: 51\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=16198865\n",
      "\t\tFILE: Number of bytes written=32741682\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=50910095\n",
      "\t\tHDFS: Number of bytes written=3188\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=68832\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=13977\n",
      "\t\tTotal time spent by all map tasks (ms)=68832\n",
      "\t\tTotal time spent by all reduce tasks (ms)=13977\n",
      "\t\tTotal vcore-seconds taken by all map tasks=68832\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=13977\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=70483968\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=14312448\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=312913\n",
      "\t\tMap output records=312914\n",
      "\t\tMap output bytes=15573031\n",
      "\t\tMap output materialized bytes=16198871\n",
      "\t\tInput split bytes=212\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=81\n",
      "\t\tReduce shuffle bytes=16198871\n",
      "\t\tReduce input records=312914\n",
      "\t\tReduce output records=79\n",
      "\t\tSpilled Records=625828\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=1061\n",
      "\t\tCPU time spent (ms)=17950\n",
      "\t\tPhysical memory (bytes) snapshot=578310144\n",
      "\t\tVirtual memory (bytes) snapshot=7588909056\n",
      "\t\tTotal committed heap usage (bytes)=392372224\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tcustom_counters\n",
      "\t\tmapper_count=2\n",
      "\t\treducer_count=1\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=50909883\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=3188\n",
      "16/01/30 22:37:21 INFO streaming.StreamJob: Output directory: /w261/hw3/hw3_2_3_output\n"
     ]
    }
   ],
   "source": [
    "!chmod +x mapper.py; chmod +x reducer.py;\n",
    "!hdfs dfs -rm -r /w261/hw3/hw3_2_3_output;\n",
    "!$HADOOP_HOME/bin/hadoop jar $HADOOP_HOME/lib/hadoop-*streaming*.jar \\\n",
    "-D stream.num.map.output.key.fields=3 \\\n",
    "-D mapreduce.partition.keypartitioner.options=-k1,1 \\\n",
    "-D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D mapred.text.key.comparator.options=\"-k3nr -k2\" \\\n",
    "-mapper mapper.py -file /data/w261/hw3/mapper.py \\\n",
    "-reducer reducer.py -file /data/w261/hw3/reducer.py \\\n",
    "-input /w261/hw3/Consumer_Complaints.csv \\\n",
    "-output /w261/hw3/hw3_2_3_output;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 50 frequent items and their relative frequencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loan modification,collection,foreclosure\t70487\t0.225261\r\n",
      "loan servicing, payments, escrow account\t36767\t0.117499\r\n",
      "incorrect information on credit report\t29069\t0.092898\r\n",
      "cont'd attempts collect debt not owed\t17972\t0.057435\r\n",
      "account opening, closing, or management\t16205\t0.051788\r\n",
      "deposits and withdrawals\t10555\t0.033732\r\n",
      "communication tactics\t8671\t0.027711\r\n",
      "application, originator, mortgage broker\t8625\t0.027564\r\n",
      "disclosure verification of debt\t7655\t0.024464\r\n",
      "billing disputes\t6938\t0.022172\r\n",
      "other\t6273\t0.020047\r\n",
      "problems caused by my funds being low\t5663\t0.018098\r\n",
      "credit reporting company's investigation\t4858\t0.015525\r\n",
      "managing the loan or lease\t4560\t0.014573\r\n",
      "unable to get credit report/credit score\t4357\t0.013924\r\n",
      "settlement process and costs\t4350\t0.013902\r\n",
      "repaying your loan\t3844\t0.012285\r\n",
      "problems when you are unable to pay\t3821\t0.012211\r\n",
      "false statements or representation\t3621\t0.011572\r\n",
      "improper contact or sharing of info\t3489\t0.011150\r\n",
      "apr or interest rate\t3431\t0.010965\r\n",
      "identity theft / fraud / embezzlement\t3276\t0.010469\r\n",
      "making/receiving payments, sending money\t3226\t0.010310\r\n",
      "taking/threatening an illegal action\t2964\t0.009472\r\n",
      "closing/cancelling account\t2795\t0.008932\r\n",
      "credit decision / underwriting\t2774\t0.008865\r\n",
      "using a debit or atm card\t2422\t0.007740\r\n",
      "dealing with my lender or servicer\t1944\t0.006213\r\n",
      "late fee\t1797\t0.005743\r\n",
      "credit reporting\t1701\t0.005436\r\n",
      "can't repay my loan\t1647\t0.005263\r\n",
      "credit determination\t1490\t0.004762\r\n",
      "improper use of my credit report\t1477\t0.004720\r\n",
      "credit monitoring or identity protection\t1453\t0.004643\r\n",
      "customer service / customer relations\t1367\t0.004369\r\n",
      "credit card protection / debt protection\t1343\t0.004292\r\n",
      "taking out the loan or lease\t1242\t0.003969\r\n",
      "billing statement\t1220\t0.003899\r\n",
      "advertising and marketing\t1193\t0.003813\r\n",
      "payoff process\t1155\t0.003691\r\n",
      "credit line increase/decrease\t1149\t0.003672\r\n",
      "transaction issue\t1098\t0.003509\r\n",
      "other fee\t1075\t0.003435\r\n",
      "delinquent account\t1061\t0.003391\r\n",
      "collection practices\t1003\t0.003205\r\n",
      "rewards\t1002\t0.003202\r\n",
      "collection debt dispute\t904\t0.002889\r\n",
      "charged fees or interest i didn't expect\t807\t0.002579\r\n",
      "unsolicited issuance of credit card\t640\t0.002045\r\n",
      "fraud or scam\t566\t0.001809\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /w261/hw3/hw3_2_3_output/part-00000 | sort -t$'\\t' -k2,2nr -k1,1 | head -50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bottom 10 tokens and their counts and relative frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blank\t4\t0.000013\r\n",
      "incorrect/missing disclosures or info\t64\t0.000205\r\n",
      "charged bank acct wrong day or amt\t71\t0.000227\r\n",
      "convenience checks\t75\t0.000240\r\n",
      "payment to acct not credited\t92\t0.000294\r\n",
      "balance transfer fee\t95\t0.000304\r\n",
      "wrong amount charged or received\t98\t0.000313\r\n",
      "cash advance fee\t104\t0.000332\r\n",
      "received a loan i didn't apply for\t118\t0.000377\r\n",
      "overlimit fee\t127\t0.000406\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /w261/hw3/hw3_2_3_output/part-00000 | sort -t$'\\t' -k2,2n -k1,1 | head -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <b>HW3.3. Shopping Cart Analysis</b><br/>\n",
    "Product Recommendations: The action or practice of selling additional products or services \n",
    "to existing customers is called cross-selling. Giving product recommendation is \n",
    "one of the examples of cross-selling that are frequently used by online retailers. \n",
    "One simple method to give product recommendations is to recommend products that are frequently\n",
    "browsed together by the customers.<br/>\n",
    "Do some exploratory data analysis of this dataset. \n",
    "How many unique items are available from this supplier?\n",
    "Using a single reducer: Report your findings such as number of unique products; \n",
    "largest basket; report the top 50 most frequently purchased items,  their frequency,  \n",
    "and their relative frequency (break ties by sorting the products alphabetical order) etc. \n",
    "using Hadoop Map-Reduce. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "# record mapper invokation\n",
    "sys.stderr.write(\"reporter:counter:custom_counters,mapper_count,1\\n\")\n",
    "\n",
    "total_count = 0\n",
    "largest_basket_size = 0\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    line = line.strip().lower()\n",
    "    if line == '': continue\n",
    "    lineArr = line.split(' ')\n",
    "    # record largest basket per mapper\n",
    "    if len(lineArr) > largest_basket_size:\n",
    "        largest_basket_size = len(lineArr)\n",
    "    for product in lineArr:\n",
    "        total_count += 1\n",
    "        print '%s\\t%s\\t%s' % (\"products\", product, 1)\n",
    "\n",
    "# print total word count from this mapper to calculate relative frequency\n",
    "print '%s\\t%s\\t%s' % (\"products\", \"*\", total_count)\n",
    "# print additional meta data for EDA analysis\n",
    "print '%s\\t%s\\t%s' % (\"meta_data\", \"largest_basket\", largest_basket_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "# record reducer invokation\n",
    "sys.stderr.write(\"reporter:counter:custom_counters,reducer_count,1\\n\")\n",
    "total_count = 0\n",
    "current_issue = None\n",
    "current_count = 0\n",
    "largest_basket_size = 0\n",
    "unique_products = set()\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    if line == '': continue\n",
    "    lineArr = line.split('\\t')\n",
    "    issue_text = lineArr[1]\n",
    "    issue_count = int(lineArr[2])\n",
    "    # record largest basket\n",
    "    if issue_text == 'largest_basket':\n",
    "        if issue_count > largest_basket_size:\n",
    "            largest_basket_size = issue_count\n",
    "        continue\n",
    "    \n",
    "    # aggregate total counts\n",
    "    if issue_text == \"*\":\n",
    "        total_count += issue_count\n",
    "    else:\n",
    "        unique_products.add(issue_text)\n",
    "        if issue_text == current_issue:\n",
    "            current_count += issue_count\n",
    "        else:\n",
    "            if current_issue != None and total_count != 0:\n",
    "                print \"%s\\t%s\\t%.6f\" % (current_issue, current_count, float(current_count)/float(total_count))\n",
    "            current_issue = issue_text\n",
    "            current_count = issue_count\n",
    "if current_issue != None and total_count != 0:\n",
    "    print \"%s\\t%s\\t%.6f\" % (current_issue, current_count, float(current_count)/float(total_count))\n",
    "print \"----meta data below-----\"\n",
    "print \"Total Unique Products: %s\" % len(unique_products)\n",
    "print \"Largest Basket Size: %s\" % largest_basket_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/30 23:55:25 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /w261/hw3/hw3_3_1_output\n",
      "16/01/30 23:55:27 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "packageJobJar: [/data/w261/hw3/mapper.py, /data/w261/hw3/reducer.py] [/usr/lib/hadoop/lib/hadoop-streaming-2.6.0.jar] /tmp/streamjob2104852217751772906.jar tmpDir=null\n",
      "16/01/30 23:55:32 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/30 23:55:33 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/30 23:55:34 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/30 23:55:35 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/01/30 23:55:35 INFO Configuration.deprecation: mapred.output.key.comparator.class is deprecated. Instead, use mapreduce.job.output.key.comparator.class\n",
      "16/01/30 23:55:35 INFO Configuration.deprecation: mapred.text.key.comparator.options is deprecated. Instead, use mapreduce.partition.keycomparator.options\n",
      "16/01/30 23:55:35 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1454188161308_0014\n",
      "16/01/30 23:55:36 INFO impl.YarnClientImpl: Submitted application application_1454188161308_0014\n",
      "16/01/30 23:55:36 INFO mapreduce.Job: The url to track the job: http://ip-172-31-55-170.ec2.internal:8088/proxy/application_1454188161308_0014/\n",
      "16/01/30 23:55:36 INFO mapreduce.Job: Running job: job_1454188161308_0014\n",
      "16/01/30 23:55:52 INFO mapreduce.Job: Job job_1454188161308_0014 running in uber mode : false\n",
      "16/01/30 23:55:52 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/01/30 23:56:17 INFO mapreduce.Job:  map 33% reduce 0%\n",
      "16/01/30 23:56:18 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "16/01/30 23:56:22 INFO mapreduce.Job:  map 83% reduce 0%\n",
      "16/01/30 23:56:23 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/30 23:56:39 INFO mapreduce.Job:  map 100% reduce 99%\n",
      "16/01/30 23:56:40 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/30 23:56:40 INFO mapreduce.Job: Job job_1454188161308_0014 completed successfully\n",
      "16/01/30 23:56:40 INFO mapreduce.Job: Counters: 51\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=8759062\n",
      "\t\tFILE: Number of bytes written=17862079\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=3462079\n",
      "\t\tHDFS: Number of bytes written=256067\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=54917\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=14485\n",
      "\t\tTotal time spent by all map tasks (ms)=54917\n",
      "\t\tTotal time spent by all reduce tasks (ms)=14485\n",
      "\t\tTotal vcore-seconds taken by all map tasks=54917\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=14485\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=56235008\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=14832640\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=31101\n",
      "\t\tMap output records=380828\n",
      "\t\tMap output bytes=7997400\n",
      "\t\tMap output materialized bytes=8759068\n",
      "\t\tInput split bytes=212\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=12595\n",
      "\t\tReduce shuffle bytes=8759068\n",
      "\t\tReduce input records=380828\n",
      "\t\tReduce output records=12595\n",
      "\t\tSpilled Records=761656\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=958\n",
      "\t\tCPU time spent (ms)=13750\n",
      "\t\tPhysical memory (bytes) snapshot=565821440\n",
      "\t\tVirtual memory (bytes) snapshot=7590121472\n",
      "\t\tTotal committed heap usage (bytes)=392372224\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tcustom_counters\n",
      "\t\tmapper_count=2\n",
      "\t\treducer_count=1\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=3461867\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=256067\n",
      "16/01/30 23:56:40 INFO streaming.StreamJob: Output directory: /w261/hw3/hw3_3_1_output\n"
     ]
    }
   ],
   "source": [
    "!chmod +x mapper.py; chmod +x reducer.py;\n",
    "!hdfs dfs -rm -r /w261/hw3/hw3_3_1_output;\n",
    "!$HADOOP_HOME/bin/hadoop jar $HADOOP_HOME/lib/hadoop-*streaming*.jar \\\n",
    "-D stream.num.map.output.key.fields=3 \\\n",
    "-D mapreduce.partition.keypartitioner.options=-k1,1 \\\n",
    "-D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D mapred.text.key.comparator.options=\"-k3nr -k2\" \\\n",
    "-mapper mapper.py -file /data/w261/hw3/mapper.py \\\n",
    "-reducer reducer.py -file /data/w261/hw3/reducer.py \\\n",
    "-input /w261/hw3/ProductPurchaseData.txt \\\n",
    "-output /w261/hw3/hw3_3_1_output;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----meta data below-----\t\r\n",
      "Total Unique Products: 12592\t\r\n",
      "Largest Basket Size: 37\t\r\n"
     ]
    }
   ],
   "source": [
    "# total unique products and largest baskets\n",
    "!hdfs dfs -cat /w261/hw3/hw3_3_1_output/part* | tail -3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dai62779\t6667\t0.017507\r\n",
      "fro40251\t3881\t0.010191\r\n",
      "ele17451\t3875\t0.010175\r\n",
      "gro73461\t3602\t0.009458\r\n",
      "sna80324\t3044\t0.007993\r\n",
      "ele32164\t2851\t0.007486\r\n",
      "dai75645\t2736\t0.007184\r\n",
      "sna45677\t2455\t0.006447\r\n",
      "fro31317\t2330\t0.006118\r\n",
      "dai85309\t2293\t0.006021\r\n",
      "ele26917\t2292\t0.006019\r\n",
      "fro80039\t2233\t0.005864\r\n",
      "gro21487\t2115\t0.005554\r\n",
      "sna99873\t2083\t0.005470\r\n",
      "gro59710\t2004\t0.005262\r\n",
      "gro71621\t1920\t0.005042\r\n",
      "fro85978\t1918\t0.005036\r\n",
      "gro30386\t1840\t0.004832\r\n",
      "ele74009\t1816\t0.004769\r\n",
      "gro56726\t1784\t0.004685\r\n",
      "dai63921\t1773\t0.004656\r\n",
      "gro46854\t1756\t0.004611\r\n",
      "ele66600\t1713\t0.004498\r\n",
      "dai83733\t1712\t0.004496\r\n",
      "fro32293\t1702\t0.004469\r\n",
      "ele66810\t1697\t0.004456\r\n",
      "sna55762\t1646\t0.004322\r\n",
      "dai22177\t1627\t0.004272\r\n",
      "fro78087\t1531\t0.004020\r\n",
      "ele99737\t1516\t0.003981\r\n",
      "ele34057\t1489\t0.003910\r\n",
      "gro94758\t1489\t0.003910\r\n",
      "fro35904\t1436\t0.003771\r\n",
      "fro53271\t1420\t0.003729\r\n",
      "sna93860\t1407\t0.003695\r\n",
      "sna90094\t1390\t0.003650\r\n",
      "gro38814\t1352\t0.003550\r\n",
      "ele56788\t1345\t0.003532\r\n",
      "gro61133\t1321\t0.003469\r\n",
      "dai88807\t1316\t0.003456\r\n",
      "ele74482\t1316\t0.003456\r\n",
      "ele59935\t1311\t0.003443\r\n",
      "sna96271\t1295\t0.003401\r\n",
      "dai43223\t1290\t0.003387\r\n",
      "ele91337\t1289\t0.003385\r\n",
      "gro15017\t1275\t0.003348\r\n",
      "dai31081\t1261\t0.003311\r\n",
      "gro81087\t1220\t0.003204\r\n",
      "dai22896\t1219\t0.003201\r\n",
      "gro85051\t1214\t0.003188\r\n",
      "sort: write failed: standard output: Broken pipe\r\n",
      "sort: write error\r\n"
     ]
    }
   ],
   "source": [
    "# top 50 products, their frequencies and relative frequencies\n",
    "!hdfs dfs -cat /w261/hw3/hw3_3_1_output/part* | sort -t$'\\t' -k2,2nr -k1,1 | head -50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <b>HW3.4. (Computationally prohibitive but then again Hadoop can handle this) Pairs</b>\n",
    "<p>Suppose we want to recommend new products to the customer based on the products they\n",
    "have already browsed on the online website. Write a map-reduce program \n",
    "to find products which are frequently browsed together. \n",
    "Fix the support count (cooccurence count) to s = 100 \n",
    "(i.e. product pairs need to occur together at least 100 times to be considered frequent) \n",
    "and find pairs of items (sometimes referred to itemsets of size 2 in association rule mining) \n",
    "that have a support count of 100 or more.</p>\n",
    "<p>List the top 50 product pairs with corresponding support count (aka frequency), \n",
    "and relative frequency or support (number of records where they coccur, \n",
    "the number of records where they coccur/the number of baskets in the dataset)  \n",
    "in decreasing order of support  for frequent (100>count) itemsets of size 2.</p>\n",
    "<p>Use the Pairs pattern (lecture 3)  to  extract these frequent itemsets of size 2. \n",
    "Free free to use combiners if they bring value. \n",
    "Instrument your code with counters for count the number of times your mapper, \n",
    "combiner and reducers are called.</p>\n",
    "<p>Fix the ordering of the pairs lexicographically (left to right), \n",
    "and break ties in support (between pairs, if any exist) \n",
    "by taking the first ones in lexicographically increasing order. \n",
    "<br/>\n",
    "Report  the compute time for the Pairs job. Describe the computational setup used \n",
    "(E.g., single computer; dual core; linux, number of mappers, number of reducers)\n",
    "Instrument your mapper, combiner, and reducer to count how many times each is called \n",
    "using Counters and report these counts.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "# record mapper invokation\n",
    "sys.stderr.write(\"reporter:counter:custom_counters,mapper_count,1\\n\")\n",
    "total_baskets = 0\n",
    "for line in sys.stdin:\n",
    "    line = line.strip().upper()\n",
    "    if line == '': continue\n",
    "    total_baskets += 1\n",
    "    lineArr = sorted(line.split(' '))\n",
    "    for i, item in enumerate(lineArr):\n",
    "        j = i + 1\n",
    "        while j < len(lineArr):\n",
    "            print \"%s\\t%s\\t%s\\t%s\"%(\"paired_strategy\", item, lineArr[j], 1)\n",
    "            j += 1\n",
    "print \"%s\\t%s\\t%s\\t%s\"%(\"paired_strategy\", \"*\", \"*\", total_baskets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "# record reducer invokation\n",
    "sys.stderr.write(\"reporter:counter:custom_counters,reducer_count,1\\n\")\n",
    "\n",
    "total_baskets = 0\n",
    "support_threshold = 100\n",
    "\n",
    "current_pair = None\n",
    "current_support = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip().upper()\n",
    "    if line == '': continue\n",
    "    lineArr = line.split('\\t')\n",
    "    item1 = lineArr[1]\n",
    "    item2 = lineArr[2]\n",
    "    pair = (item1, item2)\n",
    "    pair_count = int(lineArr[3])\n",
    "    # gather total basket count first\n",
    "    if item1 == \"*\" and item2 == \"*\":\n",
    "        total_baskets += pair_count\n",
    "        continue\n",
    "    if current_pair == pair:\n",
    "        current_support += pair_count\n",
    "    else:\n",
    "        if current_pair != None and current_support > support_threshold:\n",
    "            print \"%s\\t%s\\t%s\\t%.6f\" % (current_pair[0], current_pair[1], current_support, float(current_support)/float(total_baskets))\n",
    "        current_pair = pair\n",
    "        current_support = pair_count\n",
    "        \n",
    "if current_pair != None and current_support > support_threshold:\n",
    "    print \"%s\\t%s\\t%s\\t%.6f\" % (current_pair[0], current_pair[1], current_support, float(current_support)/float(total_baskets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/31 02:20:28 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /w261/hw3/hw3_4_output\n",
      "16/01/31 02:20:30 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "packageJobJar: [/data/w261/hw3/mapper.py, /data/w261/hw3/reducer.py] [/usr/lib/hadoop/lib/hadoop-streaming-2.6.0.jar] /tmp/streamjob1768631432912676945.jar tmpDir=null\n",
      "16/01/31 02:20:35 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/31 02:20:36 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/31 02:20:38 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/31 02:20:38 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/01/31 02:20:38 INFO Configuration.deprecation: mapred.output.key.comparator.class is deprecated. Instead, use mapreduce.job.output.key.comparator.class\n",
      "16/01/31 02:20:38 INFO Configuration.deprecation: mapred.text.key.comparator.options is deprecated. Instead, use mapreduce.partition.keycomparator.options\n",
      "16/01/31 02:20:38 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1454188161308_0018\n",
      "16/01/31 02:20:39 INFO impl.YarnClientImpl: Submitted application application_1454188161308_0018\n",
      "16/01/31 02:20:39 INFO mapreduce.Job: The url to track the job: http://ip-172-31-55-170.ec2.internal:8088/proxy/application_1454188161308_0018/\n",
      "16/01/31 02:20:39 INFO mapreduce.Job: Running job: job_1454188161308_0018\n",
      "16/01/31 02:20:56 INFO mapreduce.Job: Job job_1454188161308_0018 running in uber mode : false\n",
      "16/01/31 02:20:56 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/01/31 02:21:20 INFO mapreduce.Job:  map 10% reduce 0%\n",
      "16/01/31 02:21:22 INFO mapreduce.Job:  map 20% reduce 0%\n",
      "16/01/31 02:21:23 INFO mapreduce.Job:  map 28% reduce 0%\n",
      "16/01/31 02:21:25 INFO mapreduce.Job:  map 33% reduce 0%\n",
      "16/01/31 02:21:26 INFO mapreduce.Job:  map 40% reduce 0%\n",
      "16/01/31 02:21:28 INFO mapreduce.Job:  map 48% reduce 0%\n",
      "16/01/31 02:21:29 INFO mapreduce.Job:  map 56% reduce 0%\n",
      "16/01/31 02:21:31 INFO mapreduce.Job:  map 63% reduce 0%\n",
      "16/01/31 02:21:33 INFO mapreduce.Job:  map 64% reduce 0%\n",
      "16/01/31 02:21:34 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "16/01/31 02:22:10 INFO mapreduce.Job:  map 83% reduce 0%\n",
      "16/01/31 02:22:14 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/31 02:22:28 INFO mapreduce.Job:  map 100% reduce 67%\n",
      "16/01/31 02:22:31 INFO mapreduce.Job:  map 100% reduce 71%\n",
      "16/01/31 02:22:34 INFO mapreduce.Job:  map 100% reduce 77%\n",
      "16/01/31 02:22:37 INFO mapreduce.Job:  map 100% reduce 83%\n",
      "16/01/31 02:22:41 INFO mapreduce.Job:  map 100% reduce 90%\n",
      "16/01/31 02:22:44 INFO mapreduce.Job:  map 100% reduce 96%\n",
      "16/01/31 02:22:47 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/31 02:22:48 INFO mapreduce.Job: Job job_1454188161308_0018 completed successfully\n",
      "16/01/31 02:22:48 INFO mapreduce.Job: Counters: 51\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=96294228\n",
      "\t\tFILE: Number of bytes written=192932399\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=3462079\n",
      "\t\tHDFS: Number of bytes written=40648\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=147298\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=32852\n",
      "\t\tTotal time spent by all map tasks (ms)=147298\n",
      "\t\tTotal time spent by all reduce tasks (ms)=32852\n",
      "\t\tTotal vcore-seconds taken by all map tasks=147298\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=32852\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=150833152\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=33640448\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=31101\n",
      "\t\tMap output records=2534059\n",
      "\t\tMap output bytes=91226104\n",
      "\t\tMap output materialized bytes=96294234\n",
      "\t\tInput split bytes=212\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=877099\n",
      "\t\tReduce shuffle bytes=96294234\n",
      "\t\tReduce input records=2534059\n",
      "\t\tReduce output records=1311\n",
      "\t\tSpilled Records=5068118\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=1873\n",
      "\t\tCPU time spent (ms)=68520\n",
      "\t\tPhysical memory (bytes) snapshot=674975744\n",
      "\t\tVirtual memory (bytes) snapshot=7589216256\n",
      "\t\tTotal committed heap usage (bytes)=509313024\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tcustom_counters\n",
      "\t\tmapper_count=2\n",
      "\t\treducer_count=1\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=3461867\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=40648\n",
      "16/01/31 02:22:48 INFO streaming.StreamJob: Output directory: /w261/hw3/hw3_4_output\n"
     ]
    }
   ],
   "source": [
    "!chmod +x mapper.py; chmod +x reducer.py;\n",
    "!hdfs dfs -rm -r /w261/hw3/hw3_4_output;\n",
    "!$HADOOP_HOME/bin/hadoop jar $HADOOP_HOME/lib/hadoop-*streaming*.jar \\\n",
    "-D stream.num.map.output.key.fields=3 \\\n",
    "-D mapreduce.partition.keypartitioner.options=-k1,1 \\\n",
    "-D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D mapred.text.key.comparator.options=\"-k2 -k3\" \\\n",
    "-mapper mapper.py -file /data/w261/hw3/mapper.py \\\n",
    "-reducer reducer.py -file /data/w261/hw3/reducer.py \\\n",
    "-input /w261/hw3/ProductPurchaseData.txt \\\n",
    "-output /w261/hw3/hw3_4_output;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DAI62779\tELE17451\t1592\t0.051188\n",
      "FRO40251\tSNA80324\t1412\t0.045400\n",
      "DAI75645\tFRO40251\t1254\t0.040320\n",
      "FRO40251\tGRO85051\t1213\t0.039002\n",
      "DAI62779\tGRO73461\t1139\t0.036623\n",
      "DAI75645\tSNA80324\t1130\t0.036333\n",
      "DAI62779\tFRO40251\t1070\t0.034404\n",
      "DAI62779\tSNA80324\t923\t0.029678\n",
      "DAI62779\tDAI85309\t918\t0.029517\n",
      "ELE32164\tGRO59710\t911\t0.029292\n",
      "DAI62779\tDAI75645\t882\t0.028359\n",
      "DAI62779\tELE92920\t877\t0.028198\n",
      "FRO40251\tFRO92469\t835\t0.026848\n",
      "DAI62779\tELE32164\t832\t0.026752\n",
      "DAI75645\tGRO73461\t712\t0.022893\n",
      "DAI43223\tELE32164\t711\t0.022861\n",
      "DAI62779\tGRO30386\t709\t0.022797\n",
      "ELE17451\tFRO40251\t697\t0.022411\n",
      "DAI85309\tELE99737\t659\t0.021189\n",
      "DAI62779\tELE26917\t650\t0.020900\n",
      "GRO21487\tGRO73461\t631\t0.020289\n",
      "DAI62779\tSNA45677\t604\t0.019421\n",
      "ELE17451\tSNA80324\t597\t0.019196\n",
      "DAI62779\tGRO71621\t595\t0.019131\n",
      "DAI62779\tSNA55762\t593\t0.019067\n",
      "DAI62779\tDAI83733\t586\t0.018842\n",
      "ELE17451\tGRO73461\t580\t0.018649\n",
      "GRO73461\tSNA80324\t562\t0.018070\n",
      "DAI62779\tGRO59710\t561\t0.018038\n",
      "DAI62779\tFRO80039\t550\t0.017684\n",
      "DAI75645\tELE17451\t547\t0.017588\n",
      "DAI62779\tSNA93860\t537\t0.017266\n",
      "DAI55148\tDAI62779\t526\t0.016913\n",
      "DAI43223\tGRO59710\t512\t0.016462\n",
      "ELE17451\tELE32164\t511\t0.016430\n",
      "DAI62779\tSNA18336\t506\t0.016270\n",
      "ELE32164\tGRO73461\t486\t0.015627\n",
      "DAI62779\tFRO78087\t482\t0.015498\n",
      "DAI62779\tGRO94758\t479\t0.015401\n",
      "DAI62779\tGRO21487\t471\t0.015144\n",
      "ELE17451\tGRO30386\t468\t0.015048\n",
      "FRO85978\tSNA95666\t463\t0.014887\n",
      "DAI62779\tFRO19221\t462\t0.014855\n",
      "DAI62779\tGRO46854\t461\t0.014823\n",
      "DAI43223\tDAI62779\t459\t0.014758\n",
      "ELE92920\tSNA18336\t455\t0.014630\n",
      "DAI88079\tFRO40251\t446\t0.014340\n",
      "DAI62779\tSNA96271\t442\t0.014212\n",
      "FRO73056\tGRO44993\t438\t0.014083\n",
      "DAI62779\tFRO85978\t434\t0.013955\n"
     ]
    }
   ],
   "source": [
    "# top 50 most co-occuring pairs.  break tie by taking the first in lexicographically increasing order\n",
    "import subprocess\n",
    "cat = subprocess.Popen([\"hadoop\", \"fs\", \"-cat\", \"/w261/hw3/hw3_4_output/part*\"], stdout=subprocess.PIPE)\n",
    "all_pairs = []\n",
    "for line in cat.stdout:\n",
    "    all_pairs.append(line.strip().split('\\t'))\n",
    "# first sort by support, then sort by item pairs lexicographically\n",
    "all_pairs = sorted(all_pairs, key=lambda x: (-1*int(x[2]), (x[0], x[1])))\n",
    "# print the top 50 results without ties\n",
    "result_printed = 0\n",
    "current_support = None\n",
    "for data in all_pairs:\n",
    "    if result_printed >= 50: break\n",
    "    item1 = data[0]\n",
    "    item2 = data[1]\n",
    "    support_count = data[2]\n",
    "    support_frq = data[3]\n",
    "    if support_count == current_support:\n",
    "        continue\n",
    "    print \"%s\\t%s\\t%s\\t%s\" % (item1, item2, support_count, support_frq)\n",
    "    current_support = support_count\n",
    "    result_printed += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Discussion on H3.4</b>\n",
    "<p>\n",
    "The process took about 2 minutes and 18 seconds to run on a single AWS Linux Medium instance (and therefore network transfer is between mapper nodes is probably not an issue in this experiment).  The setup contains one mapper phase and one reducer phase.  Hadoop called mapper twice and reducer once.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <b>HW3.5: Stripes</b>\n",
    "<p>Repeat 3.4 using the stripes design pattern for finding cooccuring pairs.\n",
    "\n",
    "Report  the compute times for stripes job versus the Pairs job. \n",
    "Describe the computational setup used \n",
    "(E.g., single computer; dual core; linux, number of mappers, number of reducers)\n",
    "\n",
    "Instrument your mapper, combiner, and reducer to count how many times each is called \n",
    "using Counters and report these counts. Discuss the differences in these counts between \n",
    "the Pairs and Stripes jobs</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "# record mapper invokation\n",
    "sys.stderr.write(\"reporter:counter:custom_counters,mapper_count,1\\n\")\n",
    "co_occurence = {}\n",
    "total_baskets = 0\n",
    "for line in sys.stdin:\n",
    "    line = line.strip().upper()\n",
    "    if line == '': continue\n",
    "    total_baskets += 1\n",
    "    lineArr = sorted(line.split(' '))\n",
    "    for i, item1 in enumerate(lineArr):\n",
    "        j = i + 1\n",
    "        while j < len(lineArr):\n",
    "            item2 = lineArr[j]\n",
    "            if item1 not in co_occurence:\n",
    "                co_occurence[item1] = {item2: 1}\n",
    "            else:\n",
    "                item1_co_occurence = co_occurence[item1]\n",
    "                if item2 in item1_co_occurence:\n",
    "                    item1_co_occurence[item2] += 1\n",
    "                else:\n",
    "                    item1_co_occurence[item2] = 1\n",
    "            j += 1\n",
    "print \"%s\\t%s\\t%s\" % (\"stripe_strategy\", \"*\", total_baskets)\n",
    "for item1 in co_occurence:\n",
    "    item1_co_occurence = co_occurence[item1]\n",
    "    associative_output = '|'.join([str(k[0]) + ',' + str(k[1]) for k in item1_co_occurence.items()])\n",
    "    print \"%s\\t%s\\t%s\" % (\"stripe_strategy\", item1, associative_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "# record reducer invokation\n",
    "sys.stderr.write(\"reporter:counter:custom_counters,reducer_count,1\\n\")\n",
    "\n",
    "support_threshold = 100\n",
    "total_baskets = 0\n",
    "current_item = None\n",
    "current_associated_counts = {}\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    if line == '': continue\n",
    "    lineArr = line.split('\\t')\n",
    "    item1 = lineArr[1]\n",
    "    associated_info = lineArr[2]\n",
    "    # gather total basket counts\n",
    "    if item1 == \"*\":\n",
    "        total_baskets += int(associated_info)\n",
    "        continue\n",
    "    if current_item == item1:\n",
    "        # parse the associated array info\n",
    "        for item2_info in associated_info.split('|'):\n",
    "            item2 = item2_info.split(',')[0]\n",
    "            item2_count = int(item2_info.split(',')[1])\n",
    "            if item2 in current_associated_counts:\n",
    "                current_associated_counts[item2] += item2_count\n",
    "            else:\n",
    "                current_associated_counts[item2] = item2_count\n",
    "    else:\n",
    "        # print out the current data\n",
    "        if current_item != None:\n",
    "            for item2 in current_associated_counts:\n",
    "                if current_associated_counts[item2] > support_threshold:\n",
    "                    print \"%s\\t%s\\t%s\\t%.6f\" % (current_item, item2, current_associated_counts[item2], float(current_associated_counts[item2])/float(total_baskets))\n",
    "        # parse new associative array info\n",
    "        current_item = item1\n",
    "        current_associated_counts = {}\n",
    "        for item2_info in associated_info.split('|'):\n",
    "            item2 = item2_info.split(',')[0]\n",
    "            item2_count = int(item2_info.split(',')[1])\n",
    "            if item2 in current_associated_counts:\n",
    "                current_associated_counts[item2] += item2_count\n",
    "            else:\n",
    "                current_associated_counts[item2] = item2_count\n",
    "# print the last record\n",
    "if current_item != None:\n",
    "    for item2 in current_associated_counts:\n",
    "        if current_associated_counts[item2] > support_threshold:\n",
    "            print \"%s\\t%s\\t%s\\t%.6f\" % (current_item, item2, current_associated_counts[item2], float(current_associated_counts[item2])/float(total_baskets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/w261/hw3/hw3_5_output': No such file or directory\n",
      "16/01/31 02:29:45 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "packageJobJar: [/data/w261/hw3/mapper.py, /data/w261/hw3/reducer.py] [/usr/lib/hadoop/lib/hadoop-streaming-2.6.0.jar] /tmp/streamjob9173898191930945882.jar tmpDir=null\n",
      "16/01/31 02:29:50 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/31 02:29:51 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/31 02:29:53 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/31 02:29:53 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/01/31 02:29:53 INFO Configuration.deprecation: mapred.output.key.comparator.class is deprecated. Instead, use mapreduce.job.output.key.comparator.class\n",
      "16/01/31 02:29:53 INFO Configuration.deprecation: mapred.text.key.comparator.options is deprecated. Instead, use mapreduce.partition.keycomparator.options\n",
      "16/01/31 02:29:54 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1454188161308_0019\n",
      "16/01/31 02:29:55 INFO impl.YarnClientImpl: Submitted application application_1454188161308_0019\n",
      "16/01/31 02:29:55 INFO mapreduce.Job: The url to track the job: http://ip-172-31-55-170.ec2.internal:8088/proxy/application_1454188161308_0019/\n",
      "16/01/31 02:29:55 INFO mapreduce.Job: Running job: job_1454188161308_0019\n",
      "16/01/31 02:30:12 INFO mapreduce.Job: Job job_1454188161308_0019 running in uber mode : false\n",
      "16/01/31 02:30:12 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/01/31 02:30:35 INFO mapreduce.Job:  map 18% reduce 0%\n",
      "16/01/31 02:30:37 INFO mapreduce.Job:  map 38% reduce 0%\n",
      "16/01/31 02:30:39 INFO mapreduce.Job:  map 54% reduce 0%\n",
      "16/01/31 02:30:41 INFO mapreduce.Job:  map 66% reduce 0%\n",
      "16/01/31 02:30:42 INFO mapreduce.Job:  map 83% reduce 0%\n",
      "16/01/31 02:30:44 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/31 02:31:00 INFO mapreduce.Job:  map 100% reduce 95%\n",
      "16/01/31 02:31:02 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/31 02:31:02 INFO mapreduce.Job: Job job_1454188161308_0019 completed successfully\n",
      "16/01/31 02:31:02 INFO mapreduce.Job: Counters: 51\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=11819464\n",
      "\t\tFILE: Number of bytes written=23982862\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=3462079\n",
      "\t\tHDFS: Number of bytes written=40648\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=58936\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=15544\n",
      "\t\tTotal time spent by all map tasks (ms)=58936\n",
      "\t\tTotal time spent by all reduce tasks (ms)=15544\n",
      "\t\tTotal vcore-seconds taken by all map tasks=58936\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=15544\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=60350464\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=15917056\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=31101\n",
      "\t\tMap output records=16942\n",
      "\t\tMap output bytes=11769764\n",
      "\t\tMap output materialized bytes=11819470\n",
      "\t\tInput split bytes=212\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=12012\n",
      "\t\tReduce shuffle bytes=11819470\n",
      "\t\tReduce input records=16942\n",
      "\t\tReduce output records=1311\n",
      "\t\tSpilled Records=33884\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=682\n",
      "\t\tCPU time spent (ms)=15310\n",
      "\t\tPhysical memory (bytes) snapshot=556462080\n",
      "\t\tVirtual memory (bytes) snapshot=7589007360\n",
      "\t\tTotal committed heap usage (bytes)=392372224\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tcustom_counters\n",
      "\t\tmapper_count=2\n",
      "\t\treducer_count=1\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=3461867\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=40648\n",
      "16/01/31 02:31:02 INFO streaming.StreamJob: Output directory: /w261/hw3/hw3_5_output\n"
     ]
    }
   ],
   "source": [
    "!chmod +x mapper.py; chmod +x reducer.py;\n",
    "!hdfs dfs -rm -r /w261/hw3/hw3_5_output;\n",
    "!$HADOOP_HOME/bin/hadoop jar $HADOOP_HOME/lib/hadoop-*streaming*.jar \\\n",
    "-D stream.num.map.output.key.fields=2 \\\n",
    "-D mapreduce.partition.keypartitioner.options=-k1,1 \\\n",
    "-D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D mapred.text.key.comparator.options=\"-k2\" \\\n",
    "-mapper mapper.py -file /data/w261/hw3/mapper.py \\\n",
    "-reducer reducer.py -file /data/w261/hw3/reducer.py \\\n",
    "-input /w261/hw3/ProductPurchaseData.txt \\\n",
    "-output /w261/hw3/hw3_5_output;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DAI62779\tELE17451\t1592\t0.051188\n",
      "FRO40251\tSNA80324\t1412\t0.045400\n",
      "DAI75645\tFRO40251\t1254\t0.040320\n",
      "FRO40251\tGRO85051\t1213\t0.039002\n",
      "DAI62779\tGRO73461\t1139\t0.036623\n",
      "DAI75645\tSNA80324\t1130\t0.036333\n",
      "DAI62779\tFRO40251\t1070\t0.034404\n",
      "DAI62779\tSNA80324\t923\t0.029678\n",
      "DAI62779\tDAI85309\t918\t0.029517\n",
      "ELE32164\tGRO59710\t911\t0.029292\n",
      "DAI62779\tDAI75645\t882\t0.028359\n",
      "DAI62779\tELE92920\t877\t0.028198\n",
      "FRO40251\tFRO92469\t835\t0.026848\n",
      "DAI62779\tELE32164\t832\t0.026752\n",
      "DAI75645\tGRO73461\t712\t0.022893\n",
      "DAI43223\tELE32164\t711\t0.022861\n",
      "DAI62779\tGRO30386\t709\t0.022797\n",
      "ELE17451\tFRO40251\t697\t0.022411\n",
      "DAI85309\tELE99737\t659\t0.021189\n",
      "DAI62779\tELE26917\t650\t0.020900\n",
      "GRO21487\tGRO73461\t631\t0.020289\n",
      "DAI62779\tSNA45677\t604\t0.019421\n",
      "ELE17451\tSNA80324\t597\t0.019196\n",
      "DAI62779\tGRO71621\t595\t0.019131\n",
      "DAI62779\tSNA55762\t593\t0.019067\n",
      "DAI62779\tDAI83733\t586\t0.018842\n",
      "ELE17451\tGRO73461\t580\t0.018649\n",
      "GRO73461\tSNA80324\t562\t0.018070\n",
      "DAI62779\tGRO59710\t561\t0.018038\n",
      "DAI62779\tFRO80039\t550\t0.017684\n",
      "DAI75645\tELE17451\t547\t0.017588\n",
      "DAI62779\tSNA93860\t537\t0.017266\n",
      "DAI55148\tDAI62779\t526\t0.016913\n",
      "DAI43223\tGRO59710\t512\t0.016462\n",
      "ELE17451\tELE32164\t511\t0.016430\n",
      "DAI62779\tSNA18336\t506\t0.016270\n",
      "ELE32164\tGRO73461\t486\t0.015627\n",
      "DAI62779\tFRO78087\t482\t0.015498\n",
      "DAI62779\tGRO94758\t479\t0.015401\n",
      "DAI62779\tGRO21487\t471\t0.015144\n",
      "ELE17451\tGRO30386\t468\t0.015048\n",
      "FRO85978\tSNA95666\t463\t0.014887\n",
      "DAI62779\tFRO19221\t462\t0.014855\n",
      "DAI62779\tGRO46854\t461\t0.014823\n",
      "DAI43223\tDAI62779\t459\t0.014758\n",
      "ELE92920\tSNA18336\t455\t0.014630\n",
      "DAI88079\tFRO40251\t446\t0.014340\n",
      "DAI62779\tSNA96271\t442\t0.014212\n",
      "FRO73056\tGRO44993\t438\t0.014083\n",
      "DAI62779\tFRO85978\t434\t0.013955\n"
     ]
    }
   ],
   "source": [
    "# top 50 most co-occuring pairs.  break tie by taking the first in lexicographically increasing order\n",
    "import subprocess\n",
    "cat = subprocess.Popen([\"hadoop\", \"fs\", \"-cat\", \"/w261/hw3/hw3_5_output/part*\"], stdout=subprocess.PIPE)\n",
    "all_pairs = []\n",
    "for line in cat.stdout:\n",
    "    all_pairs.append(line.strip().split('\\t'))\n",
    "# first sort by support, then sort by item pairs lexicographically\n",
    "all_pairs = sorted(all_pairs, key=lambda x: (-1*int(x[2]), (x[0], x[1])))\n",
    "# print the top 50 results without ties\n",
    "result_printed = 0\n",
    "current_support = None\n",
    "for data in all_pairs:\n",
    "    if result_printed >= 50: break\n",
    "    item1 = data[0]\n",
    "    item2 = data[1]\n",
    "    support_count = data[2]\n",
    "    support_frq = data[3]\n",
    "    if support_count == current_support:\n",
    "        continue\n",
    "    print \"%s\\t%s\\t%s\\t%s\" % (item1, item2, support_count, support_frq)\n",
    "    current_support = support_count\n",
    "    result_printed += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Discussion on HW3.5</b>\n",
    "<p>\n",
    "On the same hardware setup as in HW3.4, it took only 12 seconds to complete the process, which is significantly faster than the pair approach used in HW3.4.  The improvement should be coming from the reduction in the data needed to be transfered and sorted from mapper to reducer.  The setup contains one mapper phase and one reducer phase.  Hadoop called mapper twice and reducer once.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
