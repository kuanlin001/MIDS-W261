{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 5.4  (over 2Gig of Data)\n",
    "In this part of the assignment we will focus on developing methods\n",
    "for detecting synonyms, using the Google 5-grams dataset. To accomplish\n",
    "this you must script two main tasks using MRJob:\n",
    "\n",
    "(1) Build stripes of word co-ocurrence for the top 10,000 using the words ranked from 9001,-10,000 as a basis\n",
    "most frequently appearing words across the entire set of 5-grams,\n",
    "and output to a file in your bucket on s3 (bigram analysis, though the words are non-contiguous).\n",
    "\n",
    "(2) Using two (symmetric) comparison methods of your choice \n",
    "(e.g., correlations, distances, similarities), pairwise compare \n",
    "all stripes (vectors), and output to a file in your bucket on s3.\n",
    "\n",
    "==Design notes for (1)==\n",
    "For this task you will be able to modify the pattern we used in HW 3.2\n",
    "(feel free to use the solution as reference). To total the word counts \n",
    "across the 5-grams, output the support from the mappers using the total \n",
    "order inversion pattern:\n",
    "\n",
    "<*word,count>\n",
    "\n",
    "to ensure that the support arrives before the cooccurrences.\n",
    "\n",
    "In addition to ensuring the determination of the total word counts,\n",
    "the mapper must also output co-occurrence counts for the pairs of\n",
    "words inside of each 5-gram. Treat these words as a basket,\n",
    "as we have in HW 3, but count all stripes or pairs in both orders,\n",
    "i.e., count both orderings: (word1,word2), and (word2,word1), to preserve\n",
    "symmetry in our output for (2).\n",
    "\n",
    "==Design notes for (2)==\n",
    "For this task you will have to determine a method of comparison.\n",
    "Here are a few that you might consider:\n",
    "\n",
    "- Jaccard\n",
    "- Cosine similarity\n",
    "- Spearman correlation\n",
    "- Euclidean distance\n",
    "- Taxicab (Manhattan) distance\n",
    "- Shortest path graph distance (a graph, because our data is symmetric!)\n",
    "- Pearson correlation\n",
    "- Kendall correlation\n",
    "...\n",
    "\n",
    "However, be cautioned that some comparison methods are more difficult to\n",
    "parallelize than others, and do not perform more associations than is necessary, \n",
    "since your choice of association will be symmetric.\n",
    "\n",
    "Please use the inverted index (discussed in live session #5) based pattern to compute the pairwise (term-by-term) similarity matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing top_vocabs.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile top_vocabs.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.protocol import RawValueProtocol\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "# this script is to find the top 1000 vocabs words\n",
    "\n",
    "class top_vocabs(MRJob):\n",
    "    # use raw value protocol to get inversion done easier\n",
    "    #INPUT_PROTOCOL = RawValueProtocol\n",
    "    #INTERNAL_PROTOCOL = RawValueProtocol\n",
    "    #OUTPUT_PROTOCOL = RawValueProtocol\n",
    "    \n",
    "    # step1 do word counts\n",
    "    # step2 get the top 1000 vocabs by counts\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.wordCountsMapper, reducer=self.wordCountsReducer),\n",
    "            MRStep(reducer_init=self.rankWords_init, reducer=self.getTopVocabs,\n",
    "                  jobconf={\n",
    "                    'stream.num.map.output.key.fields': 2,\n",
    "                    'mapreduce.partition.keypartitioner.options': '-k1,1',\n",
    "                    'mapred.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "                    'mapred.text.key.comparator.options': \"-k2,2nr\",\n",
    "                    'mapred.reduce.tasks': 1\n",
    "                })\n",
    "        ]      \n",
    "\n",
    "    def wordCountsMapper(self, _, line):\n",
    "        lineArr = line.strip().split('\\t')\n",
    "        ngram_count = int(lineArr[1])\n",
    "        for word in lineArr[0].split(' '):\n",
    "            # each word appear n times as the ngram count\n",
    "            yield word.lower() , ngram_count\n",
    "            \n",
    "    def wordCountsReducer(self, word, counts):\n",
    "        # summing up counts for each word\n",
    "        yield word, sum(counts)\n",
    "        \n",
    "    def rankWords_init(self):\n",
    "        self.vocab = set()\n",
    "        # grab the top 1000 words as vocab\n",
    "        self.vocab_size = 1000\n",
    "        \n",
    "    def getTopVocabs(self, word, count):\n",
    "        if len(self.vocab) < self.vocab_size:\n",
    "            self.vocab.add(word)\n",
    "            yield word, max(count)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    top_vocabs.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/14 02:21:57 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /w261/hw5/dev_output\n",
      "no configs found; falling back on auto-configuration\n",
      "no configs found; falling back on auto-configuration\n",
      "creating tmp directory /tmp/top_vocabs.root.20160214.022158.308846\n",
      "writing wrapper script to /tmp/top_vocabs.root.20160214.022158.308846/setup-wrapper.sh\n",
      "Using Hadoop version 2.6.0\n",
      "Copying local files into hdfs:///user/root/tmp/mrjob/top_vocabs.root.20160214.022158.308846/files/\n",
      "\n",
      "PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "\n",
      "HADOOP: packageJobJar: [] [/usr/lib/hadoop/lib/hadoop-streaming-2.6.0.jar] /tmp/streamjob4086499293356888855.jar tmpDir=null\n",
      "HADOOP: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "HADOOP: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "HADOOP: Total input paths to process : 1\n",
      "HADOOP: number of splits:2\n",
      "HADOOP: Submitting tokens for job: job_1455411112689_0007\n",
      "HADOOP: Submitted application application_1455411112689_0007\n",
      "HADOOP: The url to track the job: http://ip-172-31-55-170.ec2.internal:8088/proxy/application_1455411112689_0007/\n",
      "HADOOP: Running job: job_1455411112689_0007\n",
      "HADOOP: Job job_1455411112689_0007 running in uber mode : false\n",
      "HADOOP:  map 0% reduce 0%\n",
      "HADOOP:  map 2% reduce 0%\n",
      "HADOOP:  map 4% reduce 0%\n",
      "HADOOP:  map 6% reduce 0%\n",
      "HADOOP:  map 8% reduce 0%\n",
      "HADOOP:  map 10% reduce 0%\n",
      "HADOOP:  map 12% reduce 0%\n",
      "HADOOP:  map 15% reduce 0%\n",
      "HADOOP:  map 17% reduce 0%\n",
      "HADOOP:  map 18% reduce 0%\n",
      "HADOOP:  map 21% reduce 0%\n",
      "HADOOP:  map 23% reduce 0%\n",
      "HADOOP:  map 26% reduce 0%\n",
      "HADOOP:  map 28% reduce 0%\n",
      "HADOOP:  map 31% reduce 0%\n",
      "HADOOP:  map 33% reduce 0%\n",
      "HADOOP:  map 34% reduce 0%\n",
      "HADOOP:  map 37% reduce 0%\n",
      "HADOOP:  map 39% reduce 0%\n",
      "HADOOP:  map 41% reduce 0%\n",
      "HADOOP:  map 44% reduce 0%\n",
      "HADOOP:  map 46% reduce 0%\n",
      "HADOOP:  map 48% reduce 0%\n",
      "HADOOP:  map 50% reduce 0%\n",
      "HADOOP:  map 53% reduce 0%\n",
      "HADOOP:  map 55% reduce 0%\n",
      "HADOOP:  map 57% reduce 0%\n",
      "HADOOP:  map 60% reduce 0%\n",
      "HADOOP:  map 62% reduce 0%\n",
      "HADOOP:  map 64% reduce 0%\n",
      "HADOOP:  map 66% reduce 0%\n",
      "HADOOP:  map 67% reduce 0%\n",
      "HADOOP:  map 83% reduce 0%\n",
      "HADOOP:  map 100% reduce 0%\n",
      "HADOOP:  map 100% reduce 68%\n",
      "HADOOP:  map 100% reduce 71%\n",
      "HADOOP:  map 100% reduce 75%\n",
      "HADOOP:  map 100% reduce 79%\n",
      "HADOOP:  map 100% reduce 82%\n",
      "HADOOP:  map 100% reduce 85%\n",
      "HADOOP:  map 100% reduce 90%\n",
      "HADOOP:  map 100% reduce 92%\n",
      "HADOOP:  map 100% reduce 96%\n",
      "HADOOP:  map 100% reduce 100%\n",
      "HADOOP: Job job_1455411112689_0007 completed successfully\n",
      "HADOOP: Counters: 49\n",
      "HADOOP: \tFile System Counters\n",
      "HADOOP: \t\tFILE: Number of bytes read=19768136\n",
      "HADOOP: \t\tFILE: Number of bytes written=39883353\n",
      "HADOOP: \t\tFILE: Number of read operations=0\n",
      "HADOOP: \t\tFILE: Number of large read operations=0\n",
      "HADOOP: \t\tFILE: Number of write operations=0\n",
      "HADOOP: \t\tHDFS: Number of bytes read=11448787\n",
      "HADOOP: \t\tHDFS: Number of bytes written=618329\n",
      "HADOOP: \t\tHDFS: Number of read operations=9\n",
      "HADOOP: \t\tHDFS: Number of large read operations=0\n",
      "HADOOP: \t\tHDFS: Number of write operations=2\n",
      "HADOOP: \tJob Counters \n",
      "HADOOP: \t\tLaunched map tasks=2\n",
      "HADOOP: \t\tLaunched reduce tasks=1\n",
      "HADOOP: \t\tData-local map tasks=2\n",
      "HADOOP: \t\tTotal time spent by all maps in occupied slots (ms)=150199\n",
      "HADOOP: \t\tTotal time spent by all reduces in occupied slots (ms)=42492\n",
      "HADOOP: \t\tTotal time spent by all map tasks (ms)=150199\n",
      "HADOOP: \t\tTotal time spent by all reduce tasks (ms)=42492\n",
      "HADOOP: \t\tTotal vcore-seconds taken by all map tasks=150199\n",
      "HADOOP: \t\tTotal vcore-seconds taken by all reduce tasks=42492\n",
      "HADOOP: \t\tTotal megabyte-seconds taken by all map tasks=153803776\n",
      "HADOOP: \t\tTotal megabyte-seconds taken by all reduce tasks=43511808\n",
      "HADOOP: \tMap-Reduce Framework\n",
      "HADOOP: \t\tMap input records=311614\n",
      "HADOOP: \t\tMap output records=1558070\n",
      "HADOOP: \t\tMap output bytes=16651990\n",
      "HADOOP: \t\tMap output materialized bytes=19768142\n",
      "HADOOP: \t\tInput split bytes=272\n",
      "HADOOP: \t\tCombine input records=0\n",
      "HADOOP: \t\tCombine output records=0\n",
      "HADOOP: \t\tReduce input groups=42161\n",
      "HADOOP: \t\tReduce shuffle bytes=19768142\n",
      "HADOOP: \t\tReduce input records=1558070\n",
      "HADOOP: \t\tReduce output records=42161\n",
      "HADOOP: \t\tSpilled Records=3116140\n",
      "HADOOP: \t\tShuffled Maps =2\n",
      "HADOOP: \t\tFailed Shuffles=0\n",
      "HADOOP: \t\tMerged Map outputs=2\n",
      "HADOOP: \t\tGC time elapsed (ms)=1318\n",
      "HADOOP: \t\tCPU time spent (ms)=72960\n",
      "HADOOP: \t\tPhysical memory (bytes) snapshot=566239232\n",
      "HADOOP: \t\tVirtual memory (bytes) snapshot=7590068224\n",
      "HADOOP: \t\tTotal committed heap usage (bytes)=392372224\n",
      "HADOOP: \tShuffle Errors\n",
      "HADOOP: \t\tBAD_ID=0\n",
      "HADOOP: \t\tCONNECTION=0\n",
      "HADOOP: \t\tIO_ERROR=0\n",
      "HADOOP: \t\tWRONG_LENGTH=0\n",
      "HADOOP: \t\tWRONG_MAP=0\n",
      "HADOOP: \t\tWRONG_REDUCE=0\n",
      "HADOOP: \tFile Input Format Counters \n",
      "HADOOP: \t\tBytes Read=11448515\n",
      "HADOOP: \tFile Output Format Counters \n",
      "HADOOP: \t\tBytes Written=618329\n",
      "HADOOP: Output directory: hdfs:///user/root/tmp/mrjob/top_vocabs.root.20160214.022158.308846/step-output/1\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.6.0:\n",
      "The have been translated as follows\n",
      " mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.reduce.tasks: mapreduce.job.reduces\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "HADOOP: mapred.output.key.comparator.class is deprecated. Instead, use mapreduce.job.output.key.comparator.class\n",
      "HADOOP: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "HADOOP: mapred.text.key.comparator.options is deprecated. Instead, use mapreduce.partition.keycomparator.options\n",
      "HADOOP: packageJobJar: [] [/usr/lib/hadoop/lib/hadoop-streaming-2.6.0.jar] /tmp/streamjob5861801393041155189.jar tmpDir=null\n",
      "HADOOP: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "HADOOP: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "HADOOP: Total input paths to process : 1\n",
      "HADOOP: number of splits:2\n",
      "HADOOP: Submitting tokens for job: job_1455411112689_0008\n",
      "HADOOP: Submitted application application_1455411112689_0008\n",
      "HADOOP: The url to track the job: http://ip-172-31-55-170.ec2.internal:8088/proxy/application_1455411112689_0008/\n",
      "HADOOP: Running job: job_1455411112689_0008\n",
      "HADOOP: Job job_1455411112689_0008 running in uber mode : false\n",
      "HADOOP:  map 0% reduce 0%\n",
      "HADOOP:  map 50% reduce 0%\n",
      "HADOOP:  map 100% reduce 0%\n",
      "HADOOP:  map 100% reduce 100%\n",
      "HADOOP: Job job_1455411112689_0008 completed successfully\n",
      "HADOOP: Counters: 49\n",
      "HADOOP: \tFile System Counters\n",
      "HADOOP: \t\tFILE: Number of bytes read=744818\n",
      "HADOOP: \t\tFILE: Number of bytes written=1838361\n",
      "HADOOP: \t\tFILE: Number of read operations=0\n",
      "HADOOP: \t\tFILE: Number of large read operations=0\n",
      "HADOOP: \t\tFILE: Number of write operations=0\n",
      "HADOOP: \t\tHDFS: Number of bytes read=620775\n",
      "HADOOP: \t\tHDFS: Number of bytes written=14964\n",
      "HADOOP: \t\tHDFS: Number of read operations=9\n",
      "HADOOP: \t\tHDFS: Number of large read operations=0\n",
      "HADOOP: \t\tHDFS: Number of write operations=2\n",
      "HADOOP: \tJob Counters \n",
      "HADOOP: \t\tLaunched map tasks=2\n",
      "HADOOP: \t\tLaunched reduce tasks=1\n",
      "HADOOP: \t\tData-local map tasks=2\n",
      "HADOOP: \t\tTotal time spent by all maps in occupied slots (ms)=43107\n",
      "HADOOP: \t\tTotal time spent by all reduces in occupied slots (ms)=13882\n",
      "HADOOP: \t\tTotal time spent by all map tasks (ms)=43107\n",
      "HADOOP: \t\tTotal time spent by all reduce tasks (ms)=13882\n",
      "HADOOP: \t\tTotal vcore-seconds taken by all map tasks=43107\n",
      "HADOOP: \t\tTotal vcore-seconds taken by all reduce tasks=13882\n",
      "HADOOP: \t\tTotal megabyte-seconds taken by all map tasks=44141568\n",
      "HADOOP: \t\tTotal megabyte-seconds taken by all reduce tasks=14215168\n",
      "HADOOP: \tMap-Reduce Framework\n",
      "HADOOP: \t\tMap input records=42161\n",
      "HADOOP: \t\tMap output records=42161\n",
      "HADOOP: \t\tMap output bytes=660490\n",
      "HADOOP: \t\tMap output materialized bytes=744824\n",
      "HADOOP: \t\tInput split bytes=314\n",
      "HADOOP: \t\tCombine input records=0\n",
      "HADOOP: \t\tCombine output records=0\n",
      "HADOOP: \t\tReduce input groups=42161\n",
      "HADOOP: \t\tReduce shuffle bytes=744824\n",
      "HADOOP: \t\tReduce input records=42161\n",
      "HADOOP: \t\tReduce output records=1000\n",
      "HADOOP: \t\tSpilled Records=84322\n",
      "HADOOP: \t\tShuffled Maps =2\n",
      "HADOOP: \t\tFailed Shuffles=0\n",
      "HADOOP: \t\tMerged Map outputs=2\n",
      "HADOOP: \t\tGC time elapsed (ms)=863\n",
      "HADOOP: \t\tCPU time spent (ms)=6910\n",
      "HADOOP: \t\tPhysical memory (bytes) snapshot=558985216\n",
      "HADOOP: \t\tVirtual memory (bytes) snapshot=7588683776\n",
      "HADOOP: \t\tTotal committed heap usage (bytes)=392372224\n",
      "HADOOP: \tShuffle Errors\n",
      "HADOOP: \t\tBAD_ID=0\n",
      "HADOOP: \t\tCONNECTION=0\n",
      "HADOOP: \t\tIO_ERROR=0\n",
      "HADOOP: \t\tWRONG_LENGTH=0\n",
      "HADOOP: \t\tWRONG_MAP=0\n",
      "HADOOP: \t\tWRONG_REDUCE=0\n",
      "HADOOP: \tFile Input Format Counters \n",
      "HADOOP: \t\tBytes Read=620461\n",
      "HADOOP: \tFile Output Format Counters \n",
      "HADOOP: \t\tBytes Written=14964\n",
      "HADOOP: Output directory: hdfs:///w261/hw5/dev_output/\n",
      "Counters from step 2:\n",
      "  (no counters found)\n",
      "removing tmp directory /tmp/top_vocabs.root.20160214.022158.308846\n",
      "deleting hdfs:///user/root/tmp/mrjob/top_vocabs.root.20160214.022158.308846 from HDFS\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /w261/hw5/dev_output\n",
    "!python top_vocabs.py -r hadoop hdfs:///w261/hw5/dev/googlebooks-eng-all-5gram-20090715-0-filtered.txt --output-dir hdfs:///w261/hw5/dev_output/ --no-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting synonym_matrix.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile synonym_matrix.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.protocol import RawValueProtocol\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "# this script creates co-occurence matrix and output into an inverted index\n",
    "\n",
    "class synonym_matrix(MRJob):\n",
    "    \n",
    "    SORT_VALUES = True\n",
    "    \n",
    "    def steps(self):\n",
    "        # first step is to count the frequencies of co-occurence terms\n",
    "        # second step is to produce inverted indexes\n",
    "        return [\n",
    "            MRStep(mapper_init=self.bigram_mapper_init, mapper=self.bigram_mapper, reducer=self.bigram_reducer),\n",
    "            MRStep(reducer=self.createInvertedIndex)\n",
    "        ]\n",
    "    \n",
    "    # load up vocabulary\n",
    "    def bigram_mapper_init(self):\n",
    "        self.vocab = set()\n",
    "        for line in open('vocabs.txt', 'r'):\n",
    "            lineArr = line.strip().split('\\t')\n",
    "            self.vocab.add(lineArr[0].replace('\"', '').lower())\n",
    "    \n",
    "    # count up co-occuring bigrams\n",
    "    def bigram_mapper(self, _, line):\n",
    "        emitted_words = set()\n",
    "        lineArr = line.strip().split('\\t')\n",
    "        words = [w.lower() for w in lineArr[0].split(' ')]\n",
    "        # total counts of this particular ngram\n",
    "        ngram_counts = int(lineArr[1])\n",
    "        if len(words) > 1:\n",
    "            for word in words:\n",
    "                if word not in self.vocab or word in emitted_words:\n",
    "                    continue\n",
    "                # emit total ngram count to calculate frequency in the reducer\n",
    "                yield word, ('*', ngram_counts)\n",
    "                emitted_words.add(word)\n",
    "                co_occured_words = set()\n",
    "                for another_word in words:\n",
    "                    if word != another_word and another_word in self.vocab and another_word not in co_occured_words:\n",
    "                        co_occured_words.add(another_word)\n",
    "                        yield word, (another_word, ngram_counts)\n",
    "    \n",
    "    # calculating co-occuring frequencies\n",
    "    def bigram_reducer(self, word, cocurs):       \n",
    "        ngram_total = 0\n",
    "        current_cocurrence = 0\n",
    "        current_coword = None\n",
    "        for coword, count in cocurs:\n",
    "            if coword == '*':\n",
    "                ngram_total += count\n",
    "                continue\n",
    "            if coword == current_coword:\n",
    "                current_cocurrence += count\n",
    "            else:\n",
    "                if current_coword != None:\n",
    "                    yield current_coword, (word, float(current_cocurrence)/float(ngram_total))\n",
    "                current_coword = coword\n",
    "                current_cocurrence = count\n",
    "        if current_coword != None:\n",
    "            yield current_coword, (word, float(current_cocurrence)/float(ngram_total))\n",
    "    \n",
    "    # make the inverted index\n",
    "    def createInvertedIndex(self, cowords, word_freq):\n",
    "        yield cowords, '|'.join([\"%s:%s\"%(word, freq) for (word, freq) in word_freq])\n",
    "     \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    synonym_matrix.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "no configs found; falling back on auto-configuration\n",
      "no configs found; falling back on auto-configuration\n",
      "ignoring partitioner keyword arg (requires real Hadoop): 'org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner'\n",
      "\n",
      "PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "\n",
      "creating tmp directory c:\\temp\\synonym_matrix.kuanlin.20160214.053618.632000\n",
      "writing to c:\\temp\\synonym_matrix.kuanlin.20160214.053618.632000\\step-0-mapper_part-00000\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "writing to c:\\temp\\synonym_matrix.kuanlin.20160214.053618.632000\\step-0-mapper-sorted\n",
      "> sort 'c:\\temp\\synonym_matrix.kuanlin.20160214.053618.632000\\step-0-mapper_part-00000'\n",
      "writing to c:\\temp\\synonym_matrix.kuanlin.20160214.053618.632000\\step-0-reducer_part-00000\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "writing to c:\\temp\\synonym_matrix.kuanlin.20160214.053618.632000\\step-1-mapper_part-00000\n",
      "Counters from step 2:\n",
      "  (no counters found)\n",
      "writing to c:\\temp\\synonym_matrix.kuanlin.20160214.053618.632000\\step-1-mapper-sorted\n",
      "> sort 'c:\\temp\\synonym_matrix.kuanlin.20160214.053618.632000\\step-1-mapper_part-00000'\n",
      "writing to c:\\temp\\synonym_matrix.kuanlin.20160214.053618.632000\\step-1-reducer_part-00000\n",
      "Counters from step 2:\n",
      "  (no counters found)\n",
      "Moving c:\\temp\\synonym_matrix.kuanlin.20160214.053618.632000\\step-1-reducer_part-00000 -> bigram_results\\part-00000\n",
      "removing tmp directory c:\\temp\\synonym_matrix.kuanlin.20160214.053618.632000\n"
     ]
    }
   ],
   "source": [
    " !python synonym_matrix.py googlebooks-eng-all-5gram-20090715-0-filtered.txt --file vocabs.txt --output-dir bigram_results --no-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting synonym_detection_cosine.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile synonym_detection_cosine.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.protocol import RawValueProtocol\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "# synonym detection with cosine similarity\n",
    "\n",
    "class synonym_detection_cosine(MRJob):\n",
    "    \n",
    "    # load up vocabulary\n",
    "    def mapper_init(self):\n",
    "        self.vocab = []\n",
    "        for line in open('vocabs.txt', 'r'):\n",
    "            lineArr = line.strip().split('\\t')\n",
    "            self.vocab.append(lineArr[0].replace('\"', '').lower())\n",
    "            \n",
    "    def mapper(self, _, line):\n",
    "        lineArr = line.strip().replace('\"', '').split('\\t')\n",
    "        co_word = lineArr[0]\n",
    "        doc_terms = {}\n",
    "        # parse the stripes of co-occuring terms\n",
    "        for term_freq in lineArr[1].split('|'):\n",
    "            term = term_freq.split(':')[0]\n",
    "            freq = float(term_freq.split(':')[1])\n",
    "            doc_terms[term] = freq\n",
    "        \n",
    "        # emits components of cosine similarity calculation\n",
    "        for i, term1 in enumerate(self.vocab):\n",
    "            if i != len(self.vocab)-1:\n",
    "                for term2 in self.vocab[i:]:\n",
    "                    if term1 == term2:\n",
    "                        continue\n",
    "                    term1_freq = 0.0\n",
    "                    term2_freq = 0.0\n",
    "                    if term1 in doc_terms:\n",
    "                        term1_freq = doc_terms[term1]\n",
    "                    if term2 in doc_terms:\n",
    "                        term2_freq = doc_terms[term2]\n",
    "                    # collect data into co-occuring terms\n",
    "                    yield term1 + \":\" + term2, (term1_freq**2, term2_freq**2, term1_freq*term2_freq)\n",
    "    \n",
    "    # calculates the overall cosine similarity\n",
    "    def reducer(self, coterms, metric_vals):\n",
    "        a_b = 0.0\n",
    "        a_sqr = 0.0\n",
    "        b_sqr = 0.0\n",
    "        for data in metric_vals:\n",
    "            a_b += data[2]\n",
    "            a_sqr += data[0]\n",
    "            b_sqr += data[1]\n",
    "        consine_sim = a_b/((a_sqr**0.5)*(b_sqr**0.5))\n",
    "        # only print out similarity scoare above 0.5\n",
    "        if consine_sim > 0.5:\n",
    "            yield coterms, consine_sim\n",
    "    \n",
    "            \n",
    "if __name__ == '__main__':\n",
    "    synonym_detection_cosine.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "no configs found; falling back on auto-configuration\n",
      "no configs found; falling back on auto-configuration\n",
      "\n",
      "PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "\n",
      "creating tmp directory c:\\temp\\synonym_detection_cosine.kuanlin.20160214.070507.920000\n",
      "writing to c:\\temp\\synonym_detection_cosine.kuanlin.20160214.070507.920000\\step-0-mapper_part-00000\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "writing to c:\\temp\\synonym_detection_cosine.kuanlin.20160214.070507.920000\\step-0-mapper-sorted\n",
      "> sort 'c:\\temp\\synonym_detection_cosine.kuanlin.20160214.070507.920000\\step-0-mapper_part-00000'\n",
      "writing to c:\\temp\\synonym_detection_cosine.kuanlin.20160214.070507.920000\\step-0-reducer_part-00000\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "Moving c:\\temp\\synonym_detection_cosine.kuanlin.20160214.070507.920000\\step-0-reducer_part-00000 -> synonym_result1\\part-00000\n",
      "removing tmp directory c:\\temp\\synonym_detection_cosine.kuanlin.20160214.070507.920000\n"
     ]
    }
   ],
   "source": [
    " !python synonym_detection_cosine.py bigram_reduced.txt --file vocabs.txt --output-dir synonym_result1 --no-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting synonym_detection_jaccard.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile synonym_detection_jaccard.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.protocol import RawValueProtocol\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "# synonym detection with Jaccard index\n",
    "\n",
    "class synonym_detection_jaccard(MRJob):\n",
    "    \n",
    "    # load up vocabulary\n",
    "    def mapper_init(self):\n",
    "        self.vocab = []\n",
    "        for line in open('vocabs.txt', 'r'):\n",
    "            lineArr = line.strip().split('\\t')\n",
    "            self.vocab.append(lineArr[0].replace('\"', '').lower())\n",
    "            \n",
    "    def mapper(self, _, line):\n",
    "        lineArr = line.strip().replace('\"', '').split('\\t')\n",
    "        co_word = lineArr[0]\n",
    "        doc_terms = {}\n",
    "        # parse the stripes of co-occuring terms\n",
    "        for term_freq in lineArr[1].split('|'):\n",
    "            term = term_freq.split(':')[0]\n",
    "            freq = float(term_freq.split(':')[1])\n",
    "            doc_terms[term] = freq\n",
    "        \n",
    "        # emits components of jaccard similarity calculation\n",
    "        for i, term1 in enumerate(self.vocab):\n",
    "            if i != len(self.vocab)-1:\n",
    "                for term2 in self.vocab[i:]:\n",
    "                    if term1 == term2:\n",
    "                        continue\n",
    "                    term1_score = 0\n",
    "                    term2_score = 0\n",
    "                    if term1 in doc_terms:\n",
    "                        if doc_terms[term1] > 0:\n",
    "                            term1_score = 1\n",
    "                    if term2 in doc_terms:\n",
    "                        if doc_terms[term2] > 0:\n",
    "                            term2_score = 1\n",
    "                    # collect data into co-occuring terms\n",
    "                    yield term1 + \":\" + term2, (term1_score, term2_score)\n",
    "    \n",
    "    # calculates the overall cosine similarity\n",
    "    def reducer(self, coterms, metric_vals):\n",
    "        a_sum = 0\n",
    "        b_sum = 0\n",
    "        a_inter_b = 0\n",
    "        for data in metric_vals:\n",
    "            a_sum += data[0]\n",
    "            b_sum += data[1]\n",
    "            if data[0] > 0 and data[1] > 0:\n",
    "                a_inter_b += 1\n",
    "        jaccard_sim = float(a_inter_b)/float(a_sum + b_sum - a_inter_b)\n",
    "        # only print out similarity scoare above 0.2\n",
    "        if jaccard_sim > 0.2:\n",
    "            yield coterms, jaccard_sim\n",
    "    \n",
    "            \n",
    "if __name__ == '__main__':\n",
    "    synonym_detection_jaccard.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "no configs found; falling back on auto-configuration\n",
      "no configs found; falling back on auto-configuration\n",
      "\n",
      "PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "\n",
      "creating tmp directory c:\\temp\\synonym_detection_jaccard.kuanlin.20160214.071851.269000\n",
      "writing to c:\\temp\\synonym_detection_jaccard.kuanlin.20160214.071851.269000\\step-0-mapper_part-00000\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "writing to c:\\temp\\synonym_detection_jaccard.kuanlin.20160214.071851.269000\\step-0-mapper-sorted\n",
      "> sort 'c:\\temp\\synonym_detection_jaccard.kuanlin.20160214.071851.269000\\step-0-mapper_part-00000'\n",
      "writing to c:\\temp\\synonym_detection_jaccard.kuanlin.20160214.071851.269000\\step-0-reducer_part-00000\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "Moving c:\\temp\\synonym_detection_jaccard.kuanlin.20160214.071851.269000\\step-0-reducer_part-00000 -> synonym_result2\\part-00000\n",
      "removing tmp directory c:\\temp\\synonym_detection_jaccard.kuanlin.20160214.071851.269000\n"
     ]
    }
   ],
   "source": [
    " !python synonym_detection_jaccard.py bigram_reduced.txt --file vocabs.txt --output-dir synonym_result2 --no-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
