{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIDS W206 Assignment 1\n",
    "Student: Kuan Lin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <b>HW1.0.0. Define big data. Provide an example of a big data problem in your domain of expertise</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to do...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <b>HW1.0.1.In 500 words (English or pseudo code or a combination) describe how to estimate the bias, the variance, the irreduciable error for a test dataset T when using polynomial regression models of degree 1, 2,3, 4,5 are considered. How would you select a model?</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to do..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>spam filter section</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing pNaiveBayes.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile pNaiveBayes.sh\n",
    "## pNaiveBayes.sh\n",
    "## Author: Jake Ryland Williams\n",
    "## Usage: pNaiveBayes.sh m wordlist\n",
    "## Input:\n",
    "##       m = number of processes (maps), e.g., 4\n",
    "##       wordlist = a space-separated list of words in quotes, e.g., \"the and of\"\n",
    "##\n",
    "## Instructions: Read this script and its comments closely.\n",
    "##               Do your best to understand the purpose of each command,\n",
    "##               and focus on how arguments are supplied to mapper.py/reducer.py,\n",
    "##               as this will determine how the python scripts take input.\n",
    "##               When you are comfortable with the unix code below,\n",
    "##               answer the questions on the LMS for HW1 about the starter code.\n",
    "\n",
    "## collect user input\n",
    "m=$1 ## the number of parallel processes (maps) to run\n",
    "wordlist=$2 ## if set to \"*\", then all words are used\n",
    "\n",
    "## a test set data of 100 messages\n",
    "data=\"enronemail_1h.txt\" \n",
    "\n",
    "## the full set of data (33746 messages)\n",
    "# data=\"enronemail.txt\" \n",
    "\n",
    "## 'wc' determines the number of lines in the data\n",
    "## 'perl -pe' regex strips the piped wc output to a number\n",
    "linesindata=`wc -l $data | perl -pe 's/^.*?(\\d+).*?$/$1/'`\n",
    "\n",
    "## determine the lines per chunk for the desired number of processes\n",
    "linesinchunk=`echo \"$linesindata/$m+1\" | bc`\n",
    "\n",
    "## split the original file into chunks by line\n",
    "split -l $linesinchunk $data $data.chunk.\n",
    "\n",
    "## assign python mappers (mapper.py) to the chunks of data\n",
    "## and emit their output to temporary files\n",
    "for datachunk in $data.chunk.*; do\n",
    "    ## feed word list to the python mapper here and redirect STDOUT to a temporary file on disk\n",
    "    ####\n",
    "    ####\n",
    "    ./mapper.py $datachunk \"$wordlist\" > $datachunk.counts &\n",
    "    ####\n",
    "    ####\n",
    "done\n",
    "## wait for the mappers to finish their work\n",
    "wait\n",
    "\n",
    "## 'ls' makes a list of the temporary count files\n",
    "## 'perl -pe' regex replaces line breaks with spaces\n",
    "countfiles=`\\ls $data.chunk.*.counts | perl -pe 's/\\n/ /'`\n",
    "\n",
    "## feed the list of countfiles to the python reducer and redirect STDOUT to disk\n",
    "####\n",
    "####\n",
    "./reducer.py $countfiles > $data.output\n",
    "####\n",
    "####\n",
    "\n",
    "## clean up the data chunks and temporary count files\n",
    "\\rm $data.chunk.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x pNaiveBayes.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <b>HW1.1. Read through the provided control script (pNaiveBayes.sh)\n",
    "   and all of its comments. When you are comfortable with their\n",
    "   purpose and function, respond to the remaining homework questions below.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <b>HW1.2. Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh\n",
    "   will determine the number of occurrences of a single, user-specified word. Examine the word “assistance” and report your results.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Kuan Lin\n",
    "## Description: mapper code for HW1.2\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import string\n",
    "\n",
    "## collect user input\n",
    "filename = sys.argv[1]\n",
    "if sys.argv[2] == \"*\":\n",
    "\tuseAllWords = True\n",
    "else:\n",
    "\tuseAllWords = False\n",
    "\tfindwords = re.split(\" \",sys.argv[2].lower())\n",
    "\n",
    "word_counts = {}\n",
    "\n",
    "for line in open(filename, 'r'):\n",
    "\tline = line.strip().lower()\n",
    "\tif line == \"\": continue\n",
    "\tlineArr = line.split('\\t')\n",
    "\t# check data integrity.  each valid line should have 4 fields, with second field equals either 0 or 1\n",
    "\tif len(lineArr) != 4 or (lineArr[1] != \"0\" and lineArr[1] != \"1\"): continue\n",
    "\t\n",
    "\tspam_flag = lineArr[1]\n",
    "\tsubject_body = lineArr[2] + ' ' + lineArr[3]\n",
    "\t# remove punctuations\n",
    "\tsubject_body = subject_body.translate(string.maketrans(\"\",\"\"), string.punctuation)\n",
    "\t\n",
    "\tif useAllWords:\n",
    "\t\tfindwords = set([word.strip() for word in subject_body.split(' ') if word.strip() != ''])\n",
    "\t\n",
    "\tfor word in findwords:\n",
    "\t\ttry:\n",
    "\t\t\tpattern = re.compile('[\\s+]?(' + word + ')[\\s+]?')\n",
    "\t\t\tif word in word_counts:\n",
    "\t\t\t\tword_counts[word] += len(re.findall(pattern, subject_body))\n",
    "\t\t\telse:\n",
    "\t\t\t\tword_counts[word] = len(re.findall(pattern, subject_body))\n",
    "\t\texcept:\n",
    "\t\t\tif word in word_counts:\n",
    "\t\t\t\tword_counts[word] += subject_body.count(word)\n",
    "\t\t\telse:\n",
    "\t\t\t\tword_counts[word] = subject_body.count(word)\n",
    "\t\t\n",
    "print '\\n'.join([word + '\\t' + str(word_counts[word]) for word in word_counts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Kuan Lin\n",
    "## Description: reducer code for HW1.2\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "files = sys.argv[1:]\n",
    "#print str(files)\n",
    "\n",
    "word_counts = {}\n",
    "for file in files:\n",
    "\tfor line in open(file, 'r'):\n",
    "\t\tline = line.strip()\n",
    "\t\tif line == \"\": continue\n",
    "\t\t\n",
    "\t\tword = line.split('\\t')[0]\n",
    "\t\tcount = int(line.split('\\t')[1])\n",
    "\t\t\n",
    "\t\tif word in word_counts:\n",
    "\t\t\tword_counts[word] += count\n",
    "\t\telse:\n",
    "\t\t\tword_counts[word] = count\n",
    "\t\t\t\n",
    "print '\\n'.join([word + '\\t' + str(word_counts[word]) for word in word_counts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x mapper.py; chmod +x reducer.py; ./pNaiveBayes.sh 4 assistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistance\t9\r\n"
     ]
    }
   ],
   "source": [
    "!cat enronemail_1h.txt.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 email records were excluded due to bad formatting in the source data.  In the remaining valid email entries there are 9 instances of the word \"assistance\" in the email subject and body."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <b>HW1.3. Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh\n",
    "   will classify the email messages by a single, user-specified word using the multinomial Naive Bayes Formulation. Examine the word “assistance” and report your results.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Kuan Lin\n",
    "## Description: mapper code for HW1.3\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import string\n",
    "\n",
    "## collect user input\n",
    "filename = sys.argv[1]\n",
    "if sys.argv[2] == \"*\":\n",
    "\tuseAllWords = True\n",
    "else:\n",
    "\tuseAllWords = False\n",
    "\tfindwords = re.split(\" \",sys.argv[2].lower())\n",
    "\n",
    "spam_word_counts = {}\n",
    "ham_word_counts = {}\n",
    "spam_total_words = 0\n",
    "ham_total_words = 0\n",
    "\n",
    "for line in open(filename, 'r'):\n",
    "\tline = line.strip().lower()\n",
    "\tif line == \"\": continue\n",
    "\tlineArr = line.split('\\t')\n",
    "\t# check data integrity.  each valid line should have 4 fields, with second field equals either 0 or 1\n",
    "\tif len(lineArr) != 4 or (lineArr[1] != \"0\" and lineArr[1] != \"1\"): continue\n",
    "\t\n",
    "\tspam_flag = lineArr[1]\n",
    "\tsubject_body = lineArr[2] + ' ' + lineArr[3]\n",
    "\t# remove punctuations\n",
    "\tsubject_body = subject_body.translate(string.maketrans(\"\",\"\"), string.punctuation).strip()\n",
    "\tall_words = [word.strip() for word in subject_body.split(' ') if word.strip() != '']\n",
    "\tif useAllWords:\n",
    "\t\tfindwords = set(all_words)\n",
    "\t\t\n",
    "\tif spam_flag == \"1\":\n",
    "\t\tspam_total_words += len(all_words)\n",
    "\telse:\n",
    "\t\tham_total_words += len(all_words)\n",
    "\t\n",
    "\tfor word in findwords:\n",
    "\t\tpattern = re.compile('[\\s+]?(' + word + ')[\\s+]?')\n",
    "\t\tif spam_flag == \"1\":\n",
    "\t\t\ttry:\n",
    "\t\t\t\tif word in spam_word_counts: spam_word_counts[word] += len(re.findall(pattern, subject_body))\n",
    "\t\t\t\telse: spam_word_counts[word] = len(re.findall(pattern, subject_body))\n",
    "\t\t\texcept:\n",
    "\t\t\t\tif word in spam_word_counts: spam_word_counts[word] += subject_body.count(word)\n",
    "\t\t\t\telse: spam_word_counts[word] = subject_body.count(word)\n",
    "\t\telse:\n",
    "\t\t\ttry:\n",
    "\t\t\t\tif word in ham_word_counts: ham_word_counts[word] += len(re.findall(pattern, subject_body))\n",
    "\t\t\t\telse: ham_word_counts[word] = len(re.findall(pattern, subject_body))\n",
    "\t\t\texcept:\n",
    "\t\t\t\tif word in ham_word_counts: ham_word_counts[word] += subject_body.count(word)\n",
    "\t\t\t\telse: ham_word_counts[word] = subject_body.count(word)\n",
    "\t\t\t\n",
    "\t\t\n",
    "print '\\n'.join([word + '\\t' + str(spam_word_counts[word]) + '\\t1' for word in spam_word_counts] + \n",
    "[word + '\\t' + str(ham_word_counts[word]) + '\\t0' for word in ham_word_counts] +\n",
    "['class_total_words\\t' + str(spam_total_words) + '\\t1', 'class_total_words\\t' + str(ham_total_words) + '\\t0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Kuan Lin\n",
    "## Description: reducer code for HW1.3\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import math\n",
    "import string\n",
    "\n",
    "files = sys.argv[1:]\n",
    "data=\"enronemail_1h.txt\"\n",
    "\n",
    "spam_word_counts = {}\n",
    "ham_word_counts = {}\n",
    "spam_total_words = 0\n",
    "ham_total_words = 0\n",
    "\n",
    "# assume equal prior for spam and ham\n",
    "spam_prior = 0.44\n",
    "ham_prior = 0.56\n",
    "\n",
    "def spamWordProb(word):\n",
    "\tcount = 0\n",
    "\tif word in spam_word_counts: count = spam_word_counts[word]\n",
    "\t# apply laplace smoothing\n",
    "\treturn float(count+1)/float(spam_total_words+len(spam_word_counts))\n",
    "\t\n",
    "def hamWordProb(word):\n",
    "\tcount = 0\n",
    "\tif word in ham_word_counts: count = ham_word_counts[word]\n",
    "\t# apply laplace smoothing\n",
    "\treturn float(count+1)/float(ham_total_words+len(ham_word_counts))\n",
    "\t\n",
    "def classifyWithWordList(document, word_list):\n",
    "\tspam_logprob = math.log(spam_prior)\n",
    "\tham_logprob = math.log(ham_prior)\n",
    "\tfor word in word_list:\n",
    "\t\tpattern = re.compile('[\\s+]?(' + word + ')[\\s+]?')\n",
    "\t\tword_count = len(re.findall(pattern, document))\n",
    "\t\tspam_logprob += math.log(spamWordProb(word))*word_count\n",
    "\t\tham_logprob += math.log(hamWordProb(word))*word_count\n",
    "\t\t\n",
    "\tif spam_logprob > ham_logprob:\n",
    "\t\treturn 1\n",
    "\telse:\n",
    "\t\treturn 0\n",
    "\n",
    "for file in files:\n",
    "\tfor line in open(file, 'r'):\n",
    "\t\tline = line.strip()\n",
    "\t\tif line == \"\": continue\n",
    "\t\t\n",
    "\t\tlineArr = line.split('\\t')\n",
    "\t\tword = lineArr[0]\n",
    "\t\tcount = int(lineArr[1])\n",
    "\t\tspam_flag = int(lineArr[2])\n",
    "\t\t\n",
    "\t\tif spam_flag == 1:\n",
    "\t\t\tif word == \"class_total_words\":\n",
    "\t\t\t\tspam_total_words += count\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tif word in spam_word_counts:\n",
    "\t\t\t\tspam_word_counts[word] += count\n",
    "\t\t\telse:\n",
    "\t\t\t\tspam_word_counts[word] = count\n",
    "\t\telse:\n",
    "\t\t\tif word == \"class_total_words\":\n",
    "\t\t\t\tham_total_words += count\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tif word in ham_word_counts:\n",
    "\t\t\t\tham_word_counts[word] += count\n",
    "\t\t\telse:\n",
    "\t\t\t\tham_word_counts[word] = count\n",
    "\t\t\t\n",
    "# perform single-word classification\n",
    "key_word = spam_word_counts.keys()[0]\n",
    "total_msg_classified = 0\n",
    "total_msg_correct = 0\n",
    "for line in open(data, 'r'):\n",
    "\tline = line.strip()\n",
    "\tlineArr = line.split('\\t')\n",
    "\t# check data integrity.  each valid line should have 4 fields, with second field equals either 0 or 1\n",
    "\tif len(lineArr) != 4 or (lineArr[1] != \"0\" and lineArr[1] != \"1\"): continue\n",
    "\tmsg_id = lineArr[0]\n",
    "\tspam_flag = lineArr[1]\n",
    "\tsubject_body = lineArr[2] + ' ' + lineArr[3]\n",
    "\t# remove punctuations\n",
    "\tsubject_body = subject_body.translate(string.maketrans(\"\",\"\"), string.punctuation).strip()\n",
    "\tspam_flag_predicted = str(classifyWithWordList(subject_body, [key_word]))\n",
    "\ttotal_msg_classified += 1\n",
    "\tif spam_flag_predicted == spam_flag: total_msg_correct += 1\n",
    "\tprint '\\t'.join([msg_id, spam_flag, spam_flag_predicted])\n",
    "print \"-----------------------------------------------------\"\n",
    "print \"total document classified: \" + str(total_msg_classified)\n",
    "print \"total document correctly classified: \" + str(total_msg_correct)\n",
    "print \"accuracy: %.4f\"%(float(total_msg_correct)/float(total_msg_classified))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x mapper.py; chmod +x reducer.py; ./pNaiveBayes.sh 4 assistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total document classified: 98\r\n",
      "total document correctly classified: 58\r\n",
      "accuracy: 0.5918\r\n"
     ]
    }
   ],
   "source": [
    "!tail -3 enronemail_1h.txt.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "again, two documents were excluded due to bad formating in the source data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <b>HW1.4. Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh\n",
    "   will classify the email messages by a list of one or more user-specified words. Examine the words “assistance”, “valium”, and “enlargementWithATypo” and report your results</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Kuan Lin\n",
    "## Description: mapper code for HW1.4\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import string\n",
    "\n",
    "## collect user input\n",
    "filename = sys.argv[1]\n",
    "if sys.argv[2] == \"*\":\n",
    "\tuseAllWords = True\n",
    "else:\n",
    "\tuseAllWords = False\n",
    "\tfindwords = re.split(\" \",sys.argv[2].lower())\n",
    "\n",
    "spam_word_counts = {}\n",
    "ham_word_counts = {}\n",
    "spam_total_words = 0\n",
    "ham_total_words = 0\n",
    "\n",
    "for line in open(filename, 'r'):\n",
    "\tline = line.strip().lower()\n",
    "\tif line == \"\": continue\n",
    "\tlineArr = line.split('\\t')\n",
    "\t# check data integrity.  each valid line should have 4 fields, with second field equals either 0 or 1\n",
    "\tif len(lineArr) != 4 or (lineArr[1] != \"0\" and lineArr[1] != \"1\"): continue\n",
    "\t\n",
    "\tspam_flag = lineArr[1]\n",
    "\tsubject_body = lineArr[2] + ' ' + lineArr[3]\n",
    "\t# remove punctuations\n",
    "\tsubject_body = subject_body.translate(string.maketrans(\"\",\"\"), string.punctuation).strip().lower()\n",
    "\tall_words = [word.strip() for word in subject_body.split(' ') if word.strip() != '']\n",
    "\tif useAllWords:\n",
    "\t\tfindwords = set(all_words)\n",
    "\t\t\n",
    "\tif spam_flag == \"1\":\n",
    "\t\tspam_total_words += len(all_words)\n",
    "\telse:\n",
    "\t\tham_total_words += len(all_words)\n",
    "\t\n",
    "\tfor word in findwords:\n",
    "\t\tpattern = re.compile('[\\s+]?(' + word + ')[\\s+]?')\n",
    "\t\tif spam_flag == \"1\":\n",
    "\t\t\ttry:\n",
    "\t\t\t\tif word in spam_word_counts: spam_word_counts[word] += len(re.findall(pattern, subject_body))\n",
    "\t\t\t\telse: spam_word_counts[word] = len(re.findall(pattern, subject_body))\n",
    "\t\t\texcept:\n",
    "\t\t\t\tif word in spam_word_counts: spam_word_counts[word] += subject_body.count(word)\n",
    "\t\t\t\telse: spam_word_counts[word] = subject_body.count(word)\n",
    "\t\telse:\n",
    "\t\t\ttry:\n",
    "\t\t\t\tif word in ham_word_counts: ham_word_counts[word] += len(re.findall(pattern, subject_body))\n",
    "\t\t\t\telse: ham_word_counts[word] = len(re.findall(pattern, subject_body))\n",
    "\t\t\texcept:\n",
    "\t\t\t\tif word in ham_word_counts: ham_word_counts[word] += subject_body.count(word)\n",
    "\t\t\t\telse: ham_word_counts[word] = subject_body.count(word)\n",
    "\t\t\t\n",
    "\t\t\n",
    "print '\\n'.join([word + '\\t' + str(spam_word_counts[word]) + '\\t1' for word in spam_word_counts] + \n",
    "[word + '\\t' + str(ham_word_counts[word]) + '\\t0' for word in ham_word_counts] +\n",
    "['class_total_words\\t' + str(spam_total_words) + '\\t1', 'class_total_words\\t' + str(ham_total_words) + '\\t0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Kuan Lin\n",
    "## Description: reducer code for HW1.4\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import math\n",
    "import string\n",
    "\n",
    "files = sys.argv[1:]\n",
    "data=\"enronemail_1h.txt\"\n",
    "\n",
    "spam_word_counts = {}\n",
    "ham_word_counts = {}\n",
    "spam_total_words = 0\n",
    "ham_total_words = 0\n",
    "\n",
    "# assume equal prior for spam and ham\n",
    "spam_prior = 0.44\n",
    "ham_prior = 0.56\n",
    "\n",
    "def spamWordProb(word):\n",
    "\tcount = 0\n",
    "\tif word in spam_word_counts: count = spam_word_counts[word]\n",
    "\t# apply laplace smoothing\n",
    "\treturn float(count+1)/float(spam_total_words+len(spam_word_counts))\n",
    "\t\n",
    "def hamWordProb(word):\n",
    "\tcount = 0\n",
    "\tif word in ham_word_counts: count = ham_word_counts[word]\n",
    "\t# apply laplace smoothing\n",
    "\treturn float(count+1)/float(ham_total_words+len(ham_word_counts))\n",
    "\t\n",
    "def classifyWithWordList(document, word_list):\n",
    "\tspam_logprob = math.log(spam_prior)\n",
    "\tham_logprob = math.log(ham_prior)\n",
    "\tfor word in word_list:\n",
    "\t\tpattern = re.compile('[\\s+]?(' + word + ')[\\s+]?')\n",
    "\t\tword_count = len(re.findall(pattern, document))\n",
    "\t\tspam_logprob += math.log(spamWordProb(word))*word_count\n",
    "\t\tham_logprob += math.log(hamWordProb(word))*word_count\n",
    "\t\t\n",
    "\tif spam_logprob > ham_logprob:\n",
    "\t\treturn 1\n",
    "\telse:\n",
    "\t\treturn 0\n",
    "\n",
    "for file in files:\n",
    "\tfor line in open(file, 'r'):\n",
    "\t\tline = line.strip()\n",
    "\t\tif line == \"\": continue\n",
    "\t\t\n",
    "\t\tlineArr = line.split('\\t')\n",
    "\t\tword = lineArr[0]\n",
    "\t\tcount = int(lineArr[1])\n",
    "\t\tspam_flag = int(lineArr[2])\n",
    "\t\t\n",
    "\t\tif spam_flag == 1:\n",
    "\t\t\tif word == \"class_total_words\":\n",
    "\t\t\t\tspam_total_words += count\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tif word in spam_word_counts:\n",
    "\t\t\t\tspam_word_counts[word] += count\n",
    "\t\t\telse:\n",
    "\t\t\t\tspam_word_counts[word] = count\n",
    "\t\telse:\n",
    "\t\t\tif word == \"class_total_words\":\n",
    "\t\t\t\tham_total_words += count\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tif word in ham_word_counts:\n",
    "\t\t\t\tham_word_counts[word] += count\n",
    "\t\t\telse:\n",
    "\t\t\t\tham_word_counts[word] = count\n",
    "\t\t\t\n",
    "# perform classification with a word-list\n",
    "key_words = spam_word_counts.keys()\n",
    "total_msg_classified = 0\n",
    "total_msg_correct = 0\n",
    "for line in open(data, 'r'):\n",
    "\tline = line.strip()\n",
    "\tlineArr = line.split('\\t')\n",
    "\t# check data integrity.  each valid line should have 4 fields, with second field equals either 0 or 1\n",
    "\tif len(lineArr) != 4 or (lineArr[1] != \"0\" and lineArr[1] != \"1\"): continue\n",
    "\tmsg_id = lineArr[0]\n",
    "\tspam_flag = lineArr[1]\n",
    "\tsubject_body = lineArr[2] + ' ' + lineArr[3]\n",
    "\t# remove punctuations\n",
    "\tsubject_body = subject_body.translate(string.maketrans(\"\",\"\"), string.punctuation).strip()\n",
    "\tspam_flag_predicted = str(classifyWithWordList(subject_body, key_words))\n",
    "\ttotal_msg_classified += 1\n",
    "\tif spam_flag_predicted == spam_flag: total_msg_correct += 1\n",
    "\tprint '\\t'.join([msg_id, spam_flag, spam_flag_predicted])\n",
    "print \"-----------------------------------------------------\"\n",
    "print \"total document classified: \" + str(total_msg_classified)\n",
    "print \"total document correctly classified: \" + str(total_msg_correct)\n",
    "print \"accuracy: %.4f\"%(float(total_msg_correct)/float(total_msg_classified))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x mapper.py; chmod +x reducer.py; ./pNaiveBayes.sh 4 \"assistance valium enlargementWithATypo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total document classified: 98\r\n",
      "total document correctly classified: 61\r\n",
      "accuracy: 0.6224\r\n"
     ]
    }
   ],
   "source": [
    "!tail -3 enronemail_1h.txt.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <b>HW1.5. Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh\n",
    "   will classify the email messages by all words present.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Kuan Lin\n",
    "## Description: mapper code for HW1.5\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import string\n",
    "\n",
    "## collect user input\n",
    "filename = sys.argv[1]\n",
    "if sys.argv[2] == \"*\":\n",
    "\tuseAllWords = True\n",
    "\tfindwords = None\n",
    "else:\n",
    "\tuseAllWords = False\n",
    "\tfindwords = re.split(\" \",sys.argv[2].lower())\n",
    "\n",
    "spam_word_counts = {}\n",
    "ham_word_counts = {}\n",
    "spam_total_words = 0\n",
    "ham_total_words = 0\n",
    "\n",
    "def findWordCount(doc, word):\n",
    "\tcount = 0\n",
    "\tfor w in doc.split(' '):\n",
    "\t\tif w == word: count += 1\n",
    "\treturn count\n",
    "\n",
    "for line in open(filename, 'r'):\n",
    "\tline = line.strip().lower()\n",
    "\tif line == \"\": continue\n",
    "\tlineArr = line.split('\\t')\n",
    "\t# check data integrity.  each valid line should have 4 fields, with second field equals either 0 or 1\n",
    "\tif len(lineArr) != 4 or (lineArr[1] != \"0\" and lineArr[1] != \"1\"): continue\n",
    "\t\n",
    "\tspam_flag = lineArr[1]\n",
    "\tsubject_body = lineArr[2] + ' ' + lineArr[3]\n",
    "\t# remove punctuations\n",
    "\tsubject_body = subject_body.translate(string.maketrans(\"\",\"\"), string.punctuation).strip().lower()\n",
    "\tall_words = [word.strip() for word in subject_body.split(' ') if word.strip() != '']\n",
    "\t#print str(all_words)\n",
    "\tif useAllWords:\n",
    "\t\tfindwords = set([w for w in all_words])\n",
    "\t\n",
    "\tif spam_flag == \"1\":\n",
    "\t\tspam_total_words += len(all_words)\n",
    "\telse:\n",
    "\t\tham_total_words += len(all_words)\n",
    "\t\t\n",
    "\tfor word in findwords:\n",
    "\t\t#pattern = re.compile('[\\s+]?(' + word + ')[\\s+]?')\n",
    "\t\tif spam_flag == \"1\":\n",
    "\t\t\tif word in spam_word_counts: spam_word_counts[word] += findWordCount(subject_body, word)\n",
    "\t\t\telse: spam_word_counts[word] = findWordCount(subject_body, word)\n",
    "\t\telse:\n",
    "\t\t\tif word in ham_word_counts: ham_word_counts[word] += findWordCount(subject_body, word)\n",
    "\t\t\telse: ham_word_counts[word] = findWordCount(subject_body, word)\n",
    "\t\t\t\n",
    "print '\\n'.join([word + '\\t' + str(spam_word_counts[word]) + '\\t1' for word in spam_word_counts] + \n",
    "[word + '\\t' + str(ham_word_counts[word]) + '\\t0' for word in ham_word_counts] +\n",
    "['class_total_words\\t' + str(spam_total_words) + '\\t1', 'class_total_words\\t' + str(ham_total_words) + '\\t0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Kuan Lin\n",
    "## Description: reducer code for HW1.5\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import math\n",
    "import string\n",
    "\n",
    "files = sys.argv[1:]\n",
    "data=\"enronemail_1h.txt\"\n",
    "\n",
    "spam_word_counts = {}\n",
    "ham_word_counts = {}\n",
    "spam_total_words = 0\n",
    "ham_total_words = 0\n",
    "unique_words = set()\n",
    "\n",
    "# assume equal prior for spam and ham\n",
    "spam_prior = 0.44\n",
    "ham_prior = 0.56\n",
    "\n",
    "def findWordCount(doc, word):\n",
    "\tcount = 0\n",
    "\tfor w in doc.split(' '):\n",
    "\t\tif w == word: count += 1\n",
    "\treturn count\n",
    "\n",
    "def spamWordProb(word):\n",
    "\tcount = 0\n",
    "\tif word in spam_word_counts: count = spam_word_counts[word]\n",
    "\t# apply laplace smoothing\n",
    "\treturn float(count+1)/float(spam_total_words+len(unique_words))\n",
    "\t\n",
    "def hamWordProb(word):\n",
    "\tcount = 0\n",
    "\tif word in ham_word_counts: count = ham_word_counts[word]\n",
    "\t# apply laplace smoothing\n",
    "\treturn float(count+1)/float(ham_total_words+len(unique_words))\n",
    "\t\n",
    "def classifyWithWordList(document, word_list):\n",
    "\tspam_logprob = math.log(spam_prior)\n",
    "\tham_logprob = math.log(ham_prior)\n",
    "\tfor word in word_list:\n",
    "\t\t#pattern = re.compile('[\\s+]?(' + word + ')[\\s+]?')\n",
    "\t\tword_count = findWordCount(document, word)\n",
    "\t\tspam_logprob += math.log(spamWordProb(word))*word_count\n",
    "\t\tham_logprob += math.log(hamWordProb(word))*word_count\n",
    "\t\n",
    "\t#print \"%.4f | %.4f\" % (spam_logprob, ham_logprob)\n",
    "\tif spam_logprob > ham_logprob:\n",
    "\t\treturn 1\n",
    "\telse:\n",
    "\t\treturn 0\n",
    "\n",
    "for file in files:\n",
    "\tfor line in open(file, 'r'):\n",
    "\t\tline = line.strip()\n",
    "\t\tif line == \"\": continue\n",
    "\t\t\n",
    "\t\tlineArr = line.split('\\t')\n",
    "\t\tword = lineArr[0]\n",
    "\t\tcount = int(lineArr[1])\n",
    "\t\tspam_flag = int(lineArr[2])\n",
    "\t\t\n",
    "\t\tif spam_flag == 1:\n",
    "\t\t\tif word == \"class_total_words\":\n",
    "\t\t\t\tspam_total_words += count\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tif word in spam_word_counts:\n",
    "\t\t\t\tspam_word_counts[word] += count\n",
    "\t\t\telse:\n",
    "\t\t\t\tspam_word_counts[word] = count\n",
    "\t\telse:\n",
    "\t\t\tif word == \"class_total_words\":\n",
    "\t\t\t\tham_total_words += count\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tif word in ham_word_counts:\n",
    "\t\t\t\tham_word_counts[word] += count\n",
    "\t\t\telse:\n",
    "\t\t\t\tham_word_counts[word] = count\n",
    "\t\t\t\n",
    "# perform classification with all words\n",
    "unique_words = set([k for k in spam_word_counts.keys()] + [k for k in ham_word_counts.keys()])\n",
    "total_msg_classified = 0\n",
    "total_msg_correct = 0\n",
    "for line in open(data, 'r'):\n",
    "\tline = line.strip()\n",
    "\tlineArr = line.split('\\t')\n",
    "\t# check data integrity.  each valid line should have 4 fields, with second field equals either 0 or 1\n",
    "\tif len(lineArr) != 4 or (lineArr[1] != \"0\" and lineArr[1] != \"1\"): continue\n",
    "\tmsg_id = lineArr[0]\n",
    "\tspam_flag = lineArr[1]\n",
    "\tsubject_body = lineArr[2] + ' ' + lineArr[3]\n",
    "\t# remove punctuations\n",
    "\tsubject_body = subject_body.translate(string.maketrans(\"\",\"\"), string.punctuation).strip().lower()\n",
    "\tspam_flag_predicted = str(classifyWithWordList(subject_body, unique_words))\n",
    "\ttotal_msg_classified += 1\n",
    "\tif spam_flag_predicted == spam_flag: total_msg_correct += 1\n",
    "\tprint '\\t'.join([msg_id, spam_flag, spam_flag_predicted])\n",
    "print \"-----------------------------------------------------\"\n",
    "print \"feature size: \" + str(len(unique_words))\n",
    "print \"spam/ham total words: %s | %s\" % (spam_total_words, ham_total_words)\n",
    "print \"total document classified: \" + str(total_msg_classified)\n",
    "print \"total document correctly classified: \" + str(total_msg_correct)\n",
    "print \"accuracy: %.4f\"%(float(total_msg_correct)/float(total_msg_classified))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x mapper.py; chmod +x reducer.py; ./pNaiveBayes.sh 4 \"*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total document classified: 98\r\n",
      "total document correctly classified: 98\r\n",
      "accuracy: 1.0000\r\n"
     ]
    }
   ],
   "source": [
    "!tail -3 enronemail_1h.txt.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <b>HW1.6 - Benchmark with Scikit-Learn</b>\n",
    "- Run the Multinomial Naive Bayes algorithm (using default settings) from SciKit-Learn over the same training data used in HW1.5 and report the Training error (please note some data preparation might be needed to get the Multinomial Naive Bayes algorithm from SkiKit-Learn to run over this dataset)\n",
    "- Run the Bernoulli Naive Bayes algorithm from SciKit-Learn (using default settings) over the same training data used in HW1.5 and report the Training error \n",
    "- Run the Multinomial Naive Bayes algorithm you developed for HW1.5 over the same data used HW1.5 and report the Training error - Please prepare a table to present your results\n",
    "- Explain/justify any differences in terms of training error rates over the dataset in HW1.5 between your Multinomial Naive Bayes implementation (in Map Reduce) versus the Multinomial Naive Bayes implementation in SciKit-Learn (Hint: smoothing, which we will discuss in next lecture)\n",
    "- Discuss the performance differences in terms of training error rates over the dataset in HW1.5 between the  Multinomial Naive Bayes implementation in SciKit-Learn with the  Bernoulli Naive Bayes implementation in SciKit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB Accuracy: 1.0000\n",
      "BernoulliNB Accuracy: 0.7857\n"
     ]
    }
   ],
   "source": [
    "data = \"enronemail_1h.txt\"\n",
    "\n",
    "import string\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import *\n",
    "\n",
    "train_data_raw = []\n",
    "train_labels = []\n",
    "\n",
    "# load data from file\n",
    "for line in open(data, 'r'):\n",
    "    line = line.strip().lower()\n",
    "    if line == \"\": continue\n",
    "    lineArr = line.split('\\t')\n",
    "    if len(lineArr) != 4 or (lineArr[1] != \"0\" and lineArr[1] != \"1\"): continue\n",
    "    spam_flag = lineArr[1]\n",
    "    subject_body = lineArr[2] + ' ' + lineArr[3]\n",
    "    subject_body = subject_body.translate(string.maketrans(\"\",\"\"), string.punctuation).strip().lower()\n",
    "    train_data_raw.append(subject_body)\n",
    "    train_labels.append(spam_flag)\n",
    "\n",
    "# fit with count vectorizer\n",
    "train_data = CountVectorizer().fit_transform(train_data_raw)\n",
    "#print train_data.shape\n",
    "\n",
    "# Multinomial NB with default settings:\n",
    "multiNB_train_accuracy = MultinomialNB().fit(train_data, train_labels).score(train_data, train_labels)\n",
    "print \"MultinomialNB Accuracy: %.4f\" % multiNB_train_accuracy\n",
    "\n",
    "# Bernouli NB with default settings:\n",
    "bernoulliNB_train_accuracy = BernoulliNB().fit(train_data, train_labels).score(train_data, train_labels)\n",
    "print \"BernoulliNB Accuracy: %.4f\" % bernoulliNB_train_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The above code obtains classification accuracy scores for MultinomialNB and BernoulliNB classifier.  The accuracy score for our custom-built MultinomialNB is obtained previously.  Training error is just 1-accuracy.  Thus:\n",
    "\n",
    "<table>\n",
    "<tr><th></th><th>Train Accuracy</th><th>Train Error</th></tr>\n",
    "<tr><th>Sklearn MultinomialNB</th><td>1.00</td><td>0.00</td></tr>\n",
    "<tr><th>Sklearn BernoulliNB</th><td>0.7857</td><td>0.2143</td></tr>\n",
    "<tr><th>Custom MultinomialNB</th><td>1.00</td><td>0.00</td></tr>\n",
    "</table>\n",
    "\n",
    "* Since we are calculating error rate using training data, it is not surprising that the MultinomialNB from Sklearn and our own custom-built model achieved zero training error.  This means that both models can explain all of the variations in the training data.  However, this does not gaurantee that the model will perform as well on future data.\n",
    "* BernoulliNB model performs worse because it uses less information from the training data.  The BernoulliNB model only looks at if a term appears in a document, and does not take into account of how many times the term appear in the same document.  This will cause the BernoulliNB model to be unable to distinquish documents with the same words but with different frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
