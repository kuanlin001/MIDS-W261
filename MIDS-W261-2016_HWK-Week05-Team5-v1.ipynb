{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW4, DATSCI W261\n",
    "Team: Kuan Lin, Alejandro J. Rojas, Ricardo Barrera<br/>\n",
    "Emails: kuanlin@ischool.berkeley.edu, ale@ischool.berkeley.edu, ricardofrank@ischool.berkeley.edu<br/>\n",
    "Time of Initial Submission: 8:00 AM PST, Thursday, Feb 12, 2016<br/>\n",
    "W261-1, Spring 2016 Week 4 Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 4.0.\n",
    "What is MrJob? How is it different to Hadoop MapReduce? What are the mappint_init, mapper_final(), combiner_final(), reducer_final() methods? When are they called?\n",
    "<p>\n",
    "MrJob is a python object-oriented framework that let's you define classes that integrates Hadoop MapReduce in a very elegant and clean structure so you can write and run MapReduce jobs. It contains methods that leverage Hadoop streaming to accomplish MapReduce taks. An MrJob class lets you define steps that you want your routine to follow. This python program comes with a variery of methods that make it simple to implement MapReduce, among them: mappint_init() -> to initialize mapper values mapper_final() -> returns key,value tuple where value is 1 combiner_final() -> returns key,value tuple where value is local sum reducer_final() -> returns key,value tuple where value is total sum\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 4.1\n",
    "What is serialization in the context of MrJob or Hadoop? When it used in these frameworks? What is the default serialization mode for input and outputs for MrJob?\n",
    "<p>\n",
    "MrJob does not take any input in binary. It accepts either text or json which makes it slower that running Hadoop streaming directly. So after receiving its input it converts it into ninary to do all processing. This is inefficient in terms of processing time but teh framework gives the flexibility to quickly write and run easily readable code which makes it an ideal solution to write and run MapReduce jobs.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 4.2\n",
    "Recall the Microsoft logfiles data from the async lecture. The logfiles are described are located at:<br/> https://kdd.ics.uci.edu/databases/msweb/msweb.html<br/>\n",
    "http://archive.ics.uci.edu/ml/machine-learning-databases/anonymous/ <br/>\n",
    "This dataset records which areas (Vroots) of www.microsoft.com each user visited in a one-week timeframe in Feburary 1998.\n",
    "Here, you must preprocess the data on a single node (i.e., not on a cluster of nodes) from the format:\n",
    "<pre>\n",
    "C,\"10001\",10001   #Visitor id 10001\n",
    "V,1000,1          #Visit by Visitor 10001 to page id 1000\n",
    "V,1001,1          #Visit by Visitor 10001 to page id 1001\n",
    "V,1002,1          #Visit by Visitor 10001 to page id 1002\n",
    "C,\"10002\",10002   #Visitor id 10001\n",
    "V\n",
    "Note: #denotes comments\n",
    "</pre>\n",
    "to the format:\n",
    "<pre>\n",
    "V,1000,1,C, 10001\n",
    "V,1001,1,C, 10001\n",
    "V,1002,1,C, 10001\n",
    "</pre>\n",
    "Write the python code to accomplish this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "source_file = 'anonymous-msweb.data'\n",
    "output_file = 'reformated_webdata.csv'\n",
    "\n",
    "writer = open(output_file, 'w')\n",
    "current_visitor = None\n",
    "for line in open(source_file, 'r'):\n",
    "    line = line.strip()\n",
    "    if line == '' or (not line.startswith(\"C,\") and not line.startswith(\"V\")): continue\n",
    "    lineArr = line.split(',')\n",
    "    if lineArr[0] == 'C':\n",
    "        current_visitor = lineArr[2]\n",
    "    elif current_visitor != None:\n",
    "        writer.write(line + ',C,' + current_visitor + '\\n')\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# push the transformed file to HDFS\n",
    "!hdfs dfs -mkdir /w261/hw4/\n",
    "!hdfs dfs -put reformated_webdata.csv /w261/hw4/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 4.3:\n",
    "Find the 5 most frequently visited pages using MrJob from the output of 4.2 (i.e., transfromed log file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting top_pages.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile top_pages.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.protocol import RawValueProtocol\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class TopPages(MRJob):\n",
    "    #OUTPUT_PROTOCOL = RawValueProtocol\n",
    "    \n",
    "    def steps(self):\n",
    "        # two step process: first step calculate total visits for each page.\n",
    "        # second step ranks the pages using hadoop's shuffle and sort.\n",
    "        return [\n",
    "            MRStep(mapper=self.process_input, reducer=self.aggregate_page_counts),\n",
    "            MRStep(reducer=self.rank_pages, jobconf={\n",
    "                    'stream.num.map.output.key.fields': 2,\n",
    "                    'mapreduce.partition.keypartitioner.options': '-k1,1',\n",
    "                    'mapred.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "                    'mapred.text.key.comparator.options': \"-k2,2nr\",\n",
    "                    'mapred.reduce.tasks': 1\n",
    "                })\n",
    "        ]\n",
    "    \n",
    "    def process_input(self, _, line):\n",
    "        # simpily emits page counts as per raw file\n",
    "        lineArr = line.strip().split(',')\n",
    "        page_id = lineArr[1]\n",
    "        visit_counts = int(lineArr[2])\n",
    "        yield page_id, visit_counts\n",
    "    \n",
    "    def aggregate_page_counts(self, page_id, visit_counts):\n",
    "        #calculate counts per page\n",
    "        yield page_id, sum(visit_counts)\n",
    "        \n",
    "    def rank_pages(self, page_id, total_visits):\n",
    "        #page should have come in sorted by hadoop, so just printing them out.\n",
    "        yield page_id, max(total_visits)\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    TopPages.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/06 04:18:05 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /w261/hw4/4_3\n",
      "no configs found; falling back on auto-configuration\n",
      "no configs found; falling back on auto-configuration\n",
      "creating tmp directory /tmp/top_pages.root.20160206.041806.848882\n",
      "writing wrapper script to /tmp/top_pages.root.20160206.041806.848882/setup-wrapper.sh\n",
      "Using Hadoop version 2.6.0\n",
      "Copying local files into hdfs:///user/root/tmp/mrjob/top_pages.root.20160206.041806.848882/files/\n",
      "\n",
      "PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "\n",
      "HADOOP: packageJobJar: [] [/usr/lib/hadoop/lib/hadoop-streaming-2.6.0.jar] /tmp/streamjob1514787184897643950.jar tmpDir=null\n",
      "HADOOP: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "HADOOP: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "HADOOP: Total input paths to process : 1\n",
      "HADOOP: number of splits:2\n",
      "HADOOP: Submitting tokens for job: job_1454723286833_0012\n",
      "HADOOP: Submitted application application_1454723286833_0012\n",
      "HADOOP: The url to track the job: http://ip-172-31-55-170.ec2.internal:8088/proxy/application_1454723286833_0012/\n",
      "HADOOP: Running job: job_1454723286833_0012\n",
      "HADOOP: Job job_1454723286833_0012 running in uber mode : false\n",
      "HADOOP:  map 0% reduce 0%\n",
      "HADOOP:  map 42% reduce 0%\n",
      "HADOOP:  map 100% reduce 0%\n",
      "HADOOP:  map 100% reduce 100%\n",
      "HADOOP: Job job_1454723286833_0012 completed successfully\n",
      "HADOOP: Counters: 49\n",
      "HADOOP: \tFile System Counters\n",
      "HADOOP: \t\tFILE: Number of bytes read=1085200\n",
      "HADOOP: \t\tFILE: Number of bytes written=2517346\n",
      "HADOOP: \t\tFILE: Number of read operations=0\n",
      "HADOOP: \t\tFILE: Number of large read operations=0\n",
      "HADOOP: \t\tFILE: Number of write operations=0\n",
      "HADOOP: \t\tHDFS: Number of bytes read=1678449\n",
      "HADOOP: \t\tHDFS: Number of bytes written=2903\n",
      "HADOOP: \t\tHDFS: Number of read operations=9\n",
      "HADOOP: \t\tHDFS: Number of large read operations=0\n",
      "HADOOP: \t\tHDFS: Number of write operations=2\n",
      "HADOOP: \tJob Counters \n",
      "HADOOP: \t\tLaunched map tasks=2\n",
      "HADOOP: \t\tLaunched reduce tasks=1\n",
      "HADOOP: \t\tData-local map tasks=2\n",
      "HADOOP: \t\tTotal time spent by all maps in occupied slots (ms)=52046\n",
      "HADOOP: \t\tTotal time spent by all reduces in occupied slots (ms)=15115\n",
      "HADOOP: \t\tTotal time spent by all map tasks (ms)=52046\n",
      "HADOOP: \t\tTotal time spent by all reduce tasks (ms)=15115\n",
      "HADOOP: \t\tTotal vcore-seconds taken by all map tasks=52046\n",
      "HADOOP: \t\tTotal vcore-seconds taken by all reduce tasks=15115\n",
      "HADOOP: \t\tTotal megabyte-seconds taken by all map tasks=53295104\n",
      "HADOOP: \t\tTotal megabyte-seconds taken by all reduce tasks=15477760\n",
      "HADOOP: \tMap-Reduce Framework\n",
      "HADOOP: \t\tMap input records=98654\n",
      "HADOOP: \t\tMap output records=98654\n",
      "HADOOP: \t\tMap output bytes=887886\n",
      "HADOOP: \t\tMap output materialized bytes=1085206\n",
      "HADOOP: \t\tInput split bytes=210\n",
      "HADOOP: \t\tCombine input records=0\n",
      "HADOOP: \t\tCombine output records=0\n",
      "HADOOP: \t\tReduce input groups=285\n",
      "HADOOP: \t\tReduce shuffle bytes=1085206\n",
      "HADOOP: \t\tReduce input records=98654\n",
      "HADOOP: \t\tReduce output records=285\n",
      "HADOOP: \t\tSpilled Records=197308\n",
      "HADOOP: \t\tShuffled Maps =2\n",
      "HADOOP: \t\tFailed Shuffles=0\n",
      "HADOOP: \t\tMerged Map outputs=2\n",
      "HADOOP: \t\tGC time elapsed (ms)=912\n",
      "HADOOP: \t\tCPU time spent (ms)=10270\n",
      "HADOOP: \t\tPhysical memory (bytes) snapshot=557387776\n",
      "HADOOP: \t\tVirtual memory (bytes) snapshot=7588909056\n",
      "HADOOP: \t\tTotal committed heap usage (bytes)=392372224\n",
      "HADOOP: \tShuffle Errors\n",
      "HADOOP: \t\tBAD_ID=0\n",
      "HADOOP: \t\tCONNECTION=0\n",
      "HADOOP: \t\tIO_ERROR=0\n",
      "HADOOP: \t\tWRONG_LENGTH=0\n",
      "HADOOP: \t\tWRONG_MAP=0\n",
      "HADOOP: \t\tWRONG_REDUCE=0\n",
      "HADOOP: \tFile Input Format Counters \n",
      "HADOOP: \t\tBytes Read=1678239\n",
      "HADOOP: \tFile Output Format Counters \n",
      "HADOOP: \t\tBytes Written=2903\n",
      "HADOOP: Output directory: hdfs:///user/root/tmp/mrjob/top_pages.root.20160206.041806.848882/step-output/1\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.6.0:\n",
      "The have been translated as follows\n",
      " mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.reduce.tasks: mapreduce.job.reduces\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "HADOOP: mapred.output.key.comparator.class is deprecated. Instead, use mapreduce.job.output.key.comparator.class\n",
      "HADOOP: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "HADOOP: mapred.text.key.comparator.options is deprecated. Instead, use mapreduce.partition.keycomparator.options\n",
      "HADOOP: packageJobJar: [] [/usr/lib/hadoop/lib/hadoop-streaming-2.6.0.jar] /tmp/streamjob7531246948078421331.jar tmpDir=null\n",
      "HADOOP: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "HADOOP: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "HADOOP: Total input paths to process : 1\n",
      "HADOOP: number of splits:2\n",
      "HADOOP: Submitting tokens for job: job_1454723286833_0013\n",
      "HADOOP: Submitted application application_1454723286833_0013\n",
      "HADOOP: The url to track the job: http://ip-172-31-55-170.ec2.internal:8088/proxy/application_1454723286833_0013/\n",
      "HADOOP: Running job: job_1454723286833_0013\n",
      "HADOOP: Job job_1454723286833_0013 running in uber mode : false\n",
      "HADOOP:  map 0% reduce 0%\n",
      "HADOOP:  map 50% reduce 0%\n",
      "HADOOP:  map 100% reduce 0%\n",
      "HADOOP:  map 100% reduce 100%\n",
      "HADOOP: Job job_1454723286833_0013 completed successfully\n",
      "HADOOP: Counters: 49\n",
      "HADOOP: \tFile System Counters\n",
      "HADOOP: \t\tFILE: Number of bytes read=3764\n",
      "HADOOP: \t\tFILE: Number of bytes written=356193\n",
      "HADOOP: \t\tFILE: Number of read operations=0\n",
      "HADOOP: \t\tFILE: Number of large read operations=0\n",
      "HADOOP: \t\tFILE: Number of write operations=0\n",
      "HADOOP: \t\tHDFS: Number of bytes read=4667\n",
      "HADOOP: \t\tHDFS: Number of bytes written=2903\n",
      "HADOOP: \t\tHDFS: Number of read operations=9\n",
      "HADOOP: \t\tHDFS: Number of large read operations=0\n",
      "HADOOP: \t\tHDFS: Number of write operations=2\n",
      "HADOOP: \tJob Counters \n",
      "HADOOP: \t\tLaunched map tasks=2\n",
      "HADOOP: \t\tLaunched reduce tasks=1\n",
      "HADOOP: \t\tData-local map tasks=2\n",
      "HADOOP: \t\tTotal time spent by all maps in occupied slots (ms)=37943\n",
      "HADOOP: \t\tTotal time spent by all reduces in occupied slots (ms)=10145\n",
      "HADOOP: \t\tTotal time spent by all map tasks (ms)=37943\n",
      "HADOOP: \t\tTotal time spent by all reduce tasks (ms)=10145\n",
      "HADOOP: \t\tTotal vcore-seconds taken by all map tasks=37943\n",
      "HADOOP: \t\tTotal vcore-seconds taken by all reduce tasks=10145\n",
      "HADOOP: \t\tTotal megabyte-seconds taken by all map tasks=38853632\n",
      "HADOOP: \t\tTotal megabyte-seconds taken by all reduce tasks=10388480\n",
      "HADOOP: \tMap-Reduce Framework\n",
      "HADOOP: \t\tMap input records=285\n",
      "HADOOP: \t\tMap output records=285\n",
      "HADOOP: \t\tMap output bytes=3188\n",
      "HADOOP: \t\tMap output materialized bytes=3770\n",
      "HADOOP: \t\tInput split bytes=312\n",
      "HADOOP: \t\tCombine input records=0\n",
      "HADOOP: \t\tCombine output records=0\n",
      "HADOOP: \t\tReduce input groups=285\n",
      "HADOOP: \t\tReduce shuffle bytes=3770\n",
      "HADOOP: \t\tReduce input records=285\n",
      "HADOOP: \t\tReduce output records=285\n",
      "HADOOP: \t\tSpilled Records=570\n",
      "HADOOP: \t\tShuffled Maps =2\n",
      "HADOOP: \t\tFailed Shuffles=0\n",
      "HADOOP: \t\tMerged Map outputs=2\n",
      "HADOOP: \t\tGC time elapsed (ms)=879\n",
      "HADOOP: \t\tCPU time spent (ms)=3470\n",
      "HADOOP: \t\tPhysical memory (bytes) snapshot=557481984\n",
      "HADOOP: \t\tVirtual memory (bytes) snapshot=7588683776\n",
      "HADOOP: \t\tTotal committed heap usage (bytes)=392372224\n",
      "HADOOP: \tShuffle Errors\n",
      "HADOOP: \t\tBAD_ID=0\n",
      "HADOOP: \t\tCONNECTION=0\n",
      "HADOOP: \t\tIO_ERROR=0\n",
      "HADOOP: \t\tWRONG_LENGTH=0\n",
      "HADOOP: \t\tWRONG_MAP=0\n",
      "HADOOP: \t\tWRONG_REDUCE=0\n",
      "HADOOP: \tFile Input Format Counters \n",
      "HADOOP: \t\tBytes Read=4355\n",
      "HADOOP: \tFile Output Format Counters \n",
      "HADOOP: \t\tBytes Written=2903\n",
      "HADOOP: Output directory: hdfs:///w261/hw4/4_3/\n",
      "Counters from step 2:\n",
      "  (no counters found)\n",
      "removing tmp directory /tmp/top_pages.root.20160206.041806.848882\n",
      "deleting hdfs:///user/root/tmp/mrjob/top_pages.root.20160206.041806.848882 from HDFS\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /w261/hw4/4_3;\n",
    "!python top_pages.py -r hadoop hdfs:///w261/hw4/reformated_webdata.csv --output-dir hdfs:///w261/hw4/4_3/ --no-output\n",
    "#!python top_pages.py reformated_webdata.csv.backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"1008\"\t10836\r\n",
      "\"1034\"\t9383\r\n",
      "\"1004\"\t8463\r\n",
      "\"1018\"\t5330\r\n",
      "\"1017\"\t5108\r\n"
     ]
    }
   ],
   "source": [
    "# top 5 most visited pages:\n",
    "!hdfs dfs -cat /w261/hw4/4_3/part* | head -5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# HW 4.4:\n",
    "Find the most frequent visitor of each page using MrJob and the output of 4.2 (i.e., transfromed log file). In this output please include the webpage URL, webpageID and Visitor ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# need to first extract out the urls, will be used to lookup url in reducer\n",
    "import csv\n",
    "source_file = 'anonymous-msweb.data'\n",
    "output_file = 'url_data.csv'\n",
    "writer = open(output_file, 'w')\n",
    "\n",
    "for line in open(source_file, 'r'):\n",
    "    line = line.strip()\n",
    "    if line == '':\n",
    "        continue\n",
    "    if line.startswith('A,'):\n",
    "        lineArr = list(csv.reader(line.splitlines(), delimiter=',', quotechar='\"'))[0]\n",
    "        writer.write(','.join([lineArr[1], lineArr[4].replace('/', '')]) + '\\n')\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1287,autoroute\r\n",
      "1288,library\r\n",
      "1289,masterchef\r\n",
      "1297,centroam\r\n",
      "1215,developer\r\n",
      "1279,msgolf\r\n",
      "1239,msconsult\r\n",
      "1282,home\r\n",
      "1251,referencesupport\r\n",
      "1121,magazine\r\n"
     ]
    }
   ],
   "source": [
    "!cat url_data.csv | head -10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting top_visitor.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile top_visitor.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.protocol import RawValueProtocol\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class TopVisitor(MRJob):\n",
    "    \n",
    "    OUTPUT_PROTOCOL = RawValueProtocol\n",
    "    \n",
    "    def steps(self):\n",
    "        # two step processes: firs step to calculate visit counts per user and page combination\n",
    "        # second step to rank user by their visit counts within each page\n",
    "        return [\n",
    "            MRStep(mapper=self.process_visits, reducer=self.aggregate_visits),\n",
    "            MRStep(reducer_init=self.load_url_info, reducer=self.find_most_freq_visitor)\n",
    "        ]\n",
    "    \n",
    "    def process_visits(self, _, line):\n",
    "        # emiting visit counts per user and page combination\n",
    "        lineArr = line.strip().split(',')\n",
    "        page_id = lineArr[1]\n",
    "        visit_counts = int(lineArr[2])\n",
    "        visitor_id = lineArr[4]\n",
    "        yield (page_id, visitor_id), visit_counts \n",
    "        \n",
    "    def aggregate_visits(self, page_visitor_id, visit_counts):\n",
    "        # calculate how many visits from each visitor on the particular page\n",
    "        yield page_visitor_id[0], (page_visitor_id[1], sum(visit_counts))\n",
    "        \n",
    "    def load_url_info(self):\n",
    "        # loading up URL dictionary to used for lookup in reducer\n",
    "        self.URLs = {}\n",
    "        for line in open('url_data.csv', 'r'):\n",
    "            lineArr = line.strip().split(',')\n",
    "            self.URLs[lineArr[0]] = lineArr[1]\n",
    "            \n",
    "    def find_most_freq_visitor(self, page_id, visitor_data):\n",
    "        # find the most frequent visotor for each page.\n",
    "        max_visitor = None\n",
    "        max_visits = 0\n",
    "        for visitor_id, visit_counts in visitor_data:\n",
    "            if visit_counts > max_visits:\n",
    "                max_visitor = visitor_id\n",
    "                max_visits = visit_counts\n",
    "        page_url = 'UNKN'\n",
    "        # lookup URL\n",
    "        if page_id in self.URLs:\n",
    "            page_url = self.URLs[page_id]\n",
    "        \n",
    "        yield None, '\\t'.join([page_url, page_id, max_visitor])\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    TopVisitor.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no configs found; falling back on auto-configuration\n",
      "no configs found; falling back on auto-configuration\n",
      "creating tmp directory /tmp/top_visitor.root.20160206.071928.722265\n",
      "\n",
      "PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "\n",
      "writing to /tmp/top_visitor.root.20160206.071928.722265/step-0-mapper_part-00000\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "writing to /tmp/top_visitor.root.20160206.071928.722265/step-0-mapper-sorted\n",
      "> sort /tmp/top_visitor.root.20160206.071928.722265/step-0-mapper_part-00000\n",
      "writing to /tmp/top_visitor.root.20160206.071928.722265/step-0-reducer_part-00000\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "writing to /tmp/top_visitor.root.20160206.071928.722265/step-1-mapper_part-00000\n",
      "Counters from step 2:\n",
      "  (no counters found)\n",
      "writing to /tmp/top_visitor.root.20160206.071928.722265/step-1-mapper-sorted\n",
      "> sort /tmp/top_visitor.root.20160206.071928.722265/step-1-mapper_part-00000\n",
      "writing to /tmp/top_visitor.root.20160206.071928.722265/step-1-reducer_part-00000\n",
      "Counters from step 2:\n",
      "  (no counters found)\n",
      "Moving /tmp/top_visitor.root.20160206.071928.722265/step-1-reducer_part-00000 -> /tmp/top_visitor.root.20160206.071928.722265/output/part-00000\n",
      "Streaming final output from /tmp/top_visitor.root.20160206.071928.722265/output\n",
      "removing tmp directory /tmp/top_visitor.root.20160206.071928.722265\n"
     ]
    }
   ],
   "source": [
    "# local test\n",
    "!python top_visitor.py reformated_webdata.csv.backup --file url_data.csv > 4_4_local_output.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regwiz\t1000\t10001\r\n",
      "support\t1001\t10001\r\n",
      "athome\t1002\t10001\r\n",
      "kb\t1003\t10002\r\n",
      "search\t1004\t10003\r\n",
      "norge\t1005\t10004\r\n",
      "misc\t1006\t10005\r\n",
      "ie_intl\t1007\t10007\r\n",
      "msdownload\t1008\t10009\r\n",
      "windows\t1009\t10009\r\n",
      "vbasic\t1010\t10010\r\n",
      "officedev\t1011\t10010\r\n",
      "outlookdev\t1012\t10010\r\n",
      "vbasicsupport\t1013\t10010\r\n",
      "officefreestuff\t1014\t10010\r\n",
      "msexcel\t1015\t10011\r\n",
      "excel\t1016\t10011\r\n",
      "products\t1017\t10011\r\n",
      "isapi\t1018\t10011\r\n",
      "mspowerpoint\t1019\t10011\r\n",
      "msdn\t1020\t10012\r\n",
      "visualc\t1021\t10012\r\n",
      "truetype\t1022\t10013\r\n",
      "spain\t1023\t10014\r\n",
      "iis\t1024\t10015\r\n",
      "gallery\t1025\t10016\r\n",
      "sitebuilder\t1026\t10016\r\n",
      "intdev\t1027\t10017\r\n",
      "oledev\t1028\t10017\r\n",
      "clipgallerylive\t1029\t10019\r\n",
      "ntserver\t1030\t10019\r\n",
      "msoffice\t1031\t10019\r\n",
      "games\t1032\t10019\r\n",
      "logostore\t1033\t10019\r\n",
      "ie\t1034\t10020\r\n",
      "windowssupport\t1035\t10021\r\n",
      "organizations\t1036\t10021\r\n",
      "windows95\t1037\t10021\r\n",
      "sbnmember\t1038\t10021\r\n",
      "isp\t1039\t10021\r\n",
      "office\t1040\t10021\r\n",
      "workshop\t1041\t10021\r\n",
      "vstudio\t1042\t10021\r\n",
      "smallbiz\t1043\t10021\r\n",
      "mediadev\t1044\t10024\r\n",
      "netmeeting\t1045\t10025\r\n",
      "iesupport\t1046\t10027\r\n",
      "publisher\t1048\t10030\r\n",
      "supportnet\t1049\t10031\r\n",
      "macoffice\t1050\t10032\r\n"
     ]
    }
   ],
   "source": [
    "# show some sample output\n",
    "!cat 4_4_local_output.txt | head -50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# HW 4.5\n",
    "Here you will use a different dataset consisting of word-frequency distributions \n",
    "for 1,000 Twitter users. These Twitter users use language in very different ways,\n",
    "and were classified by hand according to the criteria:\n",
    "\n",
    "0: Human, where only basic human-human communication is observed.\n",
    "\n",
    "1: Cyborg, where language is primarily borrowed from other sources\n",
    "(e.g., jobs listings, classifieds postings, advertisements, etc...).\n",
    "\n",
    "2: Robot, where language is formulaically derived from unrelated sources\n",
    "(e.g., weather/seismology, police/fire event logs, etc...).\n",
    "\n",
    "3: Spammer, where language is replicated to high multiplicity\n",
    "(e.g., celebrity obsessions, personal promotion, etc... )\n",
    "\n",
    "The main data lie in the accompanying file: topUsers_Apr-Jul_2014_1000-words.txt\n",
    "and are of the form:\n",
    "USERID,CODE,TOTAL,WORD1_COUNT,WORD2_COUNT,...\n",
    "where\n",
    "USERID = unique user identifier\n",
    "CODE = 0/1/2/3 class code\n",
    "TOTAL = sum of the word counts\n",
    "\n",
    "Using this data, you will implement a 1000-dimensional K-means algorithm in MrJob on the users\n",
    "by their 1000-dimensional word stripes/vectors using several \n",
    "centroid initializations and values of K.\n",
    "\n",
    "Note that each \"point\" is a user as represented by 1000 words, and that\n",
    "word-frequency distributions are generally heavy-tailed power-laws\n",
    "(often called Zipf distributions), and are very rare in the larger class\n",
    "of discrete, random distributions. For each user you will have to normalize\n",
    "by its \"TOTAL\" column. Try several parameterizations and initializations:\n",
    "\n",
    "(A) K=4 uniform random centroid-distributions over the 1000 words\n",
    "(B) K=2 perturbation-centroids, randomly perturbed from the aggregated (user-wide) distribution \n",
    "(C) K=4 perturbation-centroids, randomly perturbed from the aggregated (user-wide) distribution \n",
    "(D) K=4 \"trained\" centroids, determined by the sums across the classes.\n",
    "\n",
    "and iterate until a threshold (try 0.001) is reached.\n",
    "After convergence, print out a summary of the classes present in each cluster.\n",
    "In particular, report the composition as measured by the total\n",
    "portion of each class type (0-3) contained in each cluster,\n",
    "and discuss your findings and any differences in outcomes across parts A-D.\n",
    "\n",
    "Note that you do not have to compute the aggregated distribution or the \n",
    "class-aggregated distributions, which are rows in the auxiliary file:\n",
    "topUsers_Apr-Jul_2014_1000-words_summaries.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Kmeans.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile Kmeans.py\n",
    "from numpy import argmin, array, random\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRJobStep\n",
    "from itertools import chain\n",
    "\n",
    "#Calculate find the nearest centroid for data point \n",
    "def MinDist(datapoint, centroid_points):\n",
    "    datapoint = array(datapoint)\n",
    "    centroid_points = array(centroid_points)\n",
    "    diff = datapoint - centroid_points \n",
    "    diffsq = diff*diff\n",
    "    # Get the nearest centroid for each instance\n",
    "    minidx = argmin(list(diffsq.sum(axis = 1)))\n",
    "    return minidx\n",
    "\n",
    "#Check whether centroids converge\n",
    "def stop_criterion(centroid_points_old, centroid_points_new,T):\n",
    "    oldvalue = list(chain(*centroid_points_old))\n",
    "    newvalue = list(chain(*centroid_points_new))\n",
    "    Diff = [abs(x-y) for x, y in zip(oldvalue, newvalue)]\n",
    "    Flag = True\n",
    "    for i in Diff:\n",
    "        if(i>T):\n",
    "            Flag = False\n",
    "            break\n",
    "    return Flag\n",
    "\n",
    "class MRKmeans(MRJob):\n",
    "    centroid_points=[]\n",
    "    k=4\n",
    "    dim=1000\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRJobStep(mapper_init = self.mapper_init, mapper=self.mapper,combiner = self.combiner,reducer=self.reducer)\n",
    "            #MRJobStep(mapper_init = self.mapper_init, mapper=self.mapper,reducer=self.reducer)\n",
    "               ]\n",
    "    #load centroids info from file\n",
    "    def mapper_init(self):\n",
    "        self.centroid_points = [map(float,s.split('\\n')[0].split(',')) for s in open(\"Centroids.txt\").readlines()]\n",
    "        self.k = len(self.centroid_points)\n",
    "        self.k = len(self.centroid_points[0])\n",
    "        #open('/data/w261/hw4/Centroids.txt', 'w').close()\n",
    "        open('/data/w261/hw4/fitted_results.txt', 'w').close()\n",
    "    #load data and output the nearest centroid index and data point \n",
    "    def mapper(self, _, line):\n",
    "        lineArr = line.strip().split(',')\n",
    "        user_id = lineArr[0]\n",
    "        user_class = lineArr[1]\n",
    "        user_total_words = float(lineArr[2])\n",
    "        # normalize by user total word counts\n",
    "        D = (map(lambda x: float(x)/user_total_words,lineArr[3:]))\n",
    "        fitted_centroid_idx = int(MinDist(D,self.centroid_points))\n",
    "        with open('/data/w261/hw4/fitted_results.txt', 'a') as f:\n",
    "            f.writelines(','.join([str(fitted_centroid_idx), user_class, user_id]) + '\\n')\n",
    "        yield fitted_centroid_idx, ([1]+D)\n",
    "    #Combine sum of data points locally\n",
    "    def combiner(self, idx, inputdata):\n",
    "        datasum = []\n",
    "        for data in inputdata:\n",
    "            if len(datasum) == 0:\n",
    "                datasum = array(data)\n",
    "            else:\n",
    "                datasum = datasum + data\n",
    "        yield idx, list(datasum)    \n",
    "    #Aggregate sum for each cluster and then calculate the new centroids\n",
    "    def reducer(self, idx, inputdata): \n",
    "        #print \"running reducer for centroid index %s\"%idx\n",
    "        centroids = []\n",
    "        num = [0]*self.k \n",
    "        for i in range(self.k):\n",
    "            centroids.append([0]*self.dim)\n",
    "        for data in inputdata:\n",
    "            n = data[0]\n",
    "            data = data[1:]\n",
    "            num[idx] = num[idx] + n\n",
    "            for data_idx, data_val in enumerate(data):\n",
    "                centroids[idx][data_idx] = centroids[idx][data_idx] + data_val\n",
    "        for centroid_val_idx, centroid_val in enumerate(centroids[idx]):\n",
    "            centroids[idx][centroid_val_idx] = centroid_val/num[idx]\n",
    "        #with open('/data/w261/hw4/Centroids.txt', 'a') as f:\n",
    "        #    f.writelines(','.join(map(str,centroids[idx])) + '\\n')\n",
    "        yield idx,centroids[idx]\n",
    "      \n",
    "if __name__ == '__main__':\n",
    "    MRKmeans.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting kmeans_driver.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile kmeans_driver.py\n",
    "from numpy import random\n",
    "from random import randint\n",
    "from Kmeans import MRKmeans, stop_criterion\n",
    "import sys\n",
    "\n",
    "init_options = sys.argv[1].upper()\n",
    "\n",
    "mr_job = MRKmeans(args=['topUsers_Apr-Jul_2014_1000-words.txt', '--file',  'Centroids.txt'])\n",
    "\n",
    "# function that prints out purity results of the fitted clusters\n",
    "def printFittedResults():\n",
    "    results = {}\n",
    "    for line in open('fitted_results.txt', 'r'):\n",
    "        line = line.strip()\n",
    "        if line == '': continue\n",
    "        lineArr = line.split(',')\n",
    "        fitted_class = lineArr[0]\n",
    "        actual_class = lineArr[1]\n",
    "        if fitted_class not in results:\n",
    "            results[fitted_class] = {actual_class: 1, 'total': 1}\n",
    "        else:\n",
    "            fitted_results = results[fitted_class]\n",
    "            fitted_results['total'] += 1\n",
    "            if actual_class not in fitted_results:\n",
    "                fitted_results[actual_class] = 1\n",
    "            else:\n",
    "                fitted_results[actual_class] += 1\n",
    "    for fitted_class in results:\n",
    "        fitted_results = results[fitted_class]\n",
    "        print '---------------------------------'\n",
    "        print \"Fitted Cluster Index: %s\" % fitted_class\n",
    "        print \"\\ttotal records: %s\" % fitted_results['total']\n",
    "        for actual_class in fitted_results:\n",
    "            if actual_class != 'total':\n",
    "                print \"\\tactual class %s: %s [%.4f]\" % (actual_class, fitted_results[actual_class], float(fitted_results[actual_class])/fitted_results['total'])\n",
    "\n",
    "#Geneate k initial centroids with uniform distribution\n",
    "def generateKUniform(k, feature_size, lower_bound, upper_bound):\n",
    "    results = []\n",
    "    for i in range(k):\n",
    "        results.append([random.uniform(0,0.5) for i in range(feature_size)])\n",
    "    return results\n",
    "\n",
    "# randomly perturb from user-wide aggregated data\n",
    "def randomPerturbUserWide(k):\n",
    "    summary_data = []\n",
    "    results = []\n",
    "    #summary_total_count = 0\n",
    "    for line in open('topUsers_Apr-Jul_2014_1000-words_summaries.txt', 'r'):\n",
    "        line = line.strip()\n",
    "        if line.startswith('ALL_CODES,'):\n",
    "            lineArr = line.split(',')\n",
    "            #summary_total_count = float(lineArr[2])\n",
    "            summary_data = map(float, lineArr[3:])\n",
    "            break\n",
    "    for i in range(k):\n",
    "        perturbed_data = map(lambda x: x+random.uniform(0,10), summary_data)\n",
    "        perturbed_sum = sum(perturbed_data)\n",
    "        results.append(map(lambda x: x/perturbed_sum, perturbed_data))\n",
    "    return results\n",
    "\n",
    "# randomly pick k records from data as centroids\n",
    "def randomPickKUsers(k):\n",
    "    results = []\n",
    "    data = []\n",
    "    for line in open('topUsers_Apr-Jul_2014_1000-words.txt', 'r'):\n",
    "        line = line.strip()\n",
    "        if line == '': continue\n",
    "        data.append(line)\n",
    "    for i in range(k):\n",
    "        selectedIdx = randint(0, len(data)-1)\n",
    "        selectedLine = data[selectedIdx].split(',')\n",
    "        total_words = float(selectedLine[2])\n",
    "        results.append(map(lambda x: float(x)/total_words, selectedLine[3:]))\n",
    "    return results\n",
    "\n",
    "# find averages from class-wide distribution as the initilizing points\n",
    "def loadTrainedCentroids():\n",
    "    results = []\n",
    "    for line in open('topUsers_Apr-Jul_2014_1000-words_summaries.txt', 'r'):\n",
    "        line = line.strip()\n",
    "        if line == '' or not line.startswith('CODE,'): continue\n",
    "        lineArr = line.split(',')\n",
    "        total_words = float(lineArr[2])\n",
    "        results.append(map(lambda x: float(x)/total_words, lineArr[3:]))\n",
    "    return results\n",
    "\n",
    "# persist centroid points to disc\n",
    "def saveCentroid():\n",
    "    with open('Centroids.txt', 'w+') as f:\n",
    "        f.writelines(','.join(str(j) for j in i) + '\\n' for i in centroid_points)\n",
    "\n",
    "# feature_size\n",
    "dim = 1000\n",
    "# convergence threahold\n",
    "thold = 0.001\n",
    "\n",
    "if init_options == 'A':\n",
    "    # (A) K=4, uniform random centroids\n",
    "    print \"K=4, uniformly random initialization\"\n",
    "    centroid_points = randomPickKUsers(4)\n",
    "elif init_options == 'B':\n",
    "    # (B) K=2 with perturbation-centroid\n",
    "    print \"K=2, perturbed centroids from user-wide aggregation\"\n",
    "    centroid_points = randomPerturbUserWide(2)\n",
    "elif init_options == 'C':\n",
    "    # (C) K=4 with perturbation-centroid\n",
    "    print \"K=4, perturbed centroids from user-wide aggregation\"\n",
    "    centroid_points = randomPerturbUserWide(4)\n",
    "else:\n",
    "    # (D) K=4 with trained centroid\n",
    "    print \"Initialize centroids with training data\"\n",
    "    centroid_points = loadTrainedCentroids()\n",
    "\n",
    "# save centroid initialization\n",
    "saveCentroid()\n",
    "\n",
    "# Update centroids iteratively\n",
    "i = 0\n",
    "while(1):\n",
    "    # save previous centoids to check convergency\n",
    "    centroid_points_old = centroid_points[:]\n",
    "    #print \"iteration\"+str(i)+\":\"\n",
    "    with mr_job.make_runner() as runner:\n",
    "        runner.run()\n",
    "        # stream_output: get access of the output \n",
    "        for line in runner.stream_output():\n",
    "            key,value =  mr_job.parse_output_line(line)\n",
    "            #print key, value\n",
    "            centroid_points[key] = value\n",
    "    # persist the current centroid point\n",
    "    saveCentroid()\n",
    "    i = i + 1\n",
    "    if(stop_criterion(centroid_points_old,centroid_points,thold)):\n",
    "        break\n",
    "#print \"Centroids\\n\"\n",
    "#print centroid_points\n",
    "print \"converged at iteration: %s\" % i\n",
    "printFittedResults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K=4, uniformly random initialization\n",
      "No handlers could be found for logger \"mrjob.runner\"\n",
      "converged at iteration: 10\n",
      "---------------------------------\n",
      "Fitted Cluster Index: 1\n",
      "\ttotal records: 130\n",
      "\tactual class 1: 88 [0.6769]\n",
      "\tactual class 3: 3 [0.0231]\n",
      "\tactual class 2: 38 [0.2923]\n",
      "\tactual class 0: 1 [0.0077]\n",
      "---------------------------------\n",
      "Fitted Cluster Index: 0\n",
      "\ttotal records: 60\n",
      "\tactual class 0: 2 [0.0333]\n",
      "\tactual class 3: 58 [0.9667]\n",
      "---------------------------------\n",
      "Fitted Cluster Index: 3\n",
      "\ttotal records: 108\n",
      "\tactual class 0: 82 [0.7593]\n",
      "\tactual class 1: 1 [0.0093]\n",
      "\tactual class 2: 15 [0.1389]\n",
      "\tactual class 3: 10 [0.0926]\n",
      "---------------------------------\n",
      "Fitted Cluster Index: 2\n",
      "\ttotal records: 702\n",
      "\tactual class 1: 2 [0.0028]\n",
      "\tactual class 0: 667 [0.9501]\n",
      "\tactual class 3: 32 [0.0456]\n",
      "\tactual class 2: 1 [0.0014]\n"
     ]
    }
   ],
   "source": [
    "!python kmeans_driver.py A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K=2, perturbed centroids from user-wide aggregation\n",
      "No handlers could be found for logger \"mrjob.runner\"\n",
      "converged at iteration: 5\n",
      "---------------------------------\n",
      "Fitted Cluster Index: 1\n",
      "\ttotal records: 133\n",
      "\tactual class 1: 88 [0.6617]\n",
      "\tactual class 3: 4 [0.0301]\n",
      "\tactual class 2: 40 [0.3008]\n",
      "\tactual class 0: 1 [0.0075]\n",
      "---------------------------------\n",
      "Fitted Cluster Index: 0\n",
      "\ttotal records: 867\n",
      "\tactual class 0: 751 [0.8662]\n",
      "\tactual class 3: 99 [0.1142]\n",
      "\tactual class 2: 14 [0.0161]\n",
      "\tactual class 1: 3 [0.0035]\n"
     ]
    }
   ],
   "source": [
    "!python kmeans_driver.py B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K=4, perturbed centroids from user-wide aggregation\n",
      "No handlers could be found for logger \"mrjob.runner\"\n",
      "converged at iteration: 6\n",
      "---------------------------------\n",
      "Fitted Cluster Index: 1\n",
      "\ttotal records: 578\n",
      "\tactual class 0: 512 [0.8858]\n",
      "\tactual class 3: 66 [0.1142]\n",
      "---------------------------------\n",
      "Fitted Cluster Index: 0\n",
      "\ttotal records: 130\n",
      "\tactual class 1: 88 [0.6769]\n",
      "\tactual class 3: 4 [0.0308]\n",
      "\tactual class 2: 37 [0.2846]\n",
      "\tactual class 0: 1 [0.0077]\n",
      "---------------------------------\n",
      "Fitted Cluster Index: 3\n",
      "\ttotal records: 182\n",
      "\tactual class 0: 162 [0.8901]\n",
      "\tactual class 1: 3 [0.0165]\n",
      "\tactual class 2: 6 [0.0330]\n",
      "\tactual class 3: 11 [0.0604]\n",
      "---------------------------------\n",
      "Fitted Cluster Index: 2\n",
      "\ttotal records: 110\n",
      "\tactual class 0: 77 [0.7000]\n",
      "\tactual class 3: 22 [0.2000]\n",
      "\tactual class 2: 11 [0.1000]\n"
     ]
    }
   ],
   "source": [
    "!python kmeans_driver.py C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize centroids with training data\n",
      "No handlers could be found for logger \"mrjob.runner\"\n",
      "converged at iteration: 5\n",
      "---------------------------------\n",
      "Fitted Cluster Index: 1\n",
      "\ttotal records: 51\n",
      "\tactual class 1: 51 [1.0000]\n",
      "---------------------------------\n",
      "Fitted Cluster Index: 0\n",
      "\ttotal records: 804\n",
      "\tactual class 0: 749 [0.9316]\n",
      "\tactual class 3: 38 [0.0473]\n",
      "\tactual class 2: 14 [0.0174]\n",
      "\tactual class 1: 3 [0.0037]\n",
      "---------------------------------\n",
      "Fitted Cluster Index: 3\n",
      "\ttotal records: 63\n",
      "\tactual class 0: 2 [0.0317]\n",
      "\tactual class 3: 61 [0.9683]\n",
      "---------------------------------\n",
      "Fitted Cluster Index: 2\n",
      "\ttotal records: 82\n",
      "\tactual class 1: 37 [0.4512]\n",
      "\tactual class 3: 4 [0.0488]\n",
      "\tactual class 2: 40 [0.4878]\n",
      "\tactual class 0: 1 [0.0122]\n"
     ]
    }
   ],
   "source": [
    "!python kmeans_driver.py D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>General Discussion on 4.5</b>\n",
    "\n",
    "Initialization with trained data obtained the result with highest purity and least number of iterations.  When using user-wide perturbation and K=4, the result is less pure but also didn't take many iterations.  K=4 uniformly pick from user records requires the longest iteration times.  With K=2, the results is not very pure, which is not surprising since we already know the there are four true classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
